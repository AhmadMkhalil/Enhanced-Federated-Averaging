avg_train_accuracy: 0.896
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.10505319148936171
- 0.18462765957446808
- 0.3254255319148936
- 0.33159574468085107
- 0.4242021276595745
- 0.4308510638297872
- 0.46632978723404256
- 0.4804255319148936
- 0.4323404255319149
- 0.5486702127659574
- 0.5137765957446808
- 0.5763829787234043
- 0.5831914893617022
- 0.5679787234042554
- 0.6293085106382978
- 0.5932446808510639
- 0.538563829787234
- 0.5287765957446808
- 0.5858510638297872
- 0.6193617021276596
- 0.5821276595744681
- 0.6466489361702128
- 0.6307978723404255
- 0.6234042553191489
- 0.6548404255319149
- 0.6060106382978724
- 0.6026595744680852
- 0.5995744680851064
- 0.5975
- 0.5935106382978723
- 0.6348404255319149
- 0.6357978723404255
- 0.6206914893617022
- 0.5876595744680851
- 0.6573936170212766
- 0.6341489361702127
- 0.6582446808510638
- 0.666968085106383
- 0.651436170212766
- 0.6660106382978723
- 0.6625531914893616
- 0.675372340425532
- 0.65
- 0.6735106382978724
- 0.6572340425531915
- 0.6299468085106383
- 0.6534574468085106
- 0.6654255319148936
- 0.6721808510638297
- 0.6037234042553191
- 0.6203191489361702
- 0.670904255319149
- 0.6759574468085107
- 0.6990957446808511
- 0.6906382978723404
- 0.6876063829787235
- 0.6268617021276596
- 0.6410638297872341
- 0.6248404255319149
- 0.6802659574468085
- 0.6759042553191489
- 0.7029255319148936
- 0.6852127659574468
- 0.6940425531914893
- 0.6757978723404255
- 0.6778191489361702
- 0.6492021276595744
- 0.674468085106383
- 0.6855851063829788
- 0.6335106382978724
- 0.6734574468085106
- 0.6844148936170212
- 0.6693617021276596
- 0.6691489361702128
- 0.6520744680851064
- 0.6629255319148936
- 0.6527659574468085
- 0.658563829787234
- 0.6121276595744681
- 0.6626063829787234
- 0.6522340425531915
- 0.6785106382978724
- 0.6602127659574468
- 0.6579787234042553
- 0.6895744680851064
- 0.7135638297872341
- 0.6942553191489361
- 0.6664893617021277
- 0.6280851063829788
- 0.6546276595744681
- 0.6626595744680851
- 0.6893617021276596
- 0.6749468085106383
- 0.6923404255319149
- 0.6586702127659575
- 0.6801595744680851
- 0.6554255319148936
- 0.6913297872340426
- 0.6782978723404255
- 0.7123404255319149
test_loss_list:
- 534.5256152153015
- 450.9188885688782
- 376.2673623561859
- 341.1367881298065
- 292.86234390735626
- 293.58475947380066
- 275.375679731369
- 256.4767920970917
- 271.006466627121
- 205.75040018558502
- 222.71508526802063
- 202.7948968410492
- 202.9403783082962
- 192.97013306617737
- 161.74773728847504
- 189.93485474586487
- 223.92824494838715
- 210.558549284935
- 188.07124757766724
- 167.23639434576035
- 185.7296725511551
- 154.1718140244484
- 164.6917873620987
- 157.48299211263657
- 143.29139280319214
- 165.8285419344902
- 187.5984412431717
- 173.2936576604843
- 177.7411606311798
- 187.00278425216675
- 155.66238874197006
- 160.84502524137497
- 154.67172920703888
- 175.6419245004654
- 140.62003016471863
- 148.89072972536087
- 152.0877583026886
- 141.78582859039307
- 146.84831780195236
- 151.23889887332916
- 143.56392723321915
- 135.9707407951355
- 145.16504734754562
- 142.59029811620712
- 143.63814276456833
- 168.81997990608215
- 145.41374623775482
- 137.80045694112778
- 155.23279732465744
- 173.3178637623787
- 153.27175933122635
- 141.77543032169342
- 137.51528769731522
- 127.41679358482361
- 136.88434582948685
- 134.8515061736107
- 158.38315320014954
- 146.71363085508347
- 149.81892466545105
- 138.70949214696884
- 144.9490761756897
- 132.761323928833
- 141.43644684553146
- 127.37372612953186
- 135.61065244674683
- 134.82038801908493
- 140.53888100385666
- 140.96805053949356
- 131.99620938301086
- 143.22447741031647
- 128.05425214767456
- 130.46405863761902
- 148.1014444231987
- 144.08565211296082
- 147.51955205202103
- 135.18396604061127
- 146.43151372671127
- 143.95135074853897
- 165.6847682595253
- 142.3684035539627
- 155.79841685295105
- 125.04024320840836
- 140.43217480182648
- 146.09907853603363
- 129.65199077129364
- 122.22198498249054
- 131.6013354063034
- 136.87789523601532
- 159.82223689556122
- 144.45466077327728
- 138.93158024549484
- 134.23722785711288
- 128.52275842428207
- 132.25288105010986
- 133.78252011537552
- 128.80115568637848
- 139.7719424366951
- 133.05681890249252
- 131.0959032177925
- 118.51056218147278
train_accuracy:
- 0.023
- 0.048
- 0.358
- 0.239
- 0.014
- 0.364
- 0.567
- 0.825
- 0.644
- 0.527
- 0.95
- 0.721
- 0.745
- 0.25
- 0.821
- 0.703
- 0.8
- 0.653
- 0.609
- 0.765
- 0.545
- 0.65
- 0.806
- 0.925
- 0.9
- 0.625
- 0.386
- 0.944
- 0.509
- 0.889
- 0.672
- 0.619
- 0.75
- 0.464
- 0.583
- 0.91
- 0.578
- 0.856
- 0.711
- 0.709
- 0.35
- 0.616
- 0.573
- 0.661
- 0.92
- 0.55
- 0.85
- 0.859
- 0.672
- 0.467
- 0.633
- 0.533
- 0.709
- 0.56
- 0.864
- 0.712
- 0.808
- 0.85
- 0.86
- 0.763
- 0.629
- 0.631
- 0.467
- 0.93
- 0.627
- 0.807
- 0.894
- 0.967
- 0.914
- 0.6
- 0.39
- 0.792
- 0.964
- 0.912
- 0.559
- 0.75
- 0.696
- 0.62
- 0.817
- 0.568
- 0.491
- 0.62
- 0.987
- 0.67
- 0.14
- 0.944
- 0.675
- 0.656
- 0.656
- 0.97
- 0.56
- 0.95
- 0.03
- 0.794
- 0.327
- 0.867
- 0.97
- 0.757
- 0.81
- 0.896
train_loss:
- 1.208
- 0.79
- 0.684
- 0.724
- 0.648
- 0.523
- 0.513
- 0.52
- 0.552
- 0.483
- 0.405
- 0.406
- 0.37
- 0.365
- 0.419
- 0.421
- 0.281
- 0.352
- 0.441
- 0.405
- 0.468
- 0.428
- 0.426
- 0.306
- 0.333
- 0.385
- 0.302
- 0.346
- 0.307
- 0.401
- 0.434
- 0.395
- 0.407
- 0.359
- 0.383
- 0.341
- 0.431
- 0.406
- 0.375
- 0.377
- 0.391
- 0.42
- 0.352
- 0.271
- 0.316
- 0.304
- 0.333
- 0.386
- 0.393
- 0.373
- 0.362
- 0.341
- 0.394
- 0.315
- 0.308
- 0.335
- 0.419
- 0.357
- 0.401
- 0.371
- 0.326
- 0.301
- 0.327
- 0.323
- 0.39
- 0.306
- 0.299
- 0.251
- 0.375
- 0.35
- 0.342
- 0.406
- 0.214
- 0.247
- 0.385
- 0.369
- 0.378
- 0.288
- 0.272
- 0.336
- 0.358
- 0.391
- 0.324
- 0.355
- 0.337
- 0.286
- 0.269
- 0.405
- 0.319
- 0.325
- 0.408
- 0.236
- 0.317
- 0.255
- 0.29
- 0.34
- 0.321
- 0.396
- 0.321
- 0.325
unequal: 1
verbose: 1
