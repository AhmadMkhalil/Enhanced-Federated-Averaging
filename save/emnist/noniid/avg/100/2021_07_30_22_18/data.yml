avg_train_accuracy: 0.85
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.18117021276595743
- 0.2622872340425532
- 0.2781914893617021
- 0.34047872340425533
- 0.4167021276595745
- 0.423936170212766
- 0.49
- 0.48781914893617023
- 0.545372340425532
- 0.5523404255319149
- 0.5892553191489361
- 0.5439893617021276
- 0.6352127659574468
- 0.5816489361702127
- 0.633031914893617
- 0.6203723404255319
- 0.5909042553191489
- 0.6042553191489362
- 0.615531914893617
- 0.6187234042553191
- 0.6545744680851063
- 0.6403723404255319
- 0.6443085106382979
- 0.5892021276595745
- 0.6202659574468085
- 0.6021808510638298
- 0.6202659574468085
- 0.6424468085106383
- 0.6574468085106383
- 0.6459574468085106
- 0.621968085106383
- 0.6200531914893617
- 0.6680851063829787
- 0.6216489361702128
- 0.6581382978723405
- 0.6432978723404256
- 0.6485106382978724
- 0.6146808510638297
- 0.6584042553191489
- 0.6630851063829787
- 0.6510638297872341
- 0.6246276595744681
- 0.6454787234042553
- 0.6696808510638298
- 0.6697872340425531
- 0.6796808510638298
- 0.6522340425531915
- 0.6678723404255319
- 0.6898404255319149
- 0.6462234042553191
- 0.6323404255319149
- 0.6277659574468085
- 0.6264893617021277
- 0.686968085106383
- 0.6593617021276595
- 0.6181914893617021
- 0.655
- 0.6377659574468085
- 0.6824468085106383
- 0.6568085106382979
- 0.6578191489361702
- 0.6710106382978723
- 0.6613297872340426
- 0.6835638297872341
- 0.6851595744680851
- 0.6819148936170213
- 0.6762234042553191
- 0.6734574468085106
- 0.6869148936170213
- 0.6847340425531915
- 0.6910106382978723
- 0.7075
- 0.6723404255319149
- 0.673031914893617
- 0.7036170212765958
- 0.7018085106382979
- 0.7093085106382979
- 0.7036170212765958
- 0.6372340425531915
- 0.6667021276595745
- 0.699627659574468
- 0.6872872340425532
- 0.6867021276595745
- 0.6725531914893617
- 0.6688297872340425
- 0.6824468085106383
- 0.676968085106383
- 0.6412765957446809
- 0.6857446808510639
- 0.6775
- 0.6398404255319149
- 0.6780851063829787
- 0.7007978723404256
- 0.6845744680851064
- 0.7018617021276595
- 0.7115957446808511
- 0.7065957446808511
- 0.7042021276595745
- 0.681595744680851
- 0.6962234042553191
test_loss_list:
- 526.9801495075226
- 445.05760049819946
- 396.3938775062561
- 339.6567361354828
- 310.77187955379486
- 286.859556555748
- 252.8595404624939
- 255.6938441991806
- 216.33687031269073
- 214.65908288955688
- 192.22042644023895
- 208.5987365245819
- 176.1572152376175
- 203.55246210098267
- 171.53348070383072
- 168.43831205368042
- 187.67552721500397
- 184.21593630313873
- 170.43716233968735
- 172.7769889831543
- 151.26016944646835
- 166.07700324058533
- 153.38870841264725
- 173.8603910803795
- 168.1156205534935
- 172.93701171875
- 164.92495280504227
- 144.57878082990646
- 155.93184930086136
- 149.47757756710052
- 151.36320167779922
- 155.13497561216354
- 153.88410592079163
- 153.7434060573578
- 141.62083160877228
- 164.95616990327835
- 144.52509719133377
- 156.99800699949265
- 150.0150311589241
- 147.11416506767273
- 145.29379451274872
- 170.75943821668625
- 159.38048815727234
- 145.76881968975067
- 144.99447226524353
- 142.5765179991722
- 152.42576974630356
- 135.37883430719376
- 134.68181890249252
- 141.05027335882187
- 158.04331105947495
- 151.92692929506302
- 149.12949454784393
- 133.0822153687477
- 138.5294474363327
- 152.43536627292633
- 135.46567261219025
- 152.68199747800827
- 135.24357670545578
- 149.90786415338516
- 143.66838026046753
- 135.79217666387558
- 142.01440995931625
- 137.25691068172455
- 137.7896614074707
- 130.2406052350998
- 145.9202343225479
- 145.69260954856873
- 132.12344110012054
- 135.0135239958763
- 126.61582493782043
- 128.52110296487808
- 137.26012325286865
- 130.09632217884064
- 124.83196747303009
- 129.31148767471313
- 116.0143711566925
- 130.09161192178726
- 158.15925127267838
- 132.61851543188095
- 127.20270174741745
- 135.73050117492676
- 133.14914083480835
- 144.690196454525
- 142.64770883321762
- 136.90595871210098
- 141.15715569257736
- 157.29470545053482
- 131.38940757513046
- 149.97471570968628
- 160.0432426929474
- 130.98093175888062
- 135.4753902554512
- 136.8703617453575
- 123.54710936546326
- 117.42253118753433
- 119.92238402366638
- 124.11145544052124
- 137.4706107378006
- 123.34468793869019
train_accuracy:
- 0.031
- 0.154
- 0.59
- 0.767
- 0.05
- 0.46
- 0.9
- 0.283
- 0.517
- 0.379
- 0.91
- 0.233
- 0.583
- 0.407
- 0.418
- 0.643
- 0.306
- 0.563
- 0.44
- 0.787
- 0.443
- 0.812
- 0.917
- 0.067
- 0.871
- 0.743
- 0.739
- 0.789
- 0.625
- 0.967
- 0.436
- 0.675
- 0.775
- 0.793
- 0.418
- 0.758
- 0.875
- 0.061
- 0.931
- 0.42
- 0.508
- 0.853
- 0.763
- 0.53
- 0.467
- 0.812
- 0.322
- 0.708
- 0.367
- 0.636
- 0.5
- 0.906
- 0.617
- 0.87
- 0.87
- 0.917
- 0.833
- 0.754
- 0.9
- 0.629
- 0.912
- 0.271
- 0.067
- 0.893
- 0.479
- 0.595
- 0.514
- 0.893
- 0.3
- 0.394
- 0.9
- 0.764
- 0.508
- 0.48
- 0.843
- 0.883
- 0.869
- 0.919
- 0.967
- 0.845
- 0.9
- 0.15
- 0.434
- 0.894
- 0.667
- 0.846
- 0.692
- 0.794
- 0.0
- 0.522
- 0.526
- 0.812
- 0.33
- 0.583
- 0.492
- 0.738
- 0.668
- 0.641
- 0.9
- 0.85
train_loss:
- 1.186
- 0.778
- 0.626
- 0.508
- 0.569
- 0.398
- 0.421
- 0.5
- 0.461
- 0.407
- 0.438
- 0.457
- 0.341
- 0.403
- 0.505
- 0.399
- 0.456
- 0.344
- 0.418
- 0.354
- 0.384
- 0.285
- 0.441
- 0.36
- 0.377
- 0.348
- 0.354
- 0.375
- 0.373
- 0.377
- 0.35
- 0.337
- 0.308
- 0.42
- 0.355
- 0.239
- 0.348
- 0.383
- 0.331
- 0.339
- 0.371
- 0.319
- 0.272
- 0.277
- 0.361
- 0.348
- 0.406
- 0.32
- 0.332
- 0.368
- 0.322
- 0.353
- 0.371
- 0.342
- 0.313
- 0.359
- 0.291
- 0.294
- 0.266
- 0.479
- 0.308
- 0.266
- 0.321
- 0.302
- 0.351
- 0.378
- 0.379
- 0.321
- 0.369
- 0.301
- 0.295
- 0.309
- 0.389
- 0.295
- 0.28
- 0.399
- 0.268
- 0.307
- 0.317
- 0.338
- 0.358
- 0.352
- 0.328
- 0.291
- 0.312
- 0.316
- 0.24
- 0.271
- 0.352
- 0.322
- 0.364
- 0.266
- 0.335
- 0.293
- 0.321
- 0.33
- 0.339
- 0.275
- 0.348
- 0.271
unequal: 1
verbose: 1
