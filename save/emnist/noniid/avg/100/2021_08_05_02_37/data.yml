avg_train_accuracy: 0.57
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.11085106382978724
- 0.23436170212765958
- 0.29356382978723405
- 0.4511702127659574
- 0.4950531914893617
- 0.4681382978723404
- 0.5235106382978724
- 0.5079255319148936
- 0.5663297872340426
- 0.5954787234042553
- 0.5803191489361702
- 0.5820744680851064
- 0.5671808510638298
- 0.6095212765957447
- 0.6081382978723404
- 0.6141489361702127
- 0.5875531914893617
- 0.638563829787234
- 0.5980319148936171
- 0.6001595744680851
- 0.5944680851063829
- 0.6323404255319149
- 0.671968085106383
- 0.6386702127659575
- 0.5989893617021277
- 0.583936170212766
- 0.6279787234042553
- 0.6263297872340425
- 0.6728723404255319
- 0.6131914893617021
- 0.6362234042553192
- 0.6212765957446809
- 0.623031914893617
- 0.6643617021276595
- 0.6672872340425532
- 0.6947340425531915
- 0.6739893617021276
- 0.643563829787234
- 0.6326063829787234
- 0.6712765957446809
- 0.681595744680851
- 0.6634042553191489
- 0.6872872340425532
- 0.5488297872340425
- 0.6862234042553191
- 0.6646808510638298
- 0.6625531914893616
- 0.7067553191489362
- 0.6714893617021277
- 0.6842021276595744
- 0.6606382978723404
- 0.6897872340425532
- 0.6670212765957447
- 0.7153723404255319
- 0.6554787234042553
- 0.7070212765957447
- 0.7018617021276595
- 0.6515425531914893
- 0.6433510638297872
- 0.6621808510638297
- 0.6835106382978723
- 0.6438829787234043
- 0.694468085106383
- 0.6866489361702127
- 0.6981914893617022
- 0.6871808510638298
- 0.6728191489361702
- 0.7241489361702128
- 0.6579255319148937
- 0.6734042553191489
- 0.6899468085106383
- 0.7115425531914894
- 0.6486702127659575
- 0.6626595744680851
- 0.6577659574468085
- 0.6906914893617021
- 0.7023936170212766
- 0.6807978723404255
- 0.671595744680851
- 0.6543617021276595
- 0.6929255319148936
- 0.6784574468085106
- 0.647127659574468
- 0.67
- 0.6893085106382979
- 0.6772340425531915
- 0.6700531914893617
- 0.6656914893617021
- 0.6555851063829787
- 0.6688829787234043
- 0.6759574468085107
- 0.6613297872340426
- 0.6871808510638298
- 0.7036170212765958
- 0.6512765957446809
- 0.6875531914893617
- 0.6677659574468086
- 0.6978191489361703
- 0.7082978723404255
- 0.6883510638297873
test_loss_list:
- 541.2295551300049
- 453.4081962108612
- 396.3762278556824
- 320.84968852996826
- 277.19891571998596
- 275.9327517747879
- 241.2763990163803
- 239.76191627979279
- 204.01667940616608
- 192.4038988351822
- 196.22789108753204
- 207.0599524974823
- 201.38993513584137
- 176.05720728635788
- 177.65304553508759
- 174.727612555027
- 188.527725815773
- 167.65429973602295
- 170.7162703871727
- 171.52557027339935
- 191.25934237241745
- 174.29301196336746
- 159.5862158536911
- 167.47163677215576
- 176.4692594408989
- 176.24479097127914
- 157.0419487953186
- 163.08544832468033
- 146.3269466161728
- 168.2824301123619
- 151.52938026189804
- 171.08264130353928
- 176.71725302934647
- 144.3656991124153
- 143.57358223199844
- 136.7971332669258
- 143.61116683483124
- 150.09314495325089
- 150.56179809570312
- 141.5366454720497
- 148.06669241189957
- 146.3790385723114
- 133.4600260257721
- 197.33189272880554
- 135.02728432416916
- 143.56307691335678
- 140.6093048453331
- 126.35041439533234
- 138.04971528053284
- 134.10001945495605
- 140.6404423713684
- 131.623197555542
- 136.54902124404907
- 129.90656304359436
- 151.80643022060394
- 135.66448318958282
- 129.6775850057602
- 155.86607402563095
- 148.74123269319534
- 147.72825682163239
- 135.9791704416275
- 165.49328130483627
- 137.0252319574356
- 130.1396489739418
- 125.48301953077316
- 134.82887595891953
- 136.69076263904572
- 121.91420155763626
- 142.7321581840515
- 133.02995592355728
- 140.75672274827957
- 130.59850823879242
- 147.35502046346664
- 153.27865487337112
- 137.9877449274063
- 128.37674808502197
- 120.16939353942871
- 136.49119037389755
- 133.13143545389175
- 141.92565029859543
- 128.4949477314949
- 138.4157930612564
- 150.6364077925682
- 142.9436210989952
- 131.91661548614502
- 131.1357426047325
- 138.53523582220078
- 142.83045554161072
- 153.42676377296448
- 143.01623505353928
- 135.37675619125366
- 148.33833587169647
- 135.77952498197556
- 129.14596503973007
- 143.80821931362152
- 130.8797751069069
- 135.0684564113617
- 123.69890534877777
- 127.27006512880325
- 131.65479266643524
train_accuracy:
- 0.0
- 0.8
- 0.221
- 0.563
- 0.917
- 0.495
- 0.692
- 0.68
- 0.332
- 0.738
- 0.276
- 0.18
- 0.738
- 0.814
- 0.357
- 0.509
- 0.404
- 0.125
- 0.644
- 0.24
- 0.553
- 0.85
- 0.6
- 0.875
- 0.522
- 0.95
- 0.527
- 0.492
- 0.34
- 0.5
- 0.862
- 0.983
- 0.9
- 0.554
- 0.473
- 0.65
- 0.795
- 0.557
- 1.0
- 0.7
- 0.808
- 0.617
- 0.912
- 0.85
- 0.26
- 0.867
- 0.85
- 0.767
- 0.22
- 0.694
- 0.891
- 0.85
- 0.791
- 1.0
- 0.708
- 0.772
- 0.67
- 0.637
- 0.64
- 0.921
- 0.491
- 0.51
- 0.77
- 0.633
- 0.68
- 0.586
- 0.742
- 0.943
- 0.847
- 0.632
- 0.556
- 0.6
- 0.439
- 0.986
- 0.582
- 0.894
- 0.03
- 0.597
- 0.363
- 0.786
- 0.812
- 0.0
- 0.722
- 0.779
- 0.775
- 0.959
- 0.888
- 0.8
- 0.626
- 0.9
- 0.39
- 0.0
- 0.88
- 0.718
- 1.0
- 0.929
- 0.877
- 0.865
- 0.593
- 0.57
train_loss:
- 1.017
- 0.785
- 0.809
- 0.654
- 0.559
- 0.643
- 0.492
- 0.558
- 0.514
- 0.518
- 0.432
- 0.425
- 0.423
- 0.374
- 0.439
- 0.358
- 0.447
- 0.312
- 0.351
- 0.324
- 0.383
- 0.35
- 0.494
- 0.405
- 0.295
- 0.383
- 0.415
- 0.366
- 0.248
- 0.44
- 0.362
- 0.409
- 0.401
- 0.357
- 0.351
- 0.392
- 0.439
- 0.293
- 0.285
- 0.327
- 0.275
- 0.281
- 0.383
- 0.447
- 0.376
- 0.319
- 0.375
- 0.402
- 0.297
- 0.412
- 0.392
- 0.308
- 0.329
- 0.39
- 0.384
- 0.377
- 0.354
- 0.385
- 0.317
- 0.299
- 0.314
- 0.322
- 0.3
- 0.345
- 0.337
- 0.424
- 0.272
- 0.29
- 0.323
- 0.327
- 0.31
- 0.363
- 0.285
- 0.373
- 0.318
- 0.377
- 0.353
- 0.33
- 0.281
- 0.385
- 0.312
- 0.275
- 0.412
- 0.397
- 0.329
- 0.314
- 0.316
- 0.298
- 0.4
- 0.391
- 0.356
- 0.291
- 0.3
- 0.306
- 0.321
- 0.29
- 0.321
- 0.368
- 0.281
- 0.299
unequal: 1
verbose: 1
