avg_train_accuracy: 0.909
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.12132978723404256
- 0.29329787234042554
- 0.31574468085106383
- 0.3400531914893617
- 0.3600531914893617
- 0.3964893617021277
- 0.4671276595744681
- 0.4605851063829787
- 0.4982446808510638
- 0.5377127659574468
- 0.5255319148936171
- 0.5127127659574469
- 0.5706382978723404
- 0.5188297872340426
- 0.6007978723404256
- 0.6065957446808511
- 0.5678723404255319
- 0.6225531914893617
- 0.6109574468085106
- 0.5551063829787234
- 0.6284574468085107
- 0.6007446808510638
- 0.6054255319148936
- 0.6146276595744681
- 0.6211702127659574
- 0.6348936170212766
- 0.6522340425531915
- 0.6159574468085106
- 0.5963829787234043
- 0.6619148936170213
- 0.644468085106383
- 0.6045744680851064
- 0.67
- 0.6543085106382979
- 0.6330851063829788
- 0.6620212765957447
- 0.6699468085106383
- 0.6782978723404255
- 0.6236170212765958
- 0.6839893617021277
- 0.6912234042553191
- 0.6961702127659575
- 0.6967553191489362
- 0.6861702127659575
- 0.6311702127659574
- 0.619095744680851
- 0.6360638297872341
- 0.6622340425531915
- 0.6444148936170213
- 0.674468085106383
- 0.7121808510638298
- 0.6952659574468085
- 0.6052659574468086
- 0.6504255319148936
- 0.6795212765957447
- 0.6472340425531915
- 0.6962765957446808
- 0.6599468085106382
- 0.6806914893617021
- 0.6901063829787234
- 0.6922340425531915
- 0.6787234042553192
- 0.7044148936170213
- 0.6723404255319149
- 0.6912234042553191
- 0.6618085106382978
- 0.6261170212765957
- 0.6430851063829788
- 0.6742021276595744
- 0.7028191489361703
- 0.6776063829787234
- 0.6687234042553192
- 0.6753191489361702
- 0.6876595744680851
- 0.7011702127659575
- 0.7419148936170212
- 0.7156382978723405
- 0.6967021276595745
- 0.683404255319149
- 0.6639893617021276
- 0.7313829787234043
- 0.693936170212766
- 0.699627659574468
- 0.7167553191489362
- 0.7048936170212766
- 0.6516489361702128
- 0.6709574468085107
- 0.6854255319148936
- 0.7015957446808511
- 0.7200531914893618
- 0.7028723404255319
- 0.7338297872340426
- 0.6593617021276595
- 0.7122872340425532
- 0.656436170212766
- 0.7439361702127659
- 0.6929787234042554
- 0.7106382978723405
- 0.7341489361702128
- 0.7201595744680851
test_loss_list:
- 534.7245168685913
- 439.007780790329
- 394.4026370048523
- 345.18049693107605
- 320.4851977825165
- 300.6062681674957
- 274.0241062641144
- 258.372691988945
- 228.23064839839935
- 208.5864760875702
- 209.12695574760437
- 228.67316377162933
- 194.9401799440384
- 216.1939581632614
- 174.27473032474518
- 169.6330971121788
- 175.6674672961235
- 161.91794776916504
- 170.58261638879776
- 190.67411983013153
- 171.27718603610992
- 178.23836547136307
- 182.39484387636185
- 172.08008593320847
- 165.9895738363266
- 151.1438598036766
- 144.06660676002502
- 161.40518462657928
- 171.05384826660156
- 154.42584520578384
- 149.2612960934639
- 167.9885899424553
- 147.56502771377563
- 157.47917306423187
- 157.6797006726265
- 145.01359677314758
- 142.39821499586105
- 147.3262060880661
- 160.46162903308868
- 135.63022303581238
- 129.69939893484116
- 134.46322178840637
- 135.47094351053238
- 131.9741132259369
- 143.69459122419357
- 150.33876264095306
- 152.62427639961243
- 136.15260642766953
- 158.85911124944687
- 141.43146979808807
- 130.39149415493011
- 129.1874703168869
- 172.04887610673904
- 149.25209766626358
- 138.54554784297943
- 150.7434422969818
- 127.78535628318787
- 144.2417105436325
- 131.1906908750534
- 129.7778992652893
- 129.7095501422882
- 133.50114953517914
- 127.64523530006409
- 137.59820955991745
- 138.73016053438187
- 144.25363719463348
- 147.4298616051674
- 143.86533677577972
- 134.92453783750534
- 118.18374800682068
- 141.53218054771423
- 138.54096007347107
- 147.31776416301727
- 128.86557322740555
- 121.44199138879776
- 113.93324375152588
- 118.37074929475784
- 130.46659594774246
- 123.79580754041672
- 136.78823906183243
- 115.34862226247787
- 124.85615414381027
- 129.27003115415573
- 117.9788309931755
- 129.97978347539902
- 141.96756172180176
- 141.63841843605042
- 133.7046462893486
- 125.55719864368439
- 118.0723021030426
- 130.4044635295868
- 112.8683043718338
- 135.9909456372261
- 116.33422249555588
- 139.7404261827469
- 107.26191902160645
- 130.05315881967545
- 120.64436024427414
- 114.50265324115753
- 120.27359008789062
train_accuracy:
- 0.269
- 0.277
- 0.112
- 0.517
- 0.229
- 0.125
- 0.731
- 0.975
- 0.396
- 0.07
- 0.53
- 0.6
- 0.779
- 0.11
- 0.496
- 0.892
- 0.704
- 0.675
- 0.737
- 0.433
- 0.95
- 0.55
- 0.704
- 0.4
- 0.8
- 0.927
- 0.532
- 0.186
- 0.08
- 0.755
- 0.806
- 0.371
- 0.775
- 0.905
- 0.617
- 0.591
- 0.52
- 0.607
- 0.9
- 0.875
- 0.654
- 0.79
- 0.877
- 0.103
- 0.671
- 0.934
- 0.513
- 0.917
- 0.776
- 0.712
- 0.593
- 0.95
- 0.825
- 0.533
- 0.883
- 0.755
- 0.858
- 0.7
- 0.979
- 0.95
- 0.725
- 0.158
- 0.95
- 0.842
- 0.939
- 0.718
- 0.783
- 0.589
- 0.423
- 0.607
- 0.606
- 0.746
- 0.607
- 0.959
- 0.728
- 0.857
- 0.746
- 0.936
- 0.714
- 0.312
- 0.731
- 0.812
- 0.954
- 0.15
- 0.653
- 0.214
- 0.767
- 0.8
- 0.596
- 0.826
- 0.875
- 0.82
- 0.582
- 0.986
- 0.45
- 0.938
- 0.714
- 0.461
- 0.758
- 0.909
train_loss:
- 1.099
- 0.858
- 0.619
- 0.603
- 0.535
- 0.577
- 0.467
- 0.389
- 0.574
- 0.39
- 0.51
- 0.449
- 0.513
- 0.447
- 0.403
- 0.416
- 0.381
- 0.431
- 0.429
- 0.462
- 0.456
- 0.347
- 0.405
- 0.412
- 0.36
- 0.416
- 0.46
- 0.389
- 0.384
- 0.34
- 0.329
- 0.366
- 0.39
- 0.381
- 0.373
- 0.379
- 0.288
- 0.44
- 0.327
- 0.358
- 0.33
- 0.289
- 0.403
- 0.37
- 0.334
- 0.331
- 0.394
- 0.384
- 0.408
- 0.285
- 0.33
- 0.322
- 0.291
- 0.367
- 0.295
- 0.389
- 0.336
- 0.423
- 0.327
- 0.343
- 0.348
- 0.345
- 0.359
- 0.351
- 0.356
- 0.449
- 0.362
- 0.374
- 0.321
- 0.341
- 0.407
- 0.404
- 0.4
- 0.378
- 0.363
- 0.369
- 0.318
- 0.331
- 0.249
- 0.371
- 0.335
- 0.35
- 0.335
- 0.326
- 0.361
- 0.334
- 0.39
- 0.362
- 0.415
- 0.338
- 0.355
- 0.342
- 0.348
- 0.293
- 0.337
- 0.318
- 0.37
- 0.323
- 0.261
- 0.318
unequal: 1
verbose: 1
