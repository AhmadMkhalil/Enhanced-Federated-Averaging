avg_train_accuracy: 0.809
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.11643617021276596
- 0.256063829787234
- 0.3293617021276596
- 0.3396276595744681
- 0.40085106382978725
- 0.4774468085106383
- 0.5076595744680851
- 0.5300531914893617
- 0.5676595744680851
- 0.4577127659574468
- 0.45680851063829786
- 0.4930851063829787
- 0.5475531914893617
- 0.5538829787234043
- 0.5667021276595745
- 0.5807446808510638
- 0.6172872340425531
- 0.6073936170212766
- 0.5535638297872341
- 0.5886702127659574
- 0.5670744680851064
- 0.6438297872340426
- 0.6551595744680851
- 0.5547872340425531
- 0.5983510638297872
- 0.571063829787234
- 0.6278723404255319
- 0.6064893617021276
- 0.6498936170212766
- 0.6517021276595745
- 0.638563829787234
- 0.6353723404255319
- 0.6638829787234043
- 0.6362765957446809
- 0.6538829787234043
- 0.6156914893617021
- 0.6432446808510638
- 0.6543617021276595
- 0.650904255319149
- 0.6127127659574468
- 0.637340425531915
- 0.6548404255319149
- 0.6310106382978723
- 0.6762765957446808
- 0.6613297872340426
- 0.6216489361702128
- 0.6573404255319149
- 0.6315425531914893
- 0.6501063829787234
- 0.6556382978723404
- 0.6872340425531915
- 0.6671808510638297
- 0.6522872340425532
- 0.6437765957446808
- 0.6771276595744681
- 0.6135106382978723
- 0.6372340425531915
- 0.6490425531914894
- 0.6341489361702127
- 0.6661702127659574
- 0.6538829787234043
- 0.6631914893617021
- 0.6983510638297873
- 0.6637765957446808
- 0.6759574468085107
- 0.6960106382978724
- 0.6398936170212766
- 0.6378723404255319
- 0.6381914893617021
- 0.6495212765957447
- 0.6992553191489361
- 0.6999468085106383
- 0.7065425531914894
- 0.6981382978723404
- 0.6813297872340426
- 0.6608510638297872
- 0.6613829787234042
- 0.6964893617021276
- 0.6822340425531915
- 0.6606382978723404
- 0.6614361702127659
- 0.673031914893617
- 0.6376063829787234
- 0.6714893617021277
- 0.6941489361702128
- 0.6700531914893617
- 0.688936170212766
- 0.6370212765957447
- 0.7298936170212766
- 0.6543085106382979
- 0.7057446808510638
- 0.6781382978723405
- 0.6594148936170213
- 0.6420744680851064
- 0.6821808510638298
- 0.6656382978723404
- 0.6637765957446808
- 0.6553191489361702
- 0.7078191489361703
- 0.6764361702127659
test_loss_list:
- 545.4209446907043
- 468.90711760520935
- 405.1647834777832
- 376.68255257606506
- 316.53987979888916
- 257.7316896915436
- 248.1847059726715
- 233.16698706150055
- 208.13056576251984
- 267.0558012723923
- 253.59394085407257
- 256.9918215274811
- 208.61622524261475
- 195.14119386672974
- 189.22836089134216
- 190.9951765537262
- 167.4290024638176
- 170.19596326351166
- 189.93713873624802
- 173.98181253671646
- 182.38059490919113
- 155.43035739660263
- 159.75437080860138
- 191.94180595874786
- 199.9645441174507
- 188.0467667579651
- 167.56829124689102
- 166.63577580451965
- 147.89260584115982
- 148.92796689271927
- 150.29699611663818
- 162.19210559129715
- 155.79317259788513
- 162.80971866846085
- 162.46562314033508
- 151.20306527614594
- 145.44122684001923
- 146.01996386051178
- 149.20423585176468
- 168.18913811445236
- 154.3466409444809
- 146.43655967712402
- 154.76013976335526
- 144.08083140850067
- 146.40393942594528
- 159.7977011203766
- 135.36057192087173
- 156.86349147558212
- 146.67832654714584
- 146.85720145702362
- 140.32815277576447
- 137.33636808395386
- 150.64755183458328
- 152.27492147684097
- 144.46724444627762
- 166.56785017251968
- 156.6862195134163
- 143.44254177808762
- 155.44935029745102
- 151.92726677656174
- 153.69240581989288
- 142.00715631246567
- 126.43285483121872
- 144.0581817626953
- 139.47014564275742
- 126.62231141328812
- 156.01286840438843
- 163.99554002285004
- 153.09456580877304
- 146.51002341508865
- 129.80620884895325
- 123.23097306489944
- 128.24835389852524
- 136.20430654287338
- 137.44343888759613
- 140.5683501958847
- 135.17647695541382
- 138.4229342341423
- 134.8687202334404
- 142.710593521595
- 141.02062755823135
- 141.9115898013115
- 156.01156836748123
- 139.56939589977264
- 139.58079528808594
- 145.56988090276718
- 127.58082497119904
- 143.8347988128662
- 118.67526108026505
- 139.75323551893234
- 118.31988841295242
- 142.48683857917786
- 132.77450865507126
- 144.930826485157
- 137.98030143976212
- 141.6259052157402
- 150.0759038925171
- 137.0394987463951
- 127.1974812746048
- 140.74037235975266
train_accuracy:
- 0.0
- 0.787
- 0.471
- 0.088
- 0.436
- 0.467
- 0.827
- 0.612
- 0.618
- 0.55
- 0.658
- 0.079
- 0.593
- 0.871
- 0.16
- 0.75
- 0.667
- 0.894
- 0.429
- 0.192
- 0.166
- 0.736
- 0.873
- 0.75
- 0.55
- 0.709
- 0.925
- 0.34
- 0.608
- 0.688
- 0.486
- 0.346
- 0.65
- 0.726
- 0.862
- 0.818
- 0.413
- 0.946
- 0.562
- 0.638
- 0.417
- 0.97
- 0.333
- 0.832
- 0.55
- 0.403
- 0.511
- 0.539
- 0.815
- 0.61
- 0.912
- 0.833
- 0.75
- 0.512
- 0.55
- 0.758
- 0.538
- 0.546
- 0.75
- 0.925
- 0.579
- 0.812
- 0.938
- 0.558
- 0.59
- 0.842
- 0.64
- 0.362
- 0.9
- 0.644
- 0.183
- 0.618
- 0.943
- 0.643
- 0.888
- 0.971
- 0.364
- 0.93
- 0.95
- 0.4
- 0.012
- 0.9
- 0.936
- 0.967
- 0.791
- 0.671
- 0.833
- 0.631
- 0.938
- 0.737
- 0.944
- 0.963
- 0.67
- 0.735
- 0.863
- 0.873
- 0.96
- 0.611
- 0.742
- 0.809
train_loss:
- 1.062
- 0.775
- 0.719
- 0.588
- 0.642
- 0.52
- 0.583
- 0.399
- 0.399
- 0.484
- 0.343
- 0.531
- 0.504
- 0.416
- 0.391
- 0.44
- 0.438
- 0.409
- 0.394
- 0.412
- 0.452
- 0.435
- 0.379
- 0.448
- 0.317
- 0.322
- 0.337
- 0.312
- 0.323
- 0.345
- 0.409
- 0.373
- 0.401
- 0.424
- 0.417
- 0.313
- 0.382
- 0.358
- 0.317
- 0.423
- 0.345
- 0.362
- 0.314
- 0.366
- 0.406
- 0.343
- 0.34
- 0.396
- 0.363
- 0.4
- 0.303
- 0.353
- 0.409
- 0.413
- 0.301
- 0.419
- 0.326
- 0.361
- 0.313
- 0.247
- 0.347
- 0.294
- 0.329
- 0.407
- 0.37
- 0.271
- 0.323
- 0.319
- 0.272
- 0.307
- 0.327
- 0.382
- 0.326
- 0.295
- 0.36
- 0.277
- 0.282
- 0.375
- 0.256
- 0.347
- 0.315
- 0.324
- 0.346
- 0.356
- 0.285
- 0.254
- 0.367
- 0.27
- 0.313
- 0.321
- 0.334
- 0.309
- 0.293
- 0.312
- 0.318
- 0.351
- 0.244
- 0.344
- 0.35
- 0.338
unequal: 1
verbose: 1
