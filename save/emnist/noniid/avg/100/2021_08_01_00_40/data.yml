avg_train_accuracy: 0.783
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.12877659574468084
- 0.20648936170212767
- 0.23547872340425532
- 0.3852127659574468
- 0.3758510638297872
- 0.4456914893617021
- 0.4570744680851064
- 0.5224468085106383
- 0.5044148936170213
- 0.5547872340425531
- 0.500904255319149
- 0.5592021276595744
- 0.5104255319148936
- 0.6006382978723405
- 0.5900531914893618
- 0.5468085106382978
- 0.6169148936170212
- 0.5594148936170212
- 0.5809042553191489
- 0.5564361702127659
- 0.5865957446808511
- 0.6414893617021277
- 0.6136170212765958
- 0.6441489361702127
- 0.6087765957446809
- 0.6152127659574468
- 0.6276595744680851
- 0.6702659574468085
- 0.6175531914893617
- 0.5929787234042553
- 0.6885638297872341
- 0.6322872340425532
- 0.636436170212766
- 0.6448404255319149
- 0.6558510638297872
- 0.6747872340425531
- 0.6837234042553192
- 0.686063829787234
- 0.6070744680851063
- 0.6334574468085107
- 0.6669148936170213
- 0.6527127659574468
- 0.6890957446808511
- 0.640531914893617
- 0.648936170212766
- 0.6584574468085106
- 0.656436170212766
- 0.7006382978723404
- 0.6570212765957447
- 0.6319148936170212
- 0.6636702127659575
- 0.6336702127659575
- 0.6567021276595745
- 0.6531382978723405
- 0.6234574468085107
- 0.7047340425531915
- 0.6544148936170213
- 0.6466489361702128
- 0.651595744680851
- 0.6371808510638298
- 0.6249468085106383
- 0.670904255319149
- 0.6580851063829787
- 0.6468617021276596
- 0.6311170212765957
- 0.6396276595744681
- 0.6800531914893617
- 0.705
- 0.6878191489361702
- 0.693404255319149
- 0.6668085106382978
- 0.6678723404255319
- 0.7247340425531915
- 0.6906914893617021
- 0.6607978723404255
- 0.6417021276595745
- 0.6628723404255319
- 0.675372340425532
- 0.6662765957446809
- 0.6901595744680851
- 0.6925
- 0.6942021276595745
- 0.6546808510638298
- 0.6418085106382979
- 0.6976063829787233
- 0.6858510638297872
- 0.6867021276595745
- 0.7084574468085106
- 0.7062765957446808
- 0.6924468085106383
- 0.6407446808510638
- 0.6831382978723404
- 0.7201595744680851
- 0.6717553191489362
- 0.6849468085106383
- 0.6740425531914893
- 0.6850531914893617
- 0.6933510638297873
- 0.7015425531914894
- 0.6680851063829787
test_loss_list:
- 529.1305999755859
- 455.4052667617798
- 407.27266025543213
- 332.74137449264526
- 313.4120409488678
- 268.9171667098999
- 265.6659072637558
- 239.00401103496552
- 230.35808217525482
- 206.95959293842316
- 242.30048382282257
- 200.5518820285797
- 222.1534115076065
- 181.38248336315155
- 187.09024691581726
- 198.8407211303711
- 175.41087770462036
- 186.44210195541382
- 193.0572850704193
- 192.6500951051712
- 182.76622593402863
- 161.40845733880997
- 161.84552609920502
- 161.0011523962021
- 171.76477813720703
- 168.24373334646225
- 165.477248609066
- 151.6849410533905
- 180.45337516069412
- 178.77733945846558
- 137.87985342741013
- 156.33034402132034
- 159.53811424970627
- 152.58778393268585
- 157.1966289281845
- 144.90396010875702
- 136.5610299706459
- 141.47669291496277
- 158.5501964688301
- 154.53703731298447
- 140.49085783958435
- 143.76244449615479
- 135.3209164738655
- 149.56805503368378
- 148.74596655368805
- 138.35673832893372
- 143.3135108947754
- 134.10701656341553
- 148.96746283769608
- 155.03159046173096
- 143.5684369802475
- 149.2026822566986
- 138.16117388010025
- 149.68737995624542
- 163.04048073291779
- 125.01793158054352
- 137.07474088668823
- 143.99982291460037
- 151.8551703095436
- 155.0013519525528
- 158.42521047592163
- 137.69309997558594
- 141.28862929344177
- 153.0828475356102
- 148.09290719032288
- 147.92597991228104
- 137.94493007659912
- 120.83070701360703
- 129.82648026943207
- 136.47394037246704
- 133.48829346895218
- 140.18475610017776
- 116.24831402301788
- 128.0843330025673
- 136.3545470237732
- 137.6052529811859
- 137.57105326652527
- 128.82409137487411
- 132.7634888291359
- 127.85121011734009
- 126.85162603855133
- 130.62473720312119
- 154.4287274479866
- 151.08451581001282
- 131.5769989490509
- 131.11037647724152
- 127.62588185071945
- 119.76922714710236
- 121.03136539459229
- 139.16905492544174
- 146.95216345787048
- 141.3762657046318
- 120.34442400932312
- 128.87236315011978
- 123.9351914525032
- 145.58030080795288
- 124.5988461971283
- 124.92172068357468
- 136.6878712773323
- 139.56744170188904
train_accuracy:
- 0.423
- 0.0
- 0.16
- 0.112
- 0.494
- 0.837
- 0.687
- 0.786
- 0.465
- 0.239
- 0.493
- 0.045
- 0.881
- 0.45
- 0.823
- 0.657
- 0.5
- 0.517
- 0.592
- 0.554
- 0.967
- 0.975
- 0.888
- 0.504
- 0.669
- 0.631
- 0.854
- 0.67
- 0.919
- 0.95
- 0.983
- 0.477
- 0.84
- 0.455
- 0.721
- 0.84
- 0.838
- 0.519
- 0.207
- 0.612
- 0.814
- 0.73
- 0.892
- 0.769
- 0.808
- 0.867
- 0.9
- 0.938
- 0.588
- 0.556
- 0.625
- 0.747
- 0.432
- 0.567
- 0.95
- 0.809
- 0.938
- 0.431
- 0.795
- 0.406
- 0.967
- 0.95
- 0.621
- 0.77
- 0.621
- 0.642
- 0.717
- 0.681
- 0.838
- 0.558
- 0.9
- 0.963
- 0.967
- 0.781
- 0.6
- 0.438
- 0.639
- 0.75
- 0.531
- 0.353
- 0.767
- 0.75
- 0.529
- 1.0
- 0.888
- 0.612
- 0.867
- 0.76
- 0.91
- 0.47
- 0.642
- 0.621
- 0.918
- 0.884
- 0.833
- 0.875
- 0.658
- 0.3
- 0.596
- 0.783
train_loss:
- 1.183
- 0.754
- 0.64
- 0.7
- 0.515
- 0.566
- 0.551
- 0.458
- 0.482
- 0.428
- 0.44
- 0.419
- 0.532
- 0.481
- 0.516
- 0.48
- 0.324
- 0.494
- 0.412
- 0.413
- 0.472
- 0.379
- 0.329
- 0.425
- 0.433
- 0.393
- 0.34
- 0.441
- 0.342
- 0.357
- 0.437
- 0.314
- 0.362
- 0.409
- 0.371
- 0.448
- 0.377
- 0.397
- 0.413
- 0.323
- 0.39
- 0.358
- 0.41
- 0.412
- 0.389
- 0.352
- 0.395
- 0.257
- 0.367
- 0.391
- 0.324
- 0.341
- 0.355
- 0.342
- 0.307
- 0.344
- 0.406
- 0.369
- 0.333
- 0.403
- 0.41
- 0.374
- 0.387
- 0.33
- 0.399
- 0.292
- 0.287
- 0.385
- 0.261
- 0.313
- 0.31
- 0.427
- 0.244
- 0.287
- 0.316
- 0.387
- 0.367
- 0.331
- 0.339
- 0.255
- 0.314
- 0.309
- 0.313
- 0.397
- 0.267
- 0.321
- 0.324
- 0.395
- 0.324
- 0.33
- 0.32
- 0.366
- 0.358
- 0.358
- 0.307
- 0.313
- 0.323
- 0.335
- 0.28
- 0.292
unequal: 1
verbose: 1
