avg_train_accuracy: 0.879
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.13425531914893618
- 0.23808510638297872
- 0.3278191489361702
- 0.37686170212765957
- 0.4272340425531915
- 0.4270212765957447
- 0.4378723404255319
- 0.5153191489361703
- 0.5712234042553191
- 0.48595744680851066
- 0.47893617021276597
- 0.550531914893617
- 0.613563829787234
- 0.6308510638297873
- 0.5906382978723405
- 0.6144148936170213
- 0.5846808510638298
- 0.5777127659574468
- 0.6343085106382979
- 0.6172872340425531
- 0.5961702127659575
- 0.5901063829787234
- 0.6756382978723404
- 0.6582446808510638
- 0.6490425531914894
- 0.6207978723404255
- 0.6622340425531915
- 0.6567553191489361
- 0.6281914893617021
- 0.6345744680851064
- 0.6272340425531915
- 0.6408510638297872
- 0.6348404255319149
- 0.6391489361702127
- 0.6186170212765958
- 0.6512765957446809
- 0.6532446808510638
- 0.6864361702127659
- 0.6384042553191489
- 0.6390425531914894
- 0.6837765957446809
- 0.6222340425531915
- 0.6689893617021276
- 0.6493085106382979
- 0.661063829787234
- 0.7013829787234043
- 0.6660106382978723
- 0.6503191489361703
- 0.6327127659574469
- 0.6923404255319149
- 0.6773404255319149
- 0.704627659574468
- 0.6797872340425531
- 0.6503191489361703
- 0.6392553191489362
- 0.6661170212765958
- 0.6855851063829788
- 0.7103723404255319
- 0.6618617021276596
- 0.6397340425531914
- 0.6269148936170212
- 0.6828723404255319
- 0.6583510638297873
- 0.6775
- 0.6707446808510639
- 0.6896808510638298
- 0.6331914893617021
- 0.6513297872340426
- 0.6568085106382979
- 0.6644148936170213
- 0.606436170212766
- 0.6930851063829787
- 0.708936170212766
- 0.6765425531914894
- 0.7311170212765957
- 0.7278191489361702
- 0.6986170212765958
- 0.7175531914893617
- 0.7076063829787234
- 0.7171808510638298
- 0.7123404255319149
- 0.6690425531914893
- 0.630531914893617
- 0.6523404255319148
- 0.6591489361702128
- 0.661063829787234
- 0.6771808510638297
- 0.7003723404255319
- 0.7100531914893617
- 0.6970744680851064
- 0.7054255319148937
- 0.7054255319148937
- 0.7142553191489361
- 0.698936170212766
- 0.7059574468085107
- 0.7131382978723404
- 0.6679255319148936
- 0.7000531914893617
- 0.7165425531914894
- 0.6901063829787234
test_loss_list:
- 535.4192235469818
- 450.31720542907715
- 378.00601506233215
- 334.0370569229126
- 303.39233803749084
- 294.8713175058365
- 275.431055188179
- 248.70755100250244
- 205.82480716705322
- 237.1611807346344
- 229.78516256809235
- 212.37153267860413
- 183.7494414448738
- 169.77536058425903
- 177.1396782398224
- 174.79370892047882
- 184.81778079271317
- 178.60332995653152
- 162.63375067710876
- 177.01998615264893
- 178.9330057501793
- 184.00298154354095
- 154.19583541154861
- 152.84352332353592
- 147.8421202301979
- 155.20448857545853
- 144.11494135856628
- 140.29408288002014
- 152.27722787857056
- 157.46071302890778
- 151.24855917692184
- 142.15226632356644
- 149.66728657484055
- 148.3522503376007
- 166.5021595954895
- 145.9648808836937
- 139.8706333041191
- 133.19141590595245
- 154.23349314928055
- 154.1050300002098
- 129.26113718748093
- 151.90476489067078
- 137.97670102119446
- 142.1770625114441
- 152.75838112831116
- 132.875661611557
- 140.18986058235168
- 141.3889298439026
- 149.14713734388351
- 127.97231215238571
- 137.66169208288193
- 126.08629190921783
- 123.9561498761177
- 136.76147150993347
- 145.13688951730728
- 134.2910385131836
- 146.65193390846252
- 119.48854690790176
- 145.05830091238022
- 145.79088973999023
- 147.2153531908989
- 131.06504434347153
- 145.02418756484985
- 134.9965916275978
- 137.02791213989258
- 133.22700262069702
- 146.41361266374588
- 146.2794114947319
- 149.70355546474457
- 144.17022168636322
- 159.5451781153679
- 124.43441581726074
- 118.23217731714249
- 126.67509984970093
- 115.92790800333023
- 122.27540898323059
- 131.22250926494598
- 118.16432350873947
- 121.61010879278183
- 129.2983599305153
- 125.69104474782944
- 144.0405125617981
- 148.16614907979965
- 143.9802423119545
- 136.77508425712585
- 140.35865408182144
- 135.36417371034622
- 123.53772783279419
- 122.44665491580963
- 120.69173777103424
- 117.8248530626297
- 119.31132757663727
- 124.6637761592865
- 128.81016963720322
- 119.83438730239868
- 115.32356536388397
- 133.1120501756668
- 131.6579927802086
- 122.32701271772385
- 122.8377777338028
train_accuracy:
- 0.175
- 0.28
- 0.0
- 0.497
- 0.0
- 0.641
- 0.864
- 0.1
- 0.888
- 0.742
- 0.447
- 0.29
- 0.888
- 0.95
- 0.747
- 0.707
- 0.705
- 0.833
- 0.625
- 0.915
- 0.556
- 0.76
- 0.646
- 0.8
- 0.862
- 0.77
- 0.85
- 0.95
- 0.63
- 0.744
- 0.665
- 0.591
- 0.93
- 0.7
- 0.733
- 0.675
- 0.8
- 0.527
- 0.8
- 0.677
- 0.737
- 0.436
- 0.731
- 0.755
- 0.918
- 0.175
- 0.494
- 0.695
- 0.881
- 0.9
- 0.786
- 0.475
- 0.777
- 0.0
- 0.455
- 0.638
- 0.35
- 0.445
- 0.13
- 0.325
- 0.363
- 0.862
- 0.407
- 0.888
- 0.0
- 0.893
- 0.394
- 0.54
- 0.91
- 0.917
- 0.924
- 0.722
- 0.612
- 0.34
- 0.795
- 0.791
- 0.617
- 0.821
- 0.63
- 0.823
- 0.89
- 0.838
- 0.8
- 0.825
- 0.9
- 0.657
- 0.871
- 0.881
- 0.374
- 0.653
- 0.845
- 0.858
- 0.526
- 0.854
- 0.678
- 0.91
- 0.9
- 0.403
- 0.83
- 0.879
train_loss:
- 1.071
- 0.873
- 0.731
- 0.517
- 0.579
- 0.551
- 0.539
- 0.51
- 0.447
- 0.512
- 0.409
- 0.439
- 0.515
- 0.351
- 0.465
- 0.386
- 0.403
- 0.367
- 0.344
- 0.538
- 0.366
- 0.428
- 0.451
- 0.402
- 0.397
- 0.386
- 0.357
- 0.33
- 0.423
- 0.395
- 0.34
- 0.262
- 0.362
- 0.404
- 0.418
- 0.414
- 0.349
- 0.372
- 0.366
- 0.398
- 0.297
- 0.312
- 0.274
- 0.374
- 0.331
- 0.352
- 0.293
- 0.316
- 0.36
- 0.339
- 0.313
- 0.315
- 0.332
- 0.392
- 0.389
- 0.406
- 0.359
- 0.374
- 0.342
- 0.34
- 0.34
- 0.348
- 0.401
- 0.319
- 0.357
- 0.335
- 0.461
- 0.331
- 0.354
- 0.388
- 0.313
- 0.323
- 0.291
- 0.367
- 0.267
- 0.424
- 0.32
- 0.335
- 0.389
- 0.275
- 0.311
- 0.342
- 0.302
- 0.341
- 0.291
- 0.337
- 0.366
- 0.328
- 0.302
- 0.38
- 0.304
- 0.325
- 0.343
- 0.298
- 0.307
- 0.27
- 0.318
- 0.326
- 0.359
- 0.326
unequal: 1
verbose: 1
