avg_train_accuracy: 0.893
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.16228723404255319
- 0.2765957446808511
- 0.2995744680851064
- 0.3126063829787234
- 0.38324468085106383
- 0.4238297872340426
- 0.43159574468085105
- 0.49398936170212765
- 0.5368085106382978
- 0.5323404255319149
- 0.5043085106382978
- 0.5718085106382979
- 0.5846808510638298
- 0.6147340425531915
- 0.5646808510638298
- 0.5685638297872341
- 0.6123936170212766
- 0.5944680851063829
- 0.5682978723404255
- 0.6056914893617021
- 0.6170744680851064
- 0.6052659574468086
- 0.6124468085106383
- 0.6288297872340426
- 0.6176595744680851
- 0.6074468085106383
- 0.6226063829787234
- 0.6580851063829787
- 0.6334042553191489
- 0.5559574468085107
- 0.6454787234042553
- 0.6011702127659575
- 0.6197872340425532
- 0.661063829787234
- 0.6297340425531915
- 0.6348936170212766
- 0.6456382978723404
- 0.6063297872340425
- 0.7
- 0.609095744680851
- 0.6142553191489362
- 0.6579255319148937
- 0.6425
- 0.655904255319149
- 0.6870744680851064
- 0.6489893617021276
- 0.6334574468085107
- 0.628563829787234
- 0.6675
- 0.6586170212765957
- 0.6455851063829787
- 0.7095744680851064
- 0.6737765957446809
- 0.6961702127659575
- 0.683936170212766
- 0.6706914893617021
- 0.6494148936170213
- 0.6271808510638298
- 0.7073936170212766
- 0.6678723404255319
- 0.6930851063829787
- 0.6320744680851064
- 0.6507446808510639
- 0.6969148936170213
- 0.6877127659574468
- 0.7088297872340426
- 0.704627659574468
- 0.6873936170212765
- 0.6578723404255319
- 0.7177127659574468
- 0.6776063829787234
- 0.6829787234042554
- 0.6475531914893617
- 0.6898936170212766
- 0.621968085106383
- 0.6474468085106383
- 0.6459574468085106
- 0.6800531914893617
- 0.6723936170212766
- 0.6429255319148937
- 0.6747340425531915
- 0.6922340425531915
- 0.6897872340425532
- 0.6430851063829788
- 0.675
- 0.6611170212765958
- 0.6677659574468086
- 0.666595744680851
- 0.6804255319148936
- 0.6855851063829788
- 0.6398936170212766
- 0.6770744680851064
- 0.6651595744680852
- 0.6488829787234043
- 0.6214893617021277
- 0.6281914893617021
- 0.6520212765957447
- 0.6760106382978723
- 0.6913829787234043
- 0.6825531914893617
test_loss_list:
- 528.9289729595184
- 440.2961435317993
- 382.1632242202759
- 351.6625120639801
- 309.11574137210846
- 284.9839713573456
- 266.79092955589294
- 241.71895217895508
- 223.9969903230667
- 228.07394742965698
- 223.51457965373993
- 202.66954267024994
- 198.30138325691223
- 176.10513854026794
- 193.02403581142426
- 190.5964103937149
- 180.32625263929367
- 177.87918078899384
- 182.66937047243118
- 169.94421499967575
- 156.7980898618698
- 167.9757210612297
- 169.1535804271698
- 169.23647093772888
- 167.24823254346848
- 171.0739860534668
- 158.68802559375763
- 149.79101395606995
- 153.3042351603508
- 181.58732962608337
- 162.99545592069626
- 184.12195336818695
- 160.93645387887955
- 145.94680899381638
- 165.53238147497177
- 155.5343125462532
- 147.19515550136566
- 168.63304114341736
- 138.52560180425644
- 187.02328485250473
- 153.70960968732834
- 148.05142438411713
- 157.2391704916954
- 143.9733066558838
- 129.64680528640747
- 152.28007352352142
- 149.24046128988266
- 153.69013857841492
- 150.74595439434052
- 130.7259842157364
- 156.06935334205627
- 139.39964133501053
- 132.38315778970718
- 137.6925481557846
- 148.16744047403336
- 137.85952198505402
- 150.9620619416237
- 161.06009185314178
- 129.16385251283646
- 143.75719368457794
- 132.9615123271942
- 157.25662797689438
- 152.7681549191475
- 124.47033911943436
- 133.40902668237686
- 125.88870006799698
- 123.5259039402008
- 139.15225875377655
- 138.4850206375122
- 120.2060706615448
- 125.10778212547302
- 138.46493566036224
- 143.25376629829407
- 133.9077313542366
- 161.58712208271027
- 144.78370541334152
- 156.6868461370468
- 131.4019917845726
- 137.32842141389847
- 151.69952303171158
- 130.96032935380936
- 122.34789037704468
- 138.9057525396347
- 145.7014797925949
- 145.64436852931976
- 141.4893114566803
- 136.67478448152542
- 131.6887732744217
- 131.53118658065796
- 127.28564751148224
- 158.58953142166138
- 142.43814158439636
- 136.39216607809067
- 139.09862130880356
- 144.53886461257935
- 151.6324758529663
- 138.92973697185516
- 143.5375496149063
- 123.21227771043777
- 129.67212337255478
train_accuracy:
- 0.056
- 0.042
- 0.664
- 0.497
- 0.212
- 0.038
- 0.314
- 0.514
- 0.413
- 0.375
- 0.3
- 0.765
- 0.71
- 0.923
- 0.75
- 0.724
- 0.119
- 0.02
- 0.614
- 0.917
- 0.7
- 0.907
- 0.843
- 0.9
- 0.55
- 0.597
- 0.483
- 0.392
- 0.889
- 0.853
- 0.827
- 0.288
- 0.309
- 0.809
- 0.288
- 0.742
- 0.573
- 0.477
- 0.793
- 0.553
- 0.094
- 0.943
- 0.925
- 0.9
- 0.868
- 0.67
- 0.906
- 0.696
- 0.663
- 0.812
- 0.661
- 0.421
- 0.681
- 0.955
- 0.25
- 0.869
- 0.779
- 0.61
- 0.779
- 0.25
- 0.884
- 0.232
- 0.647
- 0.288
- 0.284
- 0.75
- 0.715
- 0.5
- 0.712
- 0.419
- 0.665
- 0.607
- 0.923
- 0.729
- 0.38
- 0.846
- 0.929
- 0.425
- 0.794
- 0.685
- 0.77
- 0.821
- 0.871
- 0.883
- 0.934
- 0.983
- 0.677
- 0.935
- 0.96
- 0.35
- 0.732
- 0.512
- 0.567
- 0.664
- 0.286
- 0.282
- 0.981
- 0.811
- 0.814
- 0.893
train_loss:
- 1.158
- 0.787
- 0.781
- 0.592
- 0.539
- 0.51
- 0.462
- 0.528
- 0.567
- 0.58
- 0.384
- 0.378
- 0.468
- 0.426
- 0.527
- 0.508
- 0.52
- 0.391
- 0.522
- 0.404
- 0.407
- 0.395
- 0.364
- 0.369
- 0.389
- 0.455
- 0.351
- 0.372
- 0.405
- 0.412
- 0.385
- 0.374
- 0.393
- 0.42
- 0.379
- 0.347
- 0.386
- 0.407
- 0.367
- 0.402
- 0.322
- 0.298
- 0.418
- 0.418
- 0.342
- 0.384
- 0.365
- 0.322
- 0.326
- 0.363
- 0.391
- 0.338
- 0.374
- 0.234
- 0.327
- 0.346
- 0.389
- 0.337
- 0.359
- 0.338
- 0.347
- 0.319
- 0.304
- 0.327
- 0.346
- 0.31
- 0.257
- 0.365
- 0.303
- 0.31
- 0.311
- 0.378
- 0.394
- 0.316
- 0.365
- 0.357
- 0.254
- 0.294
- 0.314
- 0.316
- 0.316
- 0.345
- 0.296
- 0.286
- 0.3
- 0.35
- 0.32
- 0.274
- 0.303
- 0.376
- 0.347
- 0.334
- 0.352
- 0.368
- 0.364
- 0.33
- 0.339
- 0.313
- 0.287
- 0.289
unequal: 1
verbose: 1
