avg_train_accuracy: 0.917
avg_train_loss: 0.004
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.13063829787234044
- 0.23340425531914893
- 0.288031914893617
- 0.3744148936170213
- 0.35340425531914893
- 0.40680851063829787
- 0.48095744680851066
- 0.4573936170212766
- 0.4421276595744681
- 0.518936170212766
- 0.5037765957446808
- 0.5916489361702127
- 0.536063829787234
- 0.5488829787234043
- 0.5618085106382978
- 0.5152127659574468
- 0.598563829787234
- 0.63
- 0.6210638297872341
- 0.6342021276595745
- 0.540904255319149
- 0.5437765957446808
- 0.6406914893617022
- 0.591968085106383
- 0.638031914893617
- 0.6360106382978723
- 0.6154255319148936
- 0.6075531914893617
- 0.6332446808510638
- 0.656595744680851
- 0.6313829787234042
- 0.6032978723404255
- 0.6436170212765957
- 0.6744148936170212
- 0.6862234042553191
- 0.6531382978723405
- 0.6484574468085106
- 0.5960106382978724
- 0.6647872340425532
- 0.6272872340425532
- 0.6474468085106383
- 0.6736702127659574
- 0.6851595744680851
- 0.6328191489361702
- 0.6712765957446809
- 0.6532978723404256
- 0.6829255319148936
- 0.7203191489361702
- 0.6672340425531915
- 0.6798404255319149
- 0.6571808510638298
- 0.685
- 0.6786170212765957
- 0.6752659574468085
- 0.6340425531914894
- 0.6252127659574468
- 0.6384574468085107
- 0.6532446808510638
- 0.7197872340425532
- 0.6846276595744681
- 0.6818617021276596
- 0.6389893617021276
- 0.6909042553191489
- 0.7318617021276596
- 0.6808510638297872
- 0.6683510638297873
- 0.7250531914893616
- 0.6960106382978724
- 0.6912234042553191
- 0.6937234042553192
- 0.7031382978723404
- 0.6688297872340425
- 0.6690957446808511
- 0.6964361702127659
- 0.7021808510638298
- 0.6765425531914894
- 0.6988297872340425
- 0.7194148936170213
- 0.6651595744680852
- 0.7103191489361702
- 0.6823404255319149
- 0.6957978723404256
- 0.7289893617021277
- 0.7009574468085107
- 0.6736170212765957
- 0.730531914893617
- 0.6422340425531915
- 0.7245744680851064
- 0.7072872340425532
- 0.6528191489361702
- 0.6737765957446809
- 0.5923404255319149
- 0.6923404255319149
- 0.6983510638297873
- 0.7152659574468085
- 0.7169148936170213
- 0.6688297872340425
- 0.6956914893617021
- 0.6731914893617021
- 0.6601595744680852
test_loss_list:
- 533.103178024292
- 455.14934372901917
- 391.4924726486206
- 350.4874658584595
- 347.26184844970703
- 324.9416469335556
- 269.67618775367737
- 265.8776195049286
- 256.78386890888214
- 212.93551683425903
- 232.32336950302124
- 196.90247690677643
- 201.55712914466858
- 207.48636507987976
- 201.27009439468384
- 204.85277259349823
- 173.6789271235466
- 166.63144606351852
- 167.6927610039711
- 167.4453307390213
- 192.70400285720825
- 187.56594395637512
- 153.1776646375656
- 174.54181718826294
- 153.0396945476532
- 158.7788791656494
- 166.51249939203262
- 162.64710986614227
- 152.99794816970825
- 155.38778740167618
- 141.0161576271057
- 168.62591183185577
- 146.24518257379532
- 147.6284157037735
- 143.55064463615417
- 151.37712967395782
- 154.60995894670486
- 166.48263382911682
- 154.67223984003067
- 149.81684374809265
- 154.1731112599373
- 140.3786940574646
- 139.1920416355133
- 157.76465874910355
- 137.85801964998245
- 156.45467978715897
- 131.47875040769577
- 121.19631546735764
- 139.85451745986938
- 141.5867332816124
- 147.7124720811844
- 136.02581483125687
- 140.04475569725037
- 140.62134337425232
- 155.75080865621567
- 165.1351798772812
- 143.4224635362625
- 145.7141894698143
- 119.2786957025528
- 124.8990997672081
- 131.22583585977554
- 152.4377201795578
- 127.08051067590714
- 114.82612037658691
- 135.40969198942184
- 140.8221538066864
- 121.51048749685287
- 123.39485555887222
- 129.4649237394333
- 126.41868555545807
- 130.67911624908447
- 143.9275678396225
- 130.45762705802917
- 126.54120022058487
- 118.61944878101349
- 131.5078091621399
- 121.25492078065872
- 113.47628164291382
- 131.21484053134918
- 114.16368812322617
- 131.40458726882935
- 133.14016032218933
- 113.88934588432312
- 120.98060411214828
- 133.47328466176987
- 115.58485698699951
- 146.48592948913574
- 122.86310982704163
- 127.06298810243607
- 152.12598383426666
- 135.8442079424858
- 171.62763768434525
- 130.0608240365982
- 118.38075745105743
- 120.65540683269501
- 123.05194264650345
- 136.19967955350876
- 132.20306015014648
- 136.52224099636078
- 142.32474094629288
train_accuracy:
- 0.162
- 0.075
- 0.9
- 0.26
- 0.017
- 0.05
- 0.03
- 0.067
- 0.55
- 0.742
- 0.685
- 0.771
- 0.987
- 0.8
- 0.969
- 0.282
- 0.917
- 0.9
- 0.7
- 0.592
- 0.292
- 0.333
- 0.012
- 0.95
- 0.872
- 0.542
- 0.5
- 0.904
- 0.577
- 0.787
- 0.375
- 0.963
- 0.568
- 0.894
- 0.9
- 0.828
- 0.236
- 0.132
- 0.958
- 0.85
- 0.838
- 0.912
- 0.817
- 0.9
- 0.914
- 0.639
- 0.732
- 0.381
- 0.604
- 0.675
- 0.895
- 0.703
- 0.9
- 0.714
- 0.828
- 0.854
- 0.564
- 0.858
- 0.85
- 0.767
- 0.612
- 0.438
- 0.753
- 0.625
- 0.726
- 0.296
- 0.739
- 0.836
- 0.691
- 0.866
- 0.622
- 0.925
- 0.547
- 0.712
- 0.835
- 0.935
- 0.913
- 0.0
- 0.769
- 0.919
- 0.888
- 0.975
- 0.733
- 0.97
- 0.75
- 0.829
- 0.659
- 0.806
- 0.888
- 0.5
- 0.261
- 0.603
- 0.292
- 0.95
- 0.375
- 0.508
- 0.6
- 0.656
- 0.939
- 0.917
train_loss:
- 1.219
- 0.807
- 0.58
- 0.677
- 0.555
- 0.509
- 0.389
- 0.538
- 0.507
- 0.436
- 0.477
- 0.488
- 0.425
- 0.427
- 0.412
- 0.544
- 0.463
- 0.337
- 0.399
- 0.432
- 0.392
- 0.341
- 0.427
- 0.361
- 0.391
- 0.396
- 0.444
- 0.36
- 0.396
- 0.396
- 0.378
- 0.413
- 0.345
- 0.435
- 0.351
- 0.361
- 0.357
- 0.489
- 0.392
- 0.418
- 0.368
- 0.339
- 0.39
- 0.394
- 0.361
- 0.382
- 0.307
- 0.35
- 0.328
- 0.388
- 0.432
- 0.432
- 0.368
- 0.34
- 0.265
- 0.399
- 0.372
- 0.376
- 0.387
- 0.36
- 0.374
- 0.37
- 0.377
- 0.344
- 0.439
- 0.278
- 0.376
- 0.334
- 0.348
- 0.373
- 0.313
- 0.362
- 0.378
- 0.359
- 0.301
- 0.414
- 0.373
- 0.325
- 0.363
- 0.321
- 0.317
- 0.278
- 0.297
- 0.276
- 0.349
- 0.353
- 0.434
- 0.317
- 0.397
- 0.34
- 0.372
- 0.376
- 0.287
- 0.291
- 0.364
- 0.34
- 0.362
- 0.237
- 0.345
- 0.361
unequal: 1
verbose: 1
