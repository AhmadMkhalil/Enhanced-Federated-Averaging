avg_train_accuracy: 0.529
avg_train_loss: 0.004
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.12462765957446809
- 0.19234042553191488
- 0.30840425531914895
- 0.34117021276595744
- 0.3481382978723404
- 0.41617021276595745
- 0.43920212765957445
- 0.4796808510638298
- 0.46664893617021275
- 0.5138829787234043
- 0.5041489361702127
- 0.5588829787234042
- 0.5355851063829787
- 0.5412765957446809
- 0.5624468085106383
- 0.5937765957446809
- 0.6173404255319149
- 0.5644148936170212
- 0.6165957446808511
- 0.5423404255319149
- 0.5642021276595744
- 0.6198936170212765
- 0.5668617021276596
- 0.6601595744680852
- 0.575531914893617
- 0.5935106382978723
- 0.5756382978723404
- 0.637127659574468
- 0.6013297872340425
- 0.6762765957446808
- 0.6136702127659575
- 0.63
- 0.6749468085106383
- 0.6561702127659574
- 0.6298404255319149
- 0.6463829787234042
- 0.6059042553191489
- 0.6552659574468085
- 0.6720212765957447
- 0.6549468085106382
- 0.6507978723404255
- 0.700531914893617
- 0.6438297872340426
- 0.6710106382978723
- 0.5967021276595744
- 0.6612234042553191
- 0.6635106382978724
- 0.6704255319148936
- 0.6623936170212766
- 0.6504255319148936
- 0.6537765957446808
- 0.6193085106382978
- 0.6438829787234043
- 0.6601595744680852
- 0.6773936170212767
- 0.6759574468085107
- 0.6554255319148936
- 0.691595744680851
- 0.6157446808510638
- 0.6645744680851063
- 0.6303723404255319
- 0.6576063829787234
- 0.6712765957446809
- 0.6261702127659574
- 0.675531914893617
- 0.6597340425531915
- 0.6536702127659575
- 0.660372340425532
- 0.6498404255319149
- 0.661063829787234
- 0.6856914893617021
- 0.671063829787234
- 0.6339893617021276
- 0.638563829787234
- 0.6315957446808511
- 0.6417021276595745
- 0.6731914893617021
- 0.6727127659574468
- 0.6936702127659574
- 0.7147872340425532
- 0.6706382978723404
- 0.5930319148936171
- 0.6394148936170213
- 0.6971276595744681
- 0.6986170212765958
- 0.7098936170212766
- 0.6542553191489362
- 0.6947340425531915
- 0.6601063829787234
- 0.6772872340425532
- 0.7017021276595745
- 0.6752127659574468
- 0.6663297872340426
- 0.7026595744680851
- 0.6670212765957447
- 0.6567553191489361
- 0.6976595744680851
- 0.6695744680851063
- 0.6781914893617021
- 0.6433510638297872
test_loss_list:
- 533.3010821342468
- 461.6774911880493
- 408.4399778842926
- 366.266476392746
- 348.8717956542969
- 304.5283763408661
- 277.43144059181213
- 257.8649436235428
- 286.7748246192932
- 255.34323263168335
- 238.04575610160828
- 214.26225399971008
- 209.56185257434845
- 209.1897772550583
- 183.69360220432281
- 178.57985150814056
- 168.38592946529388
- 187.82208448648453
- 160.52362060546875
- 190.4293999671936
- 182.1125825047493
- 160.38059163093567
- 186.98325610160828
- 150.76485949754715
- 180.973286151886
- 174.10897082090378
- 176.9338260293007
- 151.82452166080475
- 163.07053089141846
- 138.114805996418
- 172.3486499786377
- 160.44363087415695
- 138.05194664001465
- 149.1843944787979
- 151.97234499454498
- 150.91945815086365
- 160.65486800670624
- 152.08401507139206
- 148.91312497854233
- 154.29579174518585
- 145.22720462083817
- 126.28567653894424
- 139.68467539548874
- 142.74121522903442
- 169.39821964502335
- 139.32569348812103
- 140.70527654886246
- 138.3890813589096
- 136.3413646221161
- 147.7866462469101
- 137.16843247413635
- 166.6640317440033
- 151.9365326166153
- 150.7673391699791
- 133.93186128139496
- 136.8955815434456
- 143.81571054458618
- 131.05515027046204
- 160.49213701486588
- 146.46802592277527
- 140.9630725979805
- 143.7446392774582
- 144.81866627931595
- 160.06839483976364
- 139.05674225091934
- 141.46853840351105
- 159.9214580655098
- 150.4642657637596
- 145.18554097414017
- 136.62064623832703
- 130.24041491746902
- 139.83043003082275
- 166.00359654426575
- 160.1437847018242
- 155.17323219776154
- 155.24011886119843
- 131.35696601867676
- 128.23756384849548
- 127.25216686725616
- 122.68917602300644
- 138.69207841157913
- 167.95621144771576
- 145.99530732631683
- 126.85545641183853
- 118.86265122890472
- 119.41764014959335
- 141.8626766204834
- 126.24631333351135
- 139.94737631082535
- 129.83676648139954
- 128.91344624757767
- 130.67926961183548
- 141.60832220315933
- 127.97487372159958
- 137.9783946275711
- 145.9922069311142
- 134.52282971143723
- 140.63289314508438
- 137.0926434993744
- 143.78144639730453
train_accuracy:
- 0.033
- 0.158
- 0.0
- 0.046
- 0.046
- 0.077
- 0.8
- 0.513
- 0.618
- 0.356
- 0.545
- 0.55
- 0.54
- 0.455
- 0.375
- 0.925
- 0.939
- 0.64
- 0.45
- 0.85
- 0.9
- 0.72
- 0.945
- 0.36
- 0.267
- 0.963
- 0.205
- 0.709
- 0.575
- 0.937
- 0.892
- 0.74
- 0.726
- 0.475
- 0.607
- 0.738
- 0.742
- 0.413
- 0.804
- 0.607
- 0.8
- 0.95
- 0.389
- 0.975
- 0.9
- 0.87
- 0.822
- 0.648
- 0.875
- 0.936
- 0.983
- 0.92
- 0.907
- 0.06
- 0.031
- 0.925
- 0.78
- 0.647
- 0.795
- 0.577
- 0.337
- 0.721
- 0.192
- 0.975
- 0.85
- 0.975
- 0.575
- 0.514
- 0.96
- 0.578
- 0.87
- 0.674
- 0.636
- 0.644
- 0.72
- 0.862
- 0.85
- 0.98
- 0.375
- 0.858
- 0.88
- 0.927
- 0.609
- 0.803
- 0.571
- 0.754
- 0.818
- 0.724
- 0.945
- 0.776
- 0.48
- 0.88
- 0.783
- 0.86
- 0.781
- 0.319
- 0.9
- 0.283
- 0.818
- 0.529
train_loss:
- 1.18
- 0.733
- 0.591
- 0.632
- 0.547
- 0.527
- 0.541
- 0.572
- 0.566
- 0.437
- 0.441
- 0.414
- 0.449
- 0.472
- 0.433
- 0.448
- 0.39
- 0.336
- 0.415
- 0.39
- 0.327
- 0.422
- 0.408
- 0.317
- 0.417
- 0.363
- 0.443
- 0.354
- 0.37
- 0.372
- 0.378
- 0.374
- 0.371
- 0.422
- 0.395
- 0.304
- 0.396
- 0.352
- 0.353
- 0.35
- 0.408
- 0.283
- 0.337
- 0.39
- 0.299
- 0.345
- 0.339
- 0.328
- 0.278
- 0.315
- 0.249
- 0.326
- 0.401
- 0.322
- 0.389
- 0.34
- 0.304
- 0.299
- 0.4
- 0.332
- 0.363
- 0.344
- 0.293
- 0.342
- 0.31
- 0.366
- 0.329
- 0.311
- 0.338
- 0.336
- 0.33
- 0.322
- 0.32
- 0.379
- 0.36
- 0.343
- 0.344
- 0.358
- 0.295
- 0.377
- 0.342
- 0.378
- 0.413
- 0.313
- 0.266
- 0.326
- 0.361
- 0.305
- 0.305
- 0.308
- 0.293
- 0.334
- 0.377
- 0.234
- 0.363
- 0.307
- 0.328
- 0.284
- 0.285
- 0.369
unequal: 1
verbose: 1
