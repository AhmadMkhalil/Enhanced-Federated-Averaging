avg_train_accuracy: 0.783
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1773936170212766
- 0.23962765957446808
- 0.2570212765957447
- 0.4098936170212766
- 0.37090425531914895
- 0.4263297872340426
- 0.4559042553191489
- 0.4782446808510638
- 0.522127659574468
- 0.5228191489361702
- 0.49377659574468086
- 0.5243617021276595
- 0.5847872340425532
- 0.5915425531914894
- 0.5601595744680851
- 0.6081382978723404
- 0.6260638297872341
- 0.5836170212765958
- 0.616436170212766
- 0.5892553191489361
- 0.6282446808510638
- 0.6456382978723404
- 0.6406914893617022
- 0.6908510638297872
- 0.6299468085106383
- 0.6236170212765958
- 0.6505851063829787
- 0.6331382978723404
- 0.6421808510638298
- 0.6643085106382979
- 0.680372340425532
- 0.6143085106382978
- 0.6331914893617021
- 0.6300531914893617
- 0.678031914893617
- 0.6498936170212766
- 0.6597872340425532
- 0.670372340425532
- 0.6734574468085106
- 0.6863829787234043
- 0.6700531914893617
- 0.6339361702127659
- 0.65
- 0.6372872340425532
- 0.6360106382978723
- 0.6770744680851064
- 0.6679787234042553
- 0.6707446808510639
- 0.6460638297872341
- 0.6493085106382979
- 0.6518617021276596
- 0.6246808510638298
- 0.6404255319148936
- 0.6420744680851064
- 0.6750531914893617
- 0.6481382978723405
- 0.635904255319149
- 0.688031914893617
- 0.6473936170212766
- 0.6724468085106383
- 0.6614893617021277
- 0.6906382978723404
- 0.663031914893617
- 0.716063829787234
- 0.7109042553191489
- 0.7038829787234042
- 0.7002127659574469
- 0.6940425531914893
- 0.6781382978723405
- 0.6551063829787234
- 0.6978723404255319
- 0.7013297872340426
- 0.7177659574468085
- 0.6800531914893617
- 0.6717021276595745
- 0.6841489361702128
- 0.7109574468085106
- 0.7147340425531915
- 0.6614893617021277
- 0.718563829787234
- 0.7271808510638298
- 0.6888297872340425
- 0.7062765957446808
- 0.6764893617021277
- 0.6851063829787234
- 0.6798936170212766
- 0.676063829787234
- 0.7404255319148936
- 0.6782978723404255
- 0.6663829787234042
- 0.6800531914893617
- 0.7117021276595744
- 0.6967021276595745
- 0.6865425531914894
- 0.6619148936170213
- 0.6655851063829787
- 0.7081382978723404
- 0.6401063829787234
- 0.6682978723404255
- 0.6555319148936171
test_loss_list:
- 535.3271255493164
- 462.53501749038696
- 410.66346621513367
- 330.20307302474976
- 327.5496233701706
- 298.3689833879471
- 273.6135364770889
- 265.7323981523514
- 239.9223747253418
- 222.33052825927734
- 236.65464842319489
- 237.94365632534027
- 200.90484416484833
- 197.2372272014618
- 201.77113246917725
- 180.57953530550003
- 175.670725107193
- 183.65360021591187
- 182.20061296224594
- 179.35601323843002
- 170.84941548109055
- 155.34587514400482
- 153.32314312458038
- 150.24137961864471
- 162.1025834083557
- 156.3256841301918
- 156.908981859684
- 163.06556630134583
- 152.1374792456627
- 155.79629182815552
- 139.2588163614273
- 163.08469849824905
- 165.57105839252472
- 161.64621204137802
- 147.01277887821198
- 153.54292047023773
- 144.79408931732178
- 142.78522926568985
- 150.9327838420868
- 140.74970883131027
- 153.21962535381317
- 155.1096515059471
- 144.0364227294922
- 152.6437427997589
- 149.95143753290176
- 132.43166798353195
- 139.4514303803444
- 142.5538266301155
- 148.96327435970306
- 142.93948858976364
- 142.12652730941772
- 161.37016755342484
- 149.01795703172684
- 149.68407440185547
- 147.9481605887413
- 146.10375982522964
- 148.5845497250557
- 134.07401901483536
- 140.3959087729454
- 140.25999850034714
- 138.74077481031418
- 139.54234302043915
- 147.1662404537201
- 130.05103713274002
- 128.65935957431793
- 144.6623175740242
- 126.82541114091873
- 127.26064747571945
- 127.29558783769608
- 143.19795686006546
- 125.3308829665184
- 125.14160186052322
- 123.58762848377228
- 131.18623489141464
- 137.5202381014824
- 125.44475334882736
- 119.57269197702408
- 122.3841518163681
- 140.62687242031097
- 118.95666640996933
- 115.3331390619278
- 123.36133790016174
- 116.85849678516388
- 135.31841951608658
- 132.93871062994003
- 145.31184417009354
- 142.10690760612488
- 122.3460545539856
- 135.02173829078674
- 146.92876935005188
- 137.15307343006134
- 127.12795740365982
- 133.88637119531631
- 133.96108305454254
- 141.6245829463005
- 136.78363174200058
- 118.03348529338837
- 156.40859258174896
- 130.8781220316887
- 136.83613914251328
train_accuracy:
- 0.3
- 0.111
- 0.1
- 0.225
- 0.433
- 0.371
- 0.8
- 0.921
- 0.817
- 0.893
- 0.888
- 0.012
- 0.733
- 0.964
- 0.483
- 0.227
- 0.612
- 0.16
- 0.869
- 0.715
- 0.597
- 0.045
- 0.743
- 0.714
- 0.856
- 0.792
- 0.275
- 0.888
- 0.889
- 0.795
- 0.743
- 0.95
- 0.385
- 0.493
- 0.559
- 0.143
- 0.706
- 0.647
- 0.893
- 0.7
- 0.787
- 0.842
- 0.48
- 0.875
- 0.727
- 0.697
- 0.5
- 0.65
- 0.843
- 0.826
- 0.72
- 0.553
- 0.867
- 0.443
- 0.807
- 0.795
- 0.572
- 0.462
- 0.925
- 0.39
- 0.183
- 1.0
- 0.774
- 0.928
- 0.942
- 0.692
- 0.564
- 0.938
- 0.694
- 0.679
- 0.312
- 0.975
- 0.85
- 0.753
- 0.756
- 0.789
- 0.673
- 0.85
- 0.496
- 0.745
- 0.489
- 0.606
- 0.987
- 0.794
- 0.738
- 0.844
- 0.864
- 0.897
- 0.608
- 0.57
- 0.58
- 0.836
- 0.772
- 0.938
- 0.818
- 0.683
- 0.113
- 0.185
- 0.963
- 0.783
train_loss:
- 1.056
- 0.861
- 0.614
- 0.632
- 0.533
- 0.548
- 0.402
- 0.423
- 0.457
- 0.56
- 0.364
- 0.289
- 0.445
- 0.406
- 0.441
- 0.455
- 0.416
- 0.484
- 0.444
- 0.375
- 0.342
- 0.385
- 0.321
- 0.452
- 0.33
- 0.324
- 0.441
- 0.428
- 0.413
- 0.406
- 0.38
- 0.343
- 0.411
- 0.335
- 0.396
- 0.469
- 0.456
- 0.321
- 0.334
- 0.393
- 0.267
- 0.357
- 0.307
- 0.319
- 0.279
- 0.394
- 0.416
- 0.441
- 0.363
- 0.432
- 0.388
- 0.418
- 0.286
- 0.352
- 0.433
- 0.309
- 0.487
- 0.349
- 0.347
- 0.374
- 0.344
- 0.361
- 0.315
- 0.402
- 0.37
- 0.403
- 0.324
- 0.331
- 0.379
- 0.311
- 0.379
- 0.312
- 0.381
- 0.436
- 0.322
- 0.355
- 0.392
- 0.31
- 0.372
- 0.302
- 0.336
- 0.287
- 0.316
- 0.321
- 0.354
- 0.359
- 0.307
- 0.409
- 0.409
- 0.262
- 0.33
- 0.262
- 0.413
- 0.319
- 0.362
- 0.309
- 0.349
- 0.382
- 0.41
- 0.292
unequal: 1
verbose: 1
