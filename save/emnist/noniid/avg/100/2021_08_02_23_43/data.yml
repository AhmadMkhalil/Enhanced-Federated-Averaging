avg_train_accuracy: 0.587
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.16138297872340426
- 0.2531914893617021
- 0.2983510638297872
- 0.3880851063829787
- 0.395531914893617
- 0.48138297872340424
- 0.5266489361702128
- 0.48648936170212764
- 0.5170744680851064
- 0.5532978723404255
- 0.5616489361702127
- 0.5635106382978723
- 0.5555851063829788
- 0.5551063829787234
- 0.6003723404255319
- 0.5923404255319149
- 0.6145212765957446
- 0.6314893617021277
- 0.665372340425532
- 0.6261702127659574
- 0.589095744680851
- 0.5753191489361702
- 0.6464893617021277
- 0.6057978723404255
- 0.645904255319149
- 0.6188297872340426
- 0.649468085106383
- 0.6423404255319148
- 0.6390425531914894
- 0.6439893617021276
- 0.5913297872340425
- 0.6486170212765957
- 0.6168617021276596
- 0.6366489361702128
- 0.59
- 0.6381914893617021
- 0.642127659574468
- 0.626436170212766
- 0.6555319148936171
- 0.6524468085106383
- 0.6654787234042553
- 0.6796276595744681
- 0.6277127659574468
- 0.6748404255319149
- 0.6026595744680852
- 0.6289361702127659
- 0.5890425531914893
- 0.6248936170212765
- 0.6505851063829787
- 0.6609574468085107
- 0.6676595744680851
- 0.6127659574468085
- 0.6618617021276596
- 0.6402127659574468
- 0.693936170212766
- 0.669468085106383
- 0.6284042553191489
- 0.6343617021276595
- 0.6735106382978724
- 0.619095744680851
- 0.6648404255319149
- 0.6835638297872341
- 0.6233510638297872
- 0.6211170212765957
- 0.6402659574468085
- 0.6960106382978724
- 0.6508510638297872
- 0.7276063829787234
- 0.6730851063829787
- 0.6246808510638298
- 0.6675
- 0.6962234042553191
- 0.7115957446808511
- 0.6626595744680851
- 0.6618085106382978
- 0.6718085106382978
- 0.665372340425532
- 0.6505319148936171
- 0.7057446808510638
- 0.6918085106382978
- 0.685531914893617
- 0.6592021276595744
- 0.7066489361702127
- 0.7050531914893617
- 0.6820744680851064
- 0.7145744680851064
- 0.6801063829787234
- 0.6494148936170213
- 0.6894148936170212
- 0.6814361702127659
- 0.6702659574468085
- 0.6374468085106383
- 0.6743085106382979
- 0.6908510638297872
- 0.6943617021276596
- 0.7270744680851063
- 0.6620212765957447
- 0.6916489361702127
- 0.6554787234042553
- 0.6957446808510638
test_loss_list:
- 534.9344193935394
- 457.3500874042511
- 396.0500774383545
- 334.46718513965607
- 321.9283837080002
- 268.88097631931305
- 243.02721095085144
- 235.14265370368958
- 227.55997490882874
- 216.81077873706818
- 202.40153789520264
- 210.5400949716568
- 220.60561525821686
- 211.858948469162
- 177.4449119567871
- 177.40038520097733
- 170.61352843046188
- 162.26023602485657
- 171.94616681337357
- 163.58723467588425
- 170.47183072566986
- 176.97475844621658
- 155.8468849658966
- 164.34282076358795
- 156.97839957475662
- 155.4891620874405
- 148.22693532705307
- 148.06926655769348
- 157.99604082107544
- 152.50073713064194
- 176.92060244083405
- 148.9617332816124
- 184.71771574020386
- 167.3936458826065
- 171.8182384967804
- 156.5193446278572
- 148.24414736032486
- 157.0847653746605
- 148.5365327000618
- 147.425577044487
- 146.5037367939949
- 139.3819192647934
- 151.2915745973587
- 155.67117428779602
- 168.07306283712387
- 146.81182026863098
- 156.22934025526047
- 150.4259557723999
- 147.56025314331055
- 144.01359117031097
- 145.65258210897446
- 174.37002658843994
- 141.74715608358383
- 166.45778572559357
- 133.78207343816757
- 153.34036058187485
- 149.09601354599
- 154.7419273853302
- 137.71769058704376
- 158.49335914850235
- 149.2340418100357
- 128.71119076013565
- 157.26904153823853
- 164.44336467981339
- 144.69371140003204
- 136.34802854061127
- 136.08654195070267
- 118.40793967247009
- 134.58488553762436
- 144.06464517116547
- 135.74901312589645
- 129.14019757509232
- 120.46662163734436
- 137.6105043888092
- 138.1919264793396
- 135.5616239309311
- 130.15841281414032
- 143.19282734394073
- 119.33266949653625
- 130.10176622867584
- 127.94058030843735
- 133.6561998128891
- 119.34126907587051
- 119.54492574930191
- 137.87213343381882
- 123.88082194328308
- 127.34413784742355
- 132.08791303634644
- 130.25001788139343
- 138.42323780059814
- 135.24654948711395
- 140.50511771440506
- 128.970357298851
- 139.77477151155472
- 134.17500859498978
- 122.20371049642563
- 130.56882584095
- 127.8242946267128
- 156.6587507724762
- 127.44783985614777
train_accuracy:
- 0.174
- 0.317
- 0.595
- 0.336
- 0.525
- 0.711
- 0.339
- 0.083
- 0.729
- 0.796
- 0.336
- 0.832
- 0.76
- 0.486
- 0.673
- 0.717
- 0.21
- 0.35
- 0.608
- 0.514
- 0.612
- 0.933
- 0.65
- 0.513
- 0.808
- 0.658
- 0.879
- 0.589
- 0.703
- 0.692
- 0.017
- 0.579
- 0.525
- 0.55
- 0.656
- 0.525
- 0.917
- 0.975
- 0.35
- 0.875
- 0.517
- 0.314
- 0.63
- 0.833
- 0.488
- 0.26
- 0.908
- 0.908
- 0.817
- 0.774
- 0.925
- 0.68
- 0.944
- 0.413
- 0.0
- 0.559
- 0.943
- 0.561
- 0.45
- 0.242
- 0.669
- 0.9
- 0.858
- 0.744
- 0.0
- 0.863
- 0.893
- 0.571
- 0.792
- 0.281
- 0.8
- 1.0
- 0.93
- 0.173
- 0.606
- 0.642
- 0.846
- 0.864
- 0.417
- 0.465
- 0.925
- 0.706
- 0.344
- 0.958
- 0.95
- 0.8
- 0.575
- 0.403
- 0.5
- 0.89
- 0.323
- 0.456
- 0.81
- 0.907
- 0.925
- 0.6
- 0.683
- 0.269
- 0.369
- 0.587
train_loss:
- 1.178
- 0.759
- 0.741
- 0.669
- 0.523
- 0.551
- 0.543
- 0.566
- 0.474
- 0.425
- 0.406
- 0.441
- 0.485
- 0.403
- 0.495
- 0.414
- 0.412
- 0.435
- 0.441
- 0.417
- 0.377
- 0.373
- 0.406
- 0.387
- 0.379
- 0.372
- 0.41
- 0.366
- 0.471
- 0.344
- 0.324
- 0.326
- 0.365
- 0.349
- 0.399
- 0.417
- 0.267
- 0.327
- 0.383
- 0.373
- 0.345
- 0.409
- 0.32
- 0.341
- 0.417
- 0.359
- 0.287
- 0.29
- 0.319
- 0.367
- 0.386
- 0.336
- 0.359
- 0.351
- 0.294
- 0.379
- 0.319
- 0.32
- 0.315
- 0.378
- 0.389
- 0.391
- 0.425
- 0.361
- 0.324
- 0.38
- 0.316
- 0.277
- 0.352
- 0.322
- 0.329
- 0.334
- 0.313
- 0.354
- 0.326
- 0.396
- 0.308
- 0.291
- 0.331
- 0.332
- 0.281
- 0.376
- 0.281
- 0.316
- 0.331
- 0.284
- 0.37
- 0.33
- 0.263
- 0.257
- 0.35
- 0.345
- 0.372
- 0.322
- 0.29
- 0.225
- 0.272
- 0.333
- 0.221
- 0.328
unequal: 1
verbose: 1
