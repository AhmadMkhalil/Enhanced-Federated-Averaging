avg_train_accuracy: 0.755
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.15345744680851064
- 0.2508510638297872
- 0.31186170212765957
- 0.38835106382978724
- 0.4356914893617021
- 0.44196808510638297
- 0.4895212765957447
- 0.554468085106383
- 0.5506382978723404
- 0.5295212765957447
- 0.5281914893617021
- 0.5720744680851064
- 0.614468085106383
- 0.5721276595744681
- 0.5752127659574469
- 0.5591489361702128
- 0.5478723404255319
- 0.6402659574468085
- 0.5956914893617021
- 0.5918085106382979
- 0.6361702127659574
- 0.63
- 0.6640957446808511
- 0.6336170212765957
- 0.6079255319148936
- 0.6593617021276595
- 0.665904255319149
- 0.678936170212766
- 0.615531914893617
- 0.631968085106383
- 0.6030319148936171
- 0.6534042553191489
- 0.6295744680851064
- 0.5906914893617021
- 0.5670744680851064
- 0.6467021276595745
- 0.6457446808510638
- 0.6773936170212767
- 0.6729787234042554
- 0.6567021276595745
- 0.6899468085106383
- 0.6261170212765957
- 0.6940425531914893
- 0.6731382978723405
- 0.6596276595744681
- 0.6646808510638298
- 0.6548936170212766
- 0.6441489361702127
- 0.6588297872340425
- 0.7003191489361702
- 0.6555851063829787
- 0.6907978723404256
- 0.6229255319148936
- 0.6643617021276595
- 0.6720212765957447
- 0.6299468085106383
- 0.6934574468085106
- 0.6125
- 0.6886702127659574
- 0.6786702127659574
- 0.683031914893617
- 0.6241489361702127
- 0.6851595744680851
- 0.6740957446808511
- 0.6307446808510638
- 0.6750531914893617
- 0.7143085106382979
- 0.6618617021276596
- 0.6538829787234043
- 0.6638297872340425
- 0.711436170212766
- 0.6514893617021277
- 0.6743085106382979
- 0.6158510638297873
- 0.6856914893617021
- 0.6945212765957447
- 0.6978723404255319
- 0.698031914893617
- 0.7002127659574469
- 0.6922872340425532
- 0.6753191489361702
- 0.6892021276595744
- 0.7056382978723404
- 0.6707446808510639
- 0.6437765957446808
- 0.6741489361702128
- 0.6600531914893617
- 0.7228191489361702
- 0.7054255319148937
- 0.6309574468085106
- 0.6526063829787234
- 0.6945744680851064
- 0.648936170212766
- 0.7102127659574468
- 0.6791489361702128
- 0.6555851063829787
- 0.6886170212765957
- 0.6529787234042553
- 0.6711170212765958
- 0.7382978723404255
test_loss_list:
- 528.2185568809509
- 435.8881583213806
- 381.9662518501282
- 334.85043811798096
- 287.2547113895416
- 277.37812519073486
- 258.49088513851166
- 216.70418119430542
- 218.53777480125427
- 216.13369405269623
- 207.63666093349457
- 197.1638638973236
- 180.55823814868927
- 193.42843866348267
- 187.4760216474533
- 192.3256568312645
- 193.11392903327942
- 159.14188766479492
- 182.30318808555603
- 179.8440259695053
- 160.33287048339844
- 161.01384377479553
- 150.3901994228363
- 160.7143104672432
- 167.93278050422668
- 152.3863247036934
- 147.3351105451584
- 142.93959081172943
- 167.47208631038666
- 164.03336787223816
- 168.57245486974716
- 156.77580165863037
- 158.70300775766373
- 166.20123732089996
- 173.15614289045334
- 166.4405814409256
- 144.9345481991768
- 137.88738667964935
- 145.73112046718597
- 148.22619897127151
- 133.81184816360474
- 161.60364973545074
- 130.0576167702675
- 140.81225299835205
- 144.71484929323196
- 149.85724258422852
- 152.6992284655571
- 146.9934442639351
- 136.0649049282074
- 124.11946088075638
- 149.29639965295792
- 137.84387511014938
- 158.96991795301437
- 143.40225702524185
- 130.16046541929245
- 163.08182841539383
- 129.7301500439644
- 153.17327570915222
- 128.0910700559616
- 144.81588965654373
- 127.33417785167694
- 161.22110897302628
- 130.10407227277756
- 134.6435455083847
- 151.1901449561119
- 137.33589965105057
- 126.86315608024597
- 141.99534785747528
- 138.2889226078987
- 140.6982890367508
- 130.0124773979187
- 140.47677117586136
- 139.2510997056961
- 153.32412207126617
- 127.34931582212448
- 125.40638470649719
- 129.39293503761292
- 127.46926045417786
- 127.59693920612335
- 130.89594626426697
- 148.33383798599243
- 128.10659658908844
- 123.08089071512222
- 138.11542457342148
- 148.56268161535263
- 135.81351828575134
- 138.90266126394272
- 121.89132404327393
- 128.3100602030754
- 151.44994074106216
- 148.60019940137863
- 134.30551308393478
- 167.1536073088646
- 143.8186628818512
- 131.36019378900528
- 135.8955615758896
- 140.42299276590347
- 143.7129983305931
- 132.6530841588974
- 106.80308973789215
train_accuracy:
- 0.0
- 0.0
- 0.113
- 0.411
- 0.268
- 0.465
- 0.891
- 0.59
- 0.789
- 0.963
- 0.319
- 0.983
- 0.5
- 0.787
- 0.812
- 0.878
- 0.323
- 0.525
- 0.39
- 0.378
- 0.675
- 0.94
- 0.831
- 0.908
- 0.47
- 0.842
- 0.85
- 0.609
- 0.725
- 0.553
- 0.526
- 0.437
- 0.78
- 0.619
- 0.218
- 0.954
- 0.635
- 0.167
- 0.337
- 0.9
- 0.954
- 0.742
- 0.579
- 0.925
- 0.9
- 0.825
- 0.142
- 0.743
- 0.444
- 0.587
- 0.911
- 0.631
- 0.688
- 0.836
- 0.732
- 0.35
- 0.363
- 0.189
- 0.495
- 0.692
- 0.804
- 0.758
- 0.645
- 0.778
- 0.316
- 0.645
- 0.883
- 0.342
- 0.842
- 0.677
- 0.755
- 0.793
- 0.137
- 0.503
- 0.744
- 0.65
- 0.93
- 0.17
- 0.758
- 0.763
- 0.83
- 0.95
- 0.854
- 0.55
- 0.808
- 0.611
- 0.839
- 0.747
- 0.325
- 0.9
- 0.125
- 0.86
- 0.791
- 0.647
- 0.865
- 0.99
- 0.731
- 0.931
- 0.6
- 0.755
train_loss:
- 1.271
- 0.776
- 0.596
- 0.643
- 0.593
- 0.492
- 0.446
- 0.396
- 0.413
- 0.434
- 0.5
- 0.386
- 0.531
- 0.399
- 0.472
- 0.387
- 0.346
- 0.314
- 0.346
- 0.374
- 0.326
- 0.367
- 0.355
- 0.404
- 0.38
- 0.308
- 0.383
- 0.421
- 0.358
- 0.336
- 0.389
- 0.392
- 0.323
- 0.391
- 0.351
- 0.356
- 0.364
- 0.302
- 0.318
- 0.317
- 0.338
- 0.316
- 0.395
- 0.347
- 0.332
- 0.346
- 0.338
- 0.317
- 0.407
- 0.345
- 0.367
- 0.307
- 0.355
- 0.402
- 0.322
- 0.414
- 0.359
- 0.372
- 0.322
- 0.402
- 0.32
- 0.33
- 0.343
- 0.383
- 0.325
- 0.309
- 0.332
- 0.267
- 0.357
- 0.27
- 0.3
- 0.347
- 0.245
- 0.248
- 0.289
- 0.297
- 0.36
- 0.299
- 0.322
- 0.283
- 0.248
- 0.309
- 0.311
- 0.297
- 0.358
- 0.347
- 0.301
- 0.316
- 0.371
- 0.298
- 0.278
- 0.36
- 0.284
- 0.274
- 0.329
- 0.321
- 0.316
- 0.391
- 0.288
- 0.335
unequal: 1
verbose: 1
