avg_train_accuracy: 0.321
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.11792553191489362
- 0.2810106382978723
- 0.2699468085106383
- 0.3809574468085106
- 0.3632446808510638
- 0.4195212765957447
- 0.4070744680851064
- 0.511436170212766
- 0.4785106382978723
- 0.48829787234042554
- 0.5103191489361703
- 0.48127659574468085
- 0.5397872340425532
- 0.604095744680851
- 0.5997872340425532
- 0.6245744680851064
- 0.5720212765957446
- 0.6135106382978723
- 0.5752127659574469
- 0.5986170212765958
- 0.6295212765957446
- 0.5973936170212766
- 0.6
- 0.6087234042553191
- 0.5998936170212766
- 0.6273404255319149
- 0.6606382978723404
- 0.6428723404255319
- 0.5726063829787233
- 0.6340957446808511
- 0.6429255319148937
- 0.644468085106383
- 0.671063829787234
- 0.6292553191489362
- 0.6577659574468085
- 0.6023936170212766
- 0.5968617021276595
- 0.6683510638297873
- 0.6253723404255319
- 0.6076595744680852
- 0.6526595744680851
- 0.6556914893617021
- 0.6470744680851064
- 0.663563829787234
- 0.6993617021276596
- 0.668936170212766
- 0.6718085106382978
- 0.6301063829787235
- 0.6671276595744681
- 0.6567021276595745
- 0.6271808510638298
- 0.6456914893617022
- 0.6866489361702127
- 0.6458510638297872
- 0.6506914893617022
- 0.6427127659574469
- 0.6806914893617021
- 0.668031914893617
- 0.6532978723404256
- 0.727872340425532
- 0.6832446808510638
- 0.6674468085106383
- 0.7019148936170213
- 0.7000531914893617
- 0.695531914893617
- 0.6703191489361702
- 0.7114893617021276
- 0.6706382978723404
- 0.7163829787234043
- 0.6498404255319149
- 0.6685106382978724
- 0.6507446808510639
- 0.6826063829787234
- 0.7173936170212766
- 0.6957446808510638
- 0.6605851063829787
- 0.7012765957446808
- 0.6790957446808511
- 0.6679787234042553
- 0.6587765957446808
- 0.6779787234042554
- 0.7
- 0.6856914893617021
- 0.6715425531914894
- 0.67
- 0.6580851063829787
- 0.6895744680851064
- 0.6981914893617022
- 0.673031914893617
- 0.6758510638297872
- 0.690531914893617
- 0.6907978723404256
- 0.6406382978723404
- 0.6996808510638298
- 0.6716489361702128
- 0.6579787234042553
- 0.6664361702127659
- 0.6806914893617021
- 0.6783510638297873
- 0.6761702127659575
test_loss_list:
- 534.2474620342255
- 445.6487650871277
- 400.82015228271484
- 357.70160698890686
- 344.0964980125427
- 295.8486758470535
- 309.41510939598083
- 252.06601178646088
- 259.1455376148224
- 236.6467500925064
- 231.14118325710297
- 238.80597186088562
- 211.36035680770874
- 183.38723409175873
- 185.06552684307098
- 179.472762465477
- 172.66237938404083
- 170.44900727272034
- 190.11747974157333
- 188.59639155864716
- 171.91818594932556
- 164.58235985040665
- 178.33013862371445
- 163.06236231327057
- 173.11910569667816
- 157.5665316581726
- 148.14037531614304
- 158.88185793161392
- 177.98320245742798
- 154.6248841881752
- 154.7869828939438
- 155.30392760038376
- 150.6230548620224
- 163.0850419998169
- 149.6520345211029
- 175.42580664157867
- 170.0584324002266
- 143.23038530349731
- 155.18800377845764
- 158.4933136701584
- 151.76613515615463
- 141.72841542959213
- 140.80730378627777
- 150.03061020374298
- 128.38471138477325
- 133.94825041294098
- 140.31563425064087
- 149.08807057142258
- 139.73283350467682
- 147.1088868379593
- 153.53625798225403
- 146.83903127908707
- 127.6238107085228
- 147.0775740146637
- 159.18629676103592
- 165.20998829603195
- 135.99415528774261
- 131.75307482481003
- 135.60682982206345
- 119.6702750325203
- 132.85674315690994
- 143.9066440463066
- 133.01063364744186
- 133.13907450437546
- 132.45025277137756
- 139.38892602920532
- 133.4494707584381
- 138.8886363506317
- 125.67734295129776
- 148.59961646795273
- 134.3361399769783
- 144.10161596536636
- 130.62886941432953
- 118.00426375865936
- 128.42017942667007
- 138.40350127220154
- 129.1679888367653
- 132.93123757839203
- 142.45852476358414
- 139.79257196187973
- 142.58732056617737
- 129.73656606674194
- 132.69067054986954
- 126.99574011564255
- 141.27181148529053
- 142.60583835840225
- 135.9269888997078
- 125.28764247894287
- 138.7819037437439
- 132.240503013134
- 129.08542054891586
- 126.85979282855988
- 148.45661509037018
- 121.07674038410187
- 139.58886474370956
- 148.9008625149727
- 137.42484891414642
- 135.33408427238464
- 133.82871061563492
- 130.46090072393417
train_accuracy:
- 0.296
- 0.01
- 0.377
- 0.0
- 0.2
- 0.83
- 0.507
- 0.142
- 0.75
- 0.057
- 0.457
- 0.364
- 0.568
- 0.521
- 0.5
- 0.413
- 0.658
- 0.638
- 0.712
- 0.275
- 0.304
- 0.767
- 0.535
- 0.656
- 0.897
- 0.428
- 0.808
- 0.642
- 0.562
- 0.971
- 0.862
- 0.369
- 0.81
- 0.979
- 0.719
- 0.777
- 0.88
- 0.736
- 0.921
- 0.453
- 0.394
- 0.134
- 0.612
- 0.619
- 0.769
- 0.692
- 0.686
- 0.646
- 0.5
- 0.95
- 0.979
- 0.936
- 0.731
- 0.918
- 0.55
- 0.505
- 0.236
- 0.0
- 0.821
- 0.796
- 0.31
- 0.679
- 0.73
- 0.55
- 0.85
- 0.575
- 0.931
- 0.492
- 0.686
- 0.963
- 0.907
- 0.736
- 0.75
- 0.7
- 0.95
- 0.754
- 0.85
- 0.264
- 0.0
- 0.429
- 0.65
- 0.561
- 0.22
- 0.945
- 0.892
- 0.581
- 0.712
- 0.58
- 0.629
- 0.297
- 0.636
- 0.672
- 0.356
- 0.775
- 0.736
- 0.917
- 0.855
- 0.912
- 0.264
- 0.321
train_loss:
- 1.048
- 0.716
- 0.664
- 0.534
- 0.629
- 0.489
- 0.525
- 0.504
- 0.458
- 0.474
- 0.403
- 0.485
- 0.425
- 0.453
- 0.518
- 0.485
- 0.346
- 0.403
- 0.367
- 0.265
- 0.395
- 0.432
- 0.366
- 0.457
- 0.415
- 0.39
- 0.374
- 0.407
- 0.36
- 0.358
- 0.435
- 0.389
- 0.409
- 0.385
- 0.418
- 0.382
- 0.45
- 0.389
- 0.37
- 0.376
- 0.346
- 0.313
- 0.299
- 0.374
- 0.371
- 0.406
- 0.335
- 0.313
- 0.374
- 0.323
- 0.304
- 0.357
- 0.31
- 0.343
- 0.41
- 0.336
- 0.312
- 0.306
- 0.319
- 0.332
- 0.294
- 0.27
- 0.303
- 0.287
- 0.39
- 0.265
- 0.344
- 0.267
- 0.337
- 0.377
- 0.372
- 0.34
- 0.294
- 0.35
- 0.331
- 0.373
- 0.322
- 0.32
- 0.377
- 0.328
- 0.321
- 0.289
- 0.332
- 0.383
- 0.336
- 0.347
- 0.378
- 0.357
- 0.314
- 0.405
- 0.316
- 0.331
- 0.418
- 0.381
- 0.263
- 0.326
- 0.29
- 0.353
- 0.301
- 0.338
unequal: 1
verbose: 1
