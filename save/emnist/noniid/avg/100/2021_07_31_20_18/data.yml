avg_train_accuracy: 0.75
avg_train_loss: 0.004
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.11702127659574468
- 0.2428723404255319
- 0.30659574468085105
- 0.37127659574468086
- 0.4072872340425532
- 0.4127127659574468
- 0.5309574468085106
- 0.5695744680851064
- 0.5235106382978724
- 0.5407446808510639
- 0.54
- 0.536595744680851
- 0.5372872340425532
- 0.535904255319149
- 0.5813297872340426
- 0.5392021276595744
- 0.5563829787234043
- 0.6039361702127659
- 0.6554787234042553
- 0.6438829787234043
- 0.6108510638297873
- 0.653936170212766
- 0.5907446808510638
- 0.626436170212766
- 0.6062765957446808
- 0.6566489361702128
- 0.6732978723404255
- 0.5474468085106383
- 0.6210638297872341
- 0.663031914893617
- 0.6
- 0.606063829787234
- 0.6397340425531914
- 0.6446808510638298
- 0.6906382978723404
- 0.641436170212766
- 0.6767553191489362
- 0.6444148936170213
- 0.6787765957446809
- 0.6560106382978723
- 0.6455851063829787
- 0.6379255319148937
- 0.6074468085106383
- 0.6127127659574468
- 0.6901595744680851
- 0.684468085106383
- 0.6907978723404256
- 0.6553191489361702
- 0.6945212765957447
- 0.6474468085106383
- 0.6668617021276596
- 0.6547340425531915
- 0.6523936170212766
- 0.6187765957446808
- 0.6848936170212766
- 0.5939893617021277
- 0.6591489361702128
- 0.6886170212765957
- 0.6736702127659574
- 0.6364893617021277
- 0.7161702127659575
- 0.6312234042553192
- 0.6714361702127659
- 0.6729255319148936
- 0.6667553191489362
- 0.679468085106383
- 0.6745744680851063
- 0.6939893617021277
- 0.6916489361702127
- 0.71
- 0.681595744680851
- 0.710531914893617
- 0.6920212765957446
- 0.6596808510638298
- 0.6783510638297873
- 0.6567021276595745
- 0.6598404255319149
- 0.6627659574468086
- 0.6872340425531915
- 0.6878723404255319
- 0.6695212765957447
- 0.7005851063829788
- 0.6690425531914893
- 0.7047340425531915
- 0.6795744680851064
- 0.6824468085106383
- 0.7047872340425532
- 0.6770212765957446
- 0.6360106382978723
- 0.6768085106382978
- 0.6768617021276596
- 0.6506382978723404
- 0.7252127659574468
- 0.7232446808510639
- 0.7021808510638298
- 0.6836170212765957
- 0.6337234042553191
- 0.6293617021276596
- 0.6721808510638297
- 0.7025531914893617
test_loss_list:
- 534.8027293682098
- 454.01077365875244
- 377.7554523944855
- 330.9984452724457
- 304.6364586353302
- 297.8360289335251
- 233.48913025856018
- 226.0946878194809
- 220.19419729709625
- 210.3813945055008
- 210.87839829921722
- 203.96608781814575
- 193.4522681236267
- 219.0853101015091
- 183.81145161390305
- 206.685187458992
- 199.495041847229
- 171.1856677532196
- 169.18118298053741
- 156.79818272590637
- 167.40132492780685
- 152.16255009174347
- 169.70736545324326
- 165.91313552856445
- 165.1566140651703
- 154.56213915348053
- 144.5199625492096
- 190.6179119348526
- 169.47220212221146
- 147.9838318824768
- 171.59107547998428
- 168.58339816331863
- 151.19374072551727
- 151.87903267145157
- 135.3899171948433
- 156.24871069192886
- 139.0409216284752
- 153.4510978460312
- 136.50608402490616
- 145.6840821504593
- 152.81891828775406
- 151.20165634155273
- 163.13672477006912
- 160.27355068922043
- 142.52358883619308
- 138.06193089485168
- 132.34321415424347
- 143.35331243276596
- 134.1748042702675
- 156.51608788967133
- 143.8418357372284
- 134.89843702316284
- 146.10043841600418
- 169.87400603294373
- 130.42781883478165
- 169.9456992149353
- 143.4404581785202
- 134.18750995397568
- 147.4214209318161
- 162.61413127183914
- 132.45275020599365
- 154.559263586998
- 134.19907170534134
- 139.43700993061066
- 139.85009598731995
- 140.44869208335876
- 150.82565033435822
- 138.80060130357742
- 134.30993646383286
- 126.42606621980667
- 126.35473608970642
- 127.71876454353333
- 130.33071184158325
- 137.40786570310593
- 125.4673261642456
- 134.44169771671295
- 145.40693551301956
- 129.38345354795456
- 132.74458438158035
- 131.95538914203644
- 143.68750888109207
- 132.79535347223282
- 140.34248560667038
- 127.66251188516617
- 143.202177464962
- 136.3867855668068
- 131.9590846300125
- 124.3201864361763
- 158.36262166500092
- 131.06507110595703
- 143.01229578256607
- 140.86914294958115
- 113.01448756456375
- 111.22823497653008
- 122.40690326690674
- 129.14060074090958
- 150.91341984272003
- 145.7220520377159
- 130.90532767772675
- 134.96935033798218
train_accuracy:
- 0.003
- 0.212
- 0.15
- 0.585
- 0.336
- 0.575
- 0.631
- 0.843
- 0.717
- 0.605
- 0.42
- 0.262
- 0.675
- 0.544
- 0.417
- 0.8
- 0.508
- 0.607
- 0.961
- 0.822
- 0.83
- 0.493
- 0.265
- 0.492
- 1.0
- 0.938
- 0.843
- 0.256
- 0.539
- 0.822
- 0.741
- 0.562
- 0.642
- 0.221
- 0.615
- 0.883
- 0.583
- 0.92
- 0.322
- 0.815
- 0.617
- 0.904
- 0.625
- 0.539
- 0.558
- 0.81
- 0.585
- 0.55
- 0.728
- 0.91
- 0.229
- 0.714
- 0.737
- 0.783
- 0.783
- 0.623
- 0.917
- 0.5
- 0.787
- 0.72
- 0.8
- 0.728
- 0.875
- 0.25
- 0.258
- 0.906
- 0.597
- 0.583
- 0.65
- 0.824
- 0.872
- 0.816
- 0.7
- 0.794
- 0.708
- 0.933
- 0.327
- 0.778
- 0.6
- 0.113
- 0.58
- 0.55
- 0.429
- 0.06
- 0.562
- 0.969
- 0.9
- 0.694
- 0.25
- 0.729
- 0.42
- 0.986
- 0.91
- 0.757
- 0.666
- 0.511
- 0.419
- 0.281
- 0.891
- 0.75
train_loss:
- 1.196
- 0.748
- 0.6
- 0.621
- 0.658
- 0.475
- 0.47
- 0.539
- 0.459
- 0.499
- 0.398
- 0.422
- 0.454
- 0.503
- 0.439
- 0.438
- 0.364
- 0.427
- 0.45
- 0.334
- 0.251
- 0.384
- 0.353
- 0.391
- 0.277
- 0.258
- 0.379
- 0.406
- 0.358
- 0.348
- 0.369
- 0.423
- 0.438
- 0.424
- 0.409
- 0.31
- 0.35
- 0.285
- 0.331
- 0.307
- 0.25
- 0.402
- 0.416
- 0.377
- 0.388
- 0.366
- 0.331
- 0.276
- 0.36
- 0.31
- 0.303
- 0.332
- 0.271
- 0.356
- 0.357
- 0.372
- 0.33
- 0.308
- 0.357
- 0.38
- 0.4
- 0.292
- 0.321
- 0.349
- 0.359
- 0.357
- 0.253
- 0.379
- 0.301
- 0.444
- 0.329
- 0.372
- 0.24
- 0.341
- 0.318
- 0.314
- 0.302
- 0.307
- 0.372
- 0.368
- 0.34
- 0.285
- 0.338
- 0.253
- 0.324
- 0.319
- 0.363
- 0.343
- 0.318
- 0.279
- 0.257
- 0.288
- 0.374
- 0.286
- 0.238
- 0.314
- 0.363
- 0.339
- 0.305
- 0.391
unequal: 1
verbose: 1
