avg_train_accuracy: 0.8
avg_train_loss: 0.004
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.14154255319148937
- 0.2378723404255319
- 0.3148404255319149
- 0.3697340425531915
- 0.4370212765957447
- 0.4702659574468085
- 0.45191489361702125
- 0.5081382978723404
- 0.553031914893617
- 0.4836170212765957
- 0.5272872340425532
- 0.5150531914893617
- 0.5946276595744681
- 0.5573404255319149
- 0.49281914893617024
- 0.536595744680851
- 0.5525
- 0.5706914893617021
- 0.5613297872340426
- 0.6169148936170212
- 0.6001595744680851
- 0.6368617021276596
- 0.644468085106383
- 0.6130851063829788
- 0.5857978723404256
- 0.5885638297872341
- 0.6296276595744681
- 0.663031914893617
- 0.6167021276595744
- 0.6153723404255319
- 0.5660106382978723
- 0.6064893617021276
- 0.5894680851063829
- 0.6206914893617022
- 0.6284042553191489
- 0.5941489361702128
- 0.6167553191489362
- 0.6508510638297872
- 0.661063829787234
- 0.6829787234042554
- 0.6487234042553192
- 0.6977659574468085
- 0.6327127659574469
- 0.6534574468085106
- 0.668031914893617
- 0.6486702127659575
- 0.6660106382978723
- 0.6168085106382979
- 0.6697872340425531
- 0.6913297872340426
- 0.7103191489361702
- 0.6913829787234043
- 0.6522872340425532
- 0.6598404255319149
- 0.6488297872340425
- 0.6777659574468086
- 0.7068617021276595
- 0.6607978723404255
- 0.7130851063829787
- 0.6487765957446808
- 0.7339361702127659
- 0.6575531914893618
- 0.7082446808510638
- 0.6900531914893617
- 0.6687765957446808
- 0.6871276595744681
- 0.6606382978723404
- 0.6871276595744681
- 0.6807978723404255
- 0.6661170212765958
- 0.6587765957446808
- 0.6682446808510638
- 0.6971276595744681
- 0.6931382978723404
- 0.6907978723404256
- 0.7113297872340425
- 0.6779255319148936
- 0.6861702127659575
- 0.6786702127659574
- 0.6836702127659574
- 0.6890425531914893
- 0.7003723404255319
- 0.6997872340425532
- 0.6572340425531915
- 0.6948936170212766
- 0.6818617021276596
- 0.6563297872340426
- 0.7104255319148937
- 0.6894148936170212
- 0.6838829787234042
- 0.7013829787234043
- 0.7152127659574468
- 0.6850531914893617
- 0.6394148936170213
- 0.6838297872340425
- 0.7260106382978724
- 0.7061702127659575
- 0.6540957446808511
- 0.7038829787234042
- 0.7044148936170213
test_loss_list:
- 529.8399834632874
- 459.4072892665863
- 386.55557084083557
- 337.5045838356018
- 303.9301588535309
- 269.9622653722763
- 272.50400829315186
- 241.96849846839905
- 221.33122789859772
- 241.8829789161682
- 221.08028149604797
- 218.75410556793213
- 198.33420383930206
- 204.75856697559357
- 223.23981595039368
- 193.16645681858063
- 193.94751942157745
- 186.71355819702148
- 197.8590680360794
- 184.16601300239563
- 169.2864100933075
- 162.00634306669235
- 151.78959596157074
- 173.95860344171524
- 176.74151879549026
- 185.10441064834595
- 150.77596724033356
- 143.17409640550613
- 162.3554728627205
- 171.63191241025925
- 183.22359836101532
- 172.98975104093552
- 183.0814961194992
- 159.972826898098
- 153.45935821533203
- 166.8134824037552
- 160.37130171060562
- 139.03611075878143
- 140.4754285812378
- 140.1150570511818
- 141.54930520057678
- 128.9418523311615
- 149.09130728244781
- 147.87996405363083
- 150.22032648324966
- 145.55194944143295
- 137.79920077323914
- 161.8829618692398
- 149.38040244579315
- 136.49530589580536
- 121.05110025405884
- 134.23228323459625
- 148.20180523395538
- 140.93729543685913
- 144.30534082651138
- 134.48026448488235
- 124.92305111885071
- 148.71764314174652
- 126.2113790512085
- 147.89452850818634
- 115.56018590927124
- 141.20659697055817
- 127.14471989870071
- 130.95610362291336
- 138.73125660419464
- 130.86707705259323
- 151.40083694458008
- 130.43743473291397
- 127.81922912597656
- 139.33045315742493
- 149.88316309452057
- 137.69069641828537
- 131.27102768421173
- 133.3736835718155
- 130.11663848161697
- 121.02383202314377
- 130.55779832601547
- 124.62510931491852
- 134.6905727982521
- 127.4737246632576
- 132.14317625761032
- 146.64458972215652
- 119.82836538553238
- 146.2728288769722
- 127.49757426977158
- 129.56120336055756
- 138.1818572282791
- 132.39211511611938
- 140.3474639058113
- 133.2961287498474
- 123.8848186135292
- 124.72120499610901
- 130.58548349142075
- 142.81221771240234
- 132.82788944244385
- 118.76647526025772
- 124.18968534469604
- 141.69057404994965
- 126.08928143978119
- 122.10978955030441
train_accuracy:
- 0.52
- 0.0
- 0.008
- 0.806
- 0.075
- 0.181
- 0.153
- 0.135
- 0.812
- 0.825
- 0.85
- 0.47
- 0.823
- 0.7
- 0.586
- 0.279
- 0.547
- 0.727
- 0.669
- 0.87
- 0.162
- 0.734
- 0.853
- 0.586
- 0.75
- 0.919
- 0.638
- 0.519
- 0.707
- 0.9
- 0.317
- 0.229
- 0.346
- 0.65
- 0.815
- 0.137
- 0.0
- 0.42
- 0.963
- 0.218
- 0.889
- 0.856
- 0.754
- 0.577
- 0.771
- 0.587
- 0.375
- 0.664
- 0.588
- 0.369
- 0.812
- 0.53
- 0.025
- 0.678
- 0.805
- 0.828
- 0.877
- 0.229
- 0.429
- 0.562
- 0.773
- 0.831
- 0.869
- 0.85
- 0.987
- 0.636
- 0.175
- 0.581
- 0.556
- 0.644
- 0.525
- 0.282
- 0.646
- 0.75
- 0.486
- 0.614
- 0.507
- 0.533
- 0.84
- 0.38
- 0.694
- 0.921
- 0.912
- 0.394
- 0.433
- 0.51
- 0.364
- 0.83
- 0.586
- 0.595
- 0.821
- 0.881
- 0.95
- 0.727
- 0.938
- 0.517
- 0.627
- 0.32
- 0.877
- 0.8
train_loss:
- 1.009
- 0.891
- 0.806
- 0.605
- 0.436
- 0.501
- 0.569
- 0.637
- 0.457
- 0.49
- 0.357
- 0.397
- 0.451
- 0.488
- 0.46
- 0.428
- 0.418
- 0.416
- 0.466
- 0.372
- 0.382
- 0.359
- 0.396
- 0.435
- 0.436
- 0.441
- 0.353
- 0.387
- 0.431
- 0.327
- 0.389
- 0.332
- 0.303
- 0.435
- 0.392
- 0.376
- 0.464
- 0.314
- 0.295
- 0.317
- 0.416
- 0.375
- 0.344
- 0.397
- 0.382
- 0.359
- 0.349
- 0.353
- 0.385
- 0.371
- 0.34
- 0.383
- 0.377
- 0.402
- 0.379
- 0.333
- 0.345
- 0.392
- 0.381
- 0.23
- 0.317
- 0.409
- 0.361
- 0.3
- 0.318
- 0.345
- 0.418
- 0.312
- 0.303
- 0.392
- 0.305
- 0.326
- 0.353
- 0.364
- 0.26
- 0.38
- 0.389
- 0.351
- 0.316
- 0.3
- 0.4
- 0.313
- 0.364
- 0.35
- 0.383
- 0.31
- 0.404
- 0.31
- 0.372
- 0.375
- 0.325
- 0.293
- 0.371
- 0.3
- 0.416
- 0.258
- 0.282
- 0.304
- 0.397
- 0.371
unequal: 1
verbose: 1
