avg_train_accuracy: 0.663
avg_train_loss: 0.004
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.14111702127659576
- 0.23292553191489362
- 0.26654255319148934
- 0.3624468085106383
- 0.4024468085106383
- 0.3949468085106383
- 0.45776595744680854
- 0.5362234042553191
- 0.47882978723404257
- 0.533031914893617
- 0.5671276595744681
- 0.5587234042553192
- 0.5797340425531915
- 0.517127659574468
- 0.538563829787234
- 0.5836170212765958
- 0.5821808510638298
- 0.5916489361702127
- 0.556063829787234
- 0.5843085106382979
- 0.5956914893617021
- 0.5901595744680851
- 0.635531914893617
- 0.5947872340425532
- 0.6183510638297872
- 0.6007978723404256
- 0.6233510638297872
- 0.6119148936170212
- 0.6139893617021277
- 0.6567553191489361
- 0.6347872340425532
- 0.6272872340425532
- 0.6487234042553192
- 0.6496276595744681
- 0.6249468085106383
- 0.5959574468085106
- 0.663563829787234
- 0.5951595744680851
- 0.5960106382978724
- 0.5680851063829787
- 0.626968085106383
- 0.5742553191489361
- 0.6304787234042554
- 0.6435106382978724
- 0.6620212765957447
- 0.6080851063829787
- 0.5856914893617021
- 0.6742021276595744
- 0.5880319148936171
- 0.5976063829787234
- 0.6371808510638298
- 0.6587765957446808
- 0.6584574468085106
- 0.6475
- 0.6542021276595744
- 0.6554787234042553
- 0.6377127659574469
- 0.6579255319148937
- 0.6431914893617021
- 0.6733510638297873
- 0.6417021276595745
- 0.6604787234042553
- 0.6185106382978723
- 0.6901063829787234
- 0.5926063829787234
- 0.6297872340425532
- 0.664468085106383
- 0.6439893617021276
- 0.671063829787234
- 0.6281382978723404
- 0.6569680851063829
- 0.7075531914893617
- 0.6648404255319149
- 0.6823404255319149
- 0.6888297872340425
- 0.6660106382978723
- 0.6742553191489362
- 0.6603191489361702
- 0.6828723404255319
- 0.6954255319148936
- 0.6661702127659574
- 0.6026063829787234
- 0.6756382978723404
- 0.6720744680851064
- 0.6314893617021277
- 0.6101595744680851
- 0.658031914893617
- 0.6682978723404255
- 0.716968085106383
- 0.6972340425531914
- 0.6735638297872341
- 0.6754787234042553
- 0.6975531914893617
- 0.6902659574468085
- 0.6961170212765957
- 0.6847340425531915
- 0.6846276595744681
- 0.6798404255319149
- 0.6691489361702128
- 0.6667553191489362
test_loss_list:
- 537.1845047473907
- 454.2686860561371
- 403.39556789398193
- 347.17251992225647
- 322.56942534446716
- 291.6711605787277
- 254.98017179965973
- 218.82756876945496
- 233.9293234348297
- 214.02573001384735
- 199.46220242977142
- 222.07976257801056
- 190.32280588150024
- 210.94598495960236
- 206.68953216075897
- 185.54446065425873
- 182.63948464393616
- 173.84231686592102
- 191.12581765651703
- 183.15128749608994
- 170.40042239427567
- 173.77139085531235
- 162.28873872756958
- 178.32531476020813
- 161.26838105916977
- 172.35980236530304
- 158.3516862988472
- 163.1100576519966
- 157.643778860569
- 145.04614996910095
- 163.64042955636978
- 161.8156944513321
- 144.57993882894516
- 151.0948635339737
- 159.61767274141312
- 168.27363270521164
- 149.75159829854965
- 172.18961519002914
- 170.17553532123566
- 192.38707995414734
- 153.8345866203308
- 175.7361227273941
- 160.65422105789185
- 143.36197113990784
- 147.00449937582016
- 177.83988988399506
- 173.21565806865692
- 140.0043272972107
- 160.7273786664009
- 170.80928826332092
- 149.96277004480362
- 147.77116602659225
- 141.8824604153633
- 148.38917738199234
- 148.63336420059204
- 140.05535858869553
- 143.6111235022545
- 145.55912631750107
- 149.10905545949936
- 141.9897523522377
- 156.51545649766922
- 136.6308894753456
- 156.42099130153656
- 132.37287241220474
- 173.5595954656601
- 143.98862558603287
- 147.26684975624084
- 148.80748802423477
- 133.56564110517502
- 154.07291740179062
- 134.1807433962822
- 124.33726650476456
- 138.77909618616104
- 138.0264401435852
- 128.51413249969482
- 133.53149873018265
- 133.83190155029297
- 135.5078964829445
- 131.93384265899658
- 126.72323018312454
- 134.30289089679718
- 166.39773780107498
- 137.49774354696274
- 140.81863850355148
- 144.82261443138123
- 160.03536987304688
- 133.73943650722504
- 131.78298115730286
- 116.46285939216614
- 130.8712272644043
- 138.6247655749321
- 127.77210313081741
- 123.81690281629562
- 123.40780407190323
- 120.12538427114487
- 133.61922883987427
- 141.74960839748383
- 135.68573200702667
- 137.66255509853363
- 140.5713797211647
train_accuracy:
- 0.019
- 0.0
- 0.238
- 0.029
- 0.862
- 0.286
- 0.889
- 0.963
- 0.9
- 0.088
- 0.929
- 0.158
- 0.638
- 0.635
- 0.464
- 0.842
- 0.666
- 0.7
- 0.258
- 0.561
- 0.825
- 0.915
- 0.92
- 0.8
- 0.544
- 0.562
- 0.489
- 0.388
- 0.707
- 0.35
- 0.856
- 0.755
- 0.405
- 0.837
- 0.715
- 0.44
- 0.928
- 0.377
- 0.828
- 0.733
- 0.673
- 0.746
- 0.808
- 0.856
- 0.725
- 0.496
- 0.69
- 0.616
- 0.183
- 0.606
- 0.95
- 0.5
- 0.856
- 0.639
- 0.875
- 0.928
- 0.737
- 0.346
- 0.606
- 0.75
- 0.321
- 0.623
- 0.622
- 0.864
- 0.496
- 0.461
- 0.77
- 0.624
- 0.784
- 0.947
- 0.992
- 0.888
- 0.611
- 0.896
- 0.838
- 0.507
- 0.95
- 0.714
- 0.412
- 0.591
- 0.813
- 0.944
- 0.894
- 0.925
- 0.659
- 0.882
- 0.271
- 0.365
- 0.167
- 0.788
- 0.592
- 0.292
- 0.775
- 0.917
- 0.821
- 0.571
- 0.921
- 0.812
- 0.825
- 0.663
train_loss:
- 1.201
- 0.825
- 0.649
- 0.627
- 0.615
- 0.45
- 0.488
- 0.503
- 0.477
- 0.539
- 0.443
- 0.487
- 0.422
- 0.427
- 0.378
- 0.398
- 0.447
- 0.424
- 0.284
- 0.421
- 0.411
- 0.379
- 0.327
- 0.38
- 0.31
- 0.412
- 0.371
- 0.34
- 0.317
- 0.314
- 0.375
- 0.356
- 0.356
- 0.366
- 0.419
- 0.393
- 0.373
- 0.327
- 0.39
- 0.325
- 0.317
- 0.364
- 0.404
- 0.372
- 0.358
- 0.404
- 0.389
- 0.423
- 0.442
- 0.368
- 0.364
- 0.338
- 0.316
- 0.371
- 0.304
- 0.328
- 0.263
- 0.353
- 0.368
- 0.346
- 0.398
- 0.351
- 0.351
- 0.382
- 0.37
- 0.292
- 0.352
- 0.372
- 0.341
- 0.365
- 0.309
- 0.37
- 0.329
- 0.385
- 0.308
- 0.293
- 0.279
- 0.357
- 0.301
- 0.301
- 0.344
- 0.311
- 0.332
- 0.297
- 0.286
- 0.346
- 0.314
- 0.346
- 0.308
- 0.298
- 0.315
- 0.366
- 0.336
- 0.351
- 0.337
- 0.357
- 0.274
- 0.338
- 0.376
- 0.37
unequal: 1
verbose: 1
