avg_train_accuracy: 0.867
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.13324468085106383
- 0.25563829787234044
- 0.396968085106383
- 0.3406914893617021
- 0.48319148936170214
- 0.4441489361702128
- 0.5037765957446808
- 0.5387765957446808
- 0.5442553191489362
- 0.5603191489361702
- 0.5409574468085107
- 0.5897340425531915
- 0.6080851063829787
- 0.5754255319148937
- 0.5762234042553191
- 0.5466489361702128
- 0.5212765957446809
- 0.5586702127659574
- 0.6285106382978723
- 0.6378723404255319
- 0.6683510638297873
- 0.6841489361702128
- 0.6612234042553191
- 0.5908510638297872
- 0.6186170212765958
- 0.6112234042553192
- 0.598563829787234
- 0.6338829787234043
- 0.6477659574468085
- 0.6398936170212766
- 0.6327659574468085
- 0.660904255319149
- 0.6012765957446808
- 0.6901063829787234
- 0.5876595744680851
- 0.6240425531914894
- 0.6237234042553191
- 0.689627659574468
- 0.6661170212765958
- 0.6508510638297872
- 0.6348404255319149
- 0.6596276595744681
- 0.6302659574468085
- 0.6697872340425531
- 0.7036170212765958
- 0.6712765957446809
- 0.6403191489361703
- 0.6740957446808511
- 0.681595744680851
- 0.6632978723404256
- 0.7083510638297872
- 0.6936702127659574
- 0.7149468085106383
- 0.7109574468085106
- 0.686968085106383
- 0.6735638297872341
- 0.7062234042553192
- 0.6592021276595744
- 0.6766489361702127
- 0.6696276595744681
- 0.6563829787234042
- 0.6617021276595745
- 0.6177127659574468
- 0.6718617021276596
- 0.6947872340425532
- 0.6864361702127659
- 0.6442021276595745
- 0.6838297872340425
- 0.6728723404255319
- 0.6379787234042553
- 0.675
- 0.7036170212765958
- 0.6872340425531915
- 0.6946808510638298
- 0.6750531914893617
- 0.7282446808510639
- 0.7010106382978724
- 0.7099468085106383
- 0.6622340425531915
- 0.7102659574468085
- 0.6995212765957447
- 0.6903723404255319
- 0.6917553191489362
- 0.6927127659574468
- 0.6997872340425532
- 0.6893085106382979
- 0.6710106382978723
- 0.6658510638297872
- 0.6745212765957447
- 0.6695744680851063
- 0.6763829787234042
- 0.7212234042553192
- 0.7228191489361702
- 0.675372340425532
- 0.6757978723404255
- 0.6484042553191489
- 0.6981382978723404
- 0.7093617021276596
- 0.7044680851063829
- 0.6948936170212766
test_loss_list:
- 533.1915373802185
- 434.4303488731384
- 361.71482157707214
- 323.42269337177277
- 268.44505417346954
- 267.49946093559265
- 236.45176208019257
- 219.70683658123016
- 213.92829310894012
- 200.98557555675507
- 210.913050532341
- 186.3898960351944
- 173.94110828638077
- 191.44962120056152
- 186.19612157344818
- 194.0702382326126
- 214.05941712856293
- 194.4351043701172
- 170.9662642478943
- 168.83337092399597
- 145.5453855395317
- 144.60000532865524
- 145.09945404529572
- 177.87682902812958
- 170.86487185955048
- 170.50785678625107
- 166.80464881658554
- 150.8129000067711
- 150.22105133533478
- 157.53238773345947
- 150.5650544166565
- 146.89769262075424
- 162.1946193575859
- 133.30206376314163
- 179.41052132844925
- 154.16731441020966
- 153.12750643491745
- 142.08630901575089
- 140.36081951856613
- 143.7650238275528
- 157.61206698417664
- 140.99525022506714
- 150.80107575654984
- 135.49702441692352
- 130.6381686925888
- 139.71082991361618
- 143.8050992488861
- 137.10848861932755
- 135.2866080403328
- 142.94507902860641
- 130.9525997042656
- 128.5220412015915
- 116.36764353513718
- 128.9132947921753
- 130.34771567583084
- 139.29812055826187
- 124.70402175188065
- 138.76115649938583
- 132.27216988801956
- 136.68676614761353
- 139.1682027578354
- 140.43790316581726
- 154.57636904716492
- 131.94419795274734
- 132.77960336208344
- 128.01180869340897
- 146.61138159036636
- 125.0710032582283
- 131.5372633934021
- 151.9359148144722
- 129.3864803314209
- 125.29877007007599
- 138.57091450691223
- 136.4197900891304
- 134.1995558142662
- 117.93438273668289
- 125.45302683115005
- 117.82726240158081
- 135.23307818174362
- 126.22104561328888
- 130.73543512821198
- 132.94022023677826
- 127.47996979951859
- 123.25896745920181
- 118.51618927717209
- 124.48729574680328
- 132.73832231760025
- 137.98509496450424
- 129.69034618139267
- 131.24804282188416
- 130.72343170642853
- 118.24348002672195
- 115.8004110455513
- 141.5069464445114
- 138.3276070356369
- 141.44813680648804
- 120.16359108686447
- 115.87356477975845
- 123.25376933813095
- 134.57961118221283
train_accuracy:
- 0.306
- 0.418
- 0.736
- 0.417
- 0.0
- 0.315
- 0.794
- 0.025
- 0.129
- 1.0
- 0.594
- 0.754
- 0.833
- 0.531
- 0.511
- 0.7
- 0.727
- 0.025
- 0.85
- 0.8
- 0.853
- 0.597
- 0.717
- 0.411
- 0.287
- 0.838
- 0.667
- 0.95
- 0.6
- 0.908
- 0.414
- 0.288
- 0.863
- 0.843
- 0.833
- 0.394
- 0.614
- 0.897
- 0.697
- 0.856
- 0.582
- 0.9
- 0.733
- 0.99
- 0.671
- 0.288
- 0.85
- 0.98
- 0.912
- 0.496
- 0.44
- 0.917
- 0.906
- 0.846
- 0.89
- 0.7
- 0.9
- 0.921
- 0.657
- 0.513
- 0.563
- 0.694
- 0.594
- 0.76
- 0.867
- 0.9
- 0.705
- 0.743
- 0.669
- 0.917
- 0.233
- 0.556
- 0.871
- 0.861
- 0.791
- 0.859
- 0.88
- 0.829
- 0.873
- 0.85
- 0.764
- 0.838
- 0.703
- 0.96
- 0.733
- 0.65
- 0.686
- 0.666
- 0.772
- 0.907
- 0.933
- 0.9
- 0.911
- 0.55
- 0.956
- 0.523
- 0.421
- 0.925
- 0.89
- 0.867
train_loss:
- 1.264
- 0.785
- 0.705
- 0.552
- 0.558
- 0.597
- 0.517
- 0.501
- 0.488
- 0.445
- 0.442
- 0.513
- 0.376
- 0.531
- 0.476
- 0.373
- 0.427
- 0.351
- 0.403
- 0.332
- 0.411
- 0.434
- 0.436
- 0.377
- 0.419
- 0.346
- 0.38
- 0.308
- 0.382
- 0.409
- 0.396
- 0.414
- 0.38
- 0.369
- 0.363
- 0.388
- 0.355
- 0.315
- 0.318
- 0.364
- 0.372
- 0.431
- 0.32
- 0.354
- 0.394
- 0.319
- 0.376
- 0.412
- 0.355
- 0.355
- 0.365
- 0.374
- 0.38
- 0.406
- 0.417
- 0.388
- 0.422
- 0.384
- 0.372
- 0.325
- 0.353
- 0.44
- 0.359
- 0.316
- 0.31
- 0.315
- 0.315
- 0.271
- 0.411
- 0.306
- 0.256
- 0.301
- 0.317
- 0.353
- 0.449
- 0.304
- 0.314
- 0.351
- 0.241
- 0.31
- 0.289
- 0.281
- 0.306
- 0.254
- 0.3
- 0.464
- 0.423
- 0.302
- 0.373
- 0.299
- 0.313
- 0.311
- 0.298
- 0.295
- 0.376
- 0.25
- 0.318
- 0.325
- 0.267
- 0.33
unequal: 1
verbose: 1
