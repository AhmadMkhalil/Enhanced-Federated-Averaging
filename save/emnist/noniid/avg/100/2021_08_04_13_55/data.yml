avg_train_accuracy: 0.543
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1352659574468085
- 0.2924468085106383
- 0.289468085106383
- 0.34643617021276596
- 0.3943617021276596
- 0.37872340425531914
- 0.5063297872340425
- 0.49851063829787234
- 0.5261702127659574
- 0.5932446808510639
- 0.5395212765957447
- 0.5948404255319149
- 0.5480851063829787
- 0.5995744680851064
- 0.5492021276595744
- 0.5957978723404256
- 0.5617553191489362
- 0.6138829787234042
- 0.5810106382978724
- 0.6218617021276596
- 0.5940425531914894
- 0.6203191489361702
- 0.6487765957446808
- 0.6734574468085106
- 0.6481382978723405
- 0.6226063829787234
- 0.5881382978723404
- 0.6551063829787234
- 0.6810106382978723
- 0.6685106382978724
- 0.5653723404255319
- 0.5689893617021277
- 0.6121808510638298
- 0.5856914893617021
- 0.6505319148936171
- 0.6455851063829787
- 0.6645212765957447
- 0.6923404255319149
- 0.6463829787234042
- 0.6547340425531915
- 0.6478723404255319
- 0.6303723404255319
- 0.6538829787234043
- 0.6931914893617022
- 0.7011170212765957
- 0.6662765957446809
- 0.6737234042553192
- 0.6627127659574468
- 0.6733510638297873
- 0.6536702127659575
- 0.6657446808510639
- 0.6542021276595744
- 0.6432446808510638
- 0.6567021276595745
- 0.6781914893617021
- 0.6555851063829787
- 0.6696808510638298
- 0.6793617021276596
- 0.6975531914893617
- 0.658936170212766
- 0.6963829787234043
- 0.6547872340425532
- 0.6612765957446809
- 0.7060106382978724
- 0.6679255319148936
- 0.6978723404255319
- 0.6839893617021277
- 0.7021276595744681
- 0.7232978723404255
- 0.7111702127659575
- 0.6594148936170213
- 0.6412234042553191
- 0.705
- 0.6605851063829787
- 0.6735106382978724
- 0.656436170212766
- 0.7071276595744681
- 0.7096808510638298
- 0.6982978723404255
- 0.7080851063829787
- 0.7028191489361703
- 0.6738829787234043
- 0.7232978723404255
- 0.6524468085106383
- 0.7301595744680851
- 0.6596808510638298
- 0.6737765957446809
- 0.6741489361702128
- 0.6888829787234042
- 0.6928191489361702
- 0.6702127659574468
- 0.691063829787234
- 0.6517021276595745
- 0.7471276595744681
- 0.6914361702127659
- 0.6634042553191489
- 0.7001063829787234
- 0.7417021276595744
- 0.7020212765957446
- 0.6786702127659574
test_loss_list:
- 531.9661679267883
- 448.8872318267822
- 381.61731004714966
- 330.11118364334106
- 310.06715738773346
- 308.47987020015717
- 238.3501454591751
- 242.8938934803009
- 229.15502631664276
- 188.73797237873077
- 206.99759018421173
- 197.3052363395691
- 192.8747947216034
- 174.6142833828926
- 200.41659820079803
- 173.17047834396362
- 182.51531940698624
- 182.6196125149727
- 183.26640033721924
- 166.7520397901535
- 175.56727612018585
- 169.82603031396866
- 151.62418925762177
- 140.90844064950943
- 149.97816795110703
- 163.79296791553497
- 167.80760407447815
- 142.3800637125969
- 140.4026351571083
- 144.0646755695343
- 180.4675351381302
- 192.03245908021927
- 169.91913735866547
- 180.51497280597687
- 154.60428947210312
- 160.4129155278206
- 152.052590072155
- 135.2069553732872
- 148.61852890253067
- 149.04844135046005
- 153.5180003643036
- 168.47615933418274
- 153.0014090538025
- 137.06862044334412
- 124.33001857995987
- 142.4572330713272
- 134.83801156282425
- 137.73068660497665
- 128.68362492322922
- 147.16566836833954
- 148.39740562438965
- 152.3857142329216
- 155.35293060541153
- 156.12675082683563
- 145.18328821659088
- 155.13651877641678
- 139.42968600988388
- 139.72252696752548
- 125.73491144180298
- 141.47182220220566
- 124.56242978572845
- 140.76154160499573
- 139.94812619686127
- 128.66275882720947
- 135.63311421871185
- 126.96672886610031
- 133.3711760044098
- 131.3259579539299
- 123.11396652460098
- 125.80974394083023
- 134.0096852183342
- 145.47562927007675
- 125.66449671983719
- 148.11745953559875
- 134.71829503774643
- 139.7771231532097
- 123.07167857885361
- 122.81776881217957
- 123.73540151119232
- 118.32991248369217
- 126.07640022039413
- 131.18386179208755
- 120.41522353887558
- 141.0436052083969
- 118.00174939632416
- 155.71803259849548
- 136.43267476558685
- 138.12334483861923
- 132.09040451049805
- 130.19131630659103
- 138.54539626836777
- 137.87097585201263
- 142.04957008361816
- 107.60757613182068
- 128.02213162183762
- 139.7122079730034
- 122.9257727265358
- 113.0470644235611
- 124.89166021347046
- 138.90878969430923
train_accuracy:
- 0.0
- 0.425
- 0.032
- 0.595
- 0.114
- 0.168
- 0.519
- 0.0
- 0.75
- 0.814
- 0.313
- 0.823
- 0.857
- 0.419
- 0.587
- 0.933
- 0.967
- 0.908
- 0.375
- 0.805
- 0.086
- 0.497
- 0.893
- 0.668
- 0.912
- 0.409
- 0.817
- 0.9
- 0.765
- 0.489
- 0.545
- 0.234
- 0.83
- 0.792
- 0.637
- 0.689
- 0.803
- 0.856
- 0.367
- 0.75
- 0.762
- 0.558
- 0.816
- 0.668
- 0.917
- 0.455
- 0.538
- 0.579
- 0.582
- 0.114
- 0.834
- 0.388
- 0.392
- 0.874
- 0.563
- 0.55
- 0.308
- 0.826
- 0.547
- 0.95
- 0.933
- 0.987
- 0.35
- 0.763
- 0.479
- 0.663
- 0.6
- 0.722
- 0.892
- 0.581
- 0.029
- 0.394
- 0.667
- 0.239
- 0.795
- 0.816
- 0.756
- 0.3
- 0.975
- 0.875
- 0.894
- 0.592
- 0.85
- 0.914
- 0.883
- 0.685
- 0.894
- 0.9
- 0.625
- 0.154
- 0.707
- 0.308
- 0.575
- 0.695
- 0.846
- 0.721
- 0.708
- 0.884
- 0.062
- 0.543
train_loss:
- 1.154
- 0.817
- 0.772
- 0.675
- 0.58
- 0.517
- 0.476
- 0.52
- 0.451
- 0.465
- 0.491
- 0.394
- 0.416
- 0.407
- 0.33
- 0.37
- 0.381
- 0.404
- 0.334
- 0.284
- 0.37
- 0.436
- 0.276
- 0.411
- 0.392
- 0.403
- 0.355
- 0.323
- 0.413
- 0.438
- 0.402
- 0.285
- 0.356
- 0.405
- 0.353
- 0.409
- 0.412
- 0.391
- 0.383
- 0.348
- 0.35
- 0.399
- 0.371
- 0.348
- 0.343
- 0.424
- 0.336
- 0.368
- 0.322
- 0.296
- 0.378
- 0.394
- 0.33
- 0.342
- 0.374
- 0.337
- 0.387
- 0.424
- 0.39
- 0.345
- 0.352
- 0.239
- 0.425
- 0.323
- 0.275
- 0.3
- 0.285
- 0.382
- 0.354
- 0.33
- 0.343
- 0.348
- 0.417
- 0.428
- 0.326
- 0.285
- 0.356
- 0.397
- 0.28
- 0.351
- 0.276
- 0.329
- 0.247
- 0.257
- 0.326
- 0.335
- 0.344
- 0.35
- 0.265
- 0.326
- 0.29
- 0.283
- 0.266
- 0.353
- 0.349
- 0.344
- 0.224
- 0.32
- 0.311
- 0.334
unequal: 1
verbose: 1
