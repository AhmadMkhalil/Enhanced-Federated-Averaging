avg_train_accuracy: 0.793
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.09180851063829787
- 0.2104255319148936
- 0.29085106382978726
- 0.3123404255319149
- 0.36925531914893617
- 0.45313829787234045
- 0.43675531914893617
- 0.4760106382978723
- 0.5550531914893617
- 0.5304255319148936
- 0.556063829787234
- 0.5218085106382979
- 0.5540957446808511
- 0.5868085106382979
- 0.5596808510638298
- 0.5507446808510639
- 0.5611170212765958
- 0.59
- 0.6256382978723404
- 0.5918617021276595
- 0.5962765957446808
- 0.6269148936170212
- 0.65
- 0.6349468085106383
- 0.635531914893617
- 0.6210106382978723
- 0.6119148936170212
- 0.6303723404255319
- 0.6176063829787234
- 0.6688297872340425
- 0.6638297872340425
- 0.6238829787234043
- 0.6390957446808511
- 0.6707446808510639
- 0.6623404255319149
- 0.6935638297872341
- 0.6706382978723404
- 0.6389893617021276
- 0.6156914893617021
- 0.5877659574468085
- 0.6857446808510639
- 0.663031914893617
- 0.7053723404255319
- 0.6704787234042553
- 0.6622872340425532
- 0.6527127659574468
- 0.6402659574468085
- 0.6251595744680851
- 0.6875
- 0.6595212765957447
- 0.6520744680851064
- 0.6342021276595745
- 0.6429255319148937
- 0.6691489361702128
- 0.698031914893617
- 0.6629787234042553
- 0.694468085106383
- 0.6809574468085107
- 0.6601063829787234
- 0.7102659574468085
- 0.6768085106382978
- 0.6498936170212766
- 0.6887765957446809
- 0.6881914893617022
- 0.7147872340425532
- 0.7187234042553191
- 0.6928723404255319
- 0.6765425531914894
- 0.6829255319148936
- 0.6619680851063829
- 0.7157446808510638
- 0.7116489361702127
- 0.721968085106383
- 0.7054787234042553
- 0.6951063829787234
- 0.6952127659574469
- 0.7013829787234043
- 0.6620744680851064
- 0.6433510638297872
- 0.6762765957446808
- 0.6870212765957446
- 0.6867021276595745
- 0.6466489361702128
- 0.671968085106383
- 0.6594148936170213
- 0.6835106382978723
- 0.7004255319148937
- 0.7281382978723404
- 0.6795744680851064
- 0.6597340425531915
- 0.6917021276595745
- 0.6591489361702128
- 0.6601595744680852
- 0.706436170212766
- 0.7350531914893617
- 0.7207446808510638
- 0.718404255319149
- 0.7137765957446809
- 0.7222340425531915
- 0.6993617021276596
test_loss_list:
- 542.7811861038208
- 458.53479266166687
- 403.6737504005432
- 387.0670347213745
- 311.999080657959
- 279.68675422668457
- 279.65669202804565
- 243.46875762939453
- 218.52297604084015
- 210.505002617836
- 197.98783242702484
- 214.4568647146225
- 196.9937905073166
- 193.80894941091537
- 205.43514680862427
- 197.58624732494354
- 186.78774225711823
- 179.49143654108047
- 157.00474619865417
- 172.10947841405869
- 170.7700327038765
- 162.8725089430809
- 164.09520971775055
- 151.49574744701385
- 161.71263712644577
- 177.42136627435684
- 176.79682725667953
- 155.6412810087204
- 166.63321441411972
- 152.17378121614456
- 142.8615728020668
- 161.89615082740784
- 149.75870943069458
- 145.3411989212036
- 141.20402652025223
- 142.40651893615723
- 152.5998374223709
- 146.06733733415604
- 157.58029508590698
- 163.68123769760132
- 130.59254467487335
- 144.34292793273926
- 126.35915547609329
- 140.58355349302292
- 147.01130479574203
- 142.6969432234764
- 150.30401450395584
- 147.62304949760437
- 132.3098132610321
- 143.59880232810974
- 142.02383202314377
- 152.10931980609894
- 168.2125107049942
- 146.41026377677917
- 127.85597831010818
- 151.15498101711273
- 132.60160315036774
- 136.1865360736847
- 135.65592408180237
- 123.271897315979
- 132.66090708971024
- 142.10407412052155
- 123.81561034917831
- 130.75447833538055
- 118.56293308734894
- 121.69269895553589
- 140.56046068668365
- 142.26798313856125
- 125.27253419160843
- 141.885655939579
- 118.87419718503952
- 123.3582124710083
- 117.4093531370163
- 131.3857164978981
- 131.78663235902786
- 125.07118529081345
- 124.08690935373306
- 135.63213527202606
- 141.6471729874611
- 136.4213594198227
- 135.0984696149826
- 134.55169427394867
- 158.3816783428192
- 139.8264485001564
- 143.66104197502136
- 141.63225376605988
- 130.38230669498444
- 112.78839522600174
- 133.11640048027039
- 132.22486472129822
- 129.69375050067902
- 165.1781286597252
- 134.82137370109558
- 119.69391590356827
- 114.16292518377304
- 113.70214796066284
- 119.8716202378273
- 124.07452946901321
- 116.1781776547432
- 125.50199627876282
train_accuracy:
- 0.0
- 0.0
- 0.125
- 0.228
- 0.093
- 0.494
- 0.482
- 0.01
- 0.732
- 0.332
- 0.522
- 0.65
- 0.629
- 0.175
- 0.37
- 0.363
- 0.514
- 0.834
- 0.65
- 0.433
- 0.727
- 0.838
- 0.105
- 0.672
- 0.813
- 0.854
- 0.791
- 0.397
- 0.71
- 0.32
- 0.654
- 0.759
- 0.821
- 0.746
- 0.344
- 0.827
- 0.625
- 0.843
- 0.542
- 0.95
- 0.388
- 0.9
- 0.739
- 0.42
- 0.475
- 0.532
- 0.56
- 0.85
- 0.958
- 0.627
- 0.672
- 0.36
- 0.575
- 0.609
- 0.591
- 0.98
- 0.686
- 0.742
- 0.7
- 0.604
- 0.571
- 0.817
- 0.55
- 0.733
- 0.862
- 0.886
- 0.53
- 0.8
- 0.664
- 0.55
- 0.872
- 0.812
- 0.975
- 0.564
- 0.885
- 0.605
- 0.488
- 0.814
- 0.0
- 0.75
- 0.818
- 0.779
- 0.133
- 0.592
- 0.764
- 0.475
- 0.9
- 0.871
- 0.642
- 0.46
- 0.782
- 0.833
- 0.9
- 0.681
- 0.306
- 0.214
- 0.7
- 0.983
- 0.622
- 0.793
train_loss:
- 1.211
- 0.78
- 0.651
- 0.554
- 0.63
- 0.562
- 0.485
- 0.53
- 0.539
- 0.416
- 0.447
- 0.47
- 0.484
- 0.45
- 0.467
- 0.408
- 0.431
- 0.456
- 0.399
- 0.458
- 0.418
- 0.396
- 0.303
- 0.429
- 0.418
- 0.416
- 0.415
- 0.41
- 0.357
- 0.431
- 0.39
- 0.318
- 0.296
- 0.348
- 0.411
- 0.355
- 0.416
- 0.433
- 0.354
- 0.408
- 0.39
- 0.36
- 0.343
- 0.351
- 0.335
- 0.342
- 0.356
- 0.294
- 0.358
- 0.371
- 0.317
- 0.46
- 0.383
- 0.397
- 0.346
- 0.348
- 0.302
- 0.256
- 0.37
- 0.33
- 0.311
- 0.357
- 0.317
- 0.291
- 0.304
- 0.275
- 0.376
- 0.315
- 0.275
- 0.311
- 0.392
- 0.333
- 0.327
- 0.336
- 0.352
- 0.31
- 0.348
- 0.325
- 0.34
- 0.397
- 0.378
- 0.292
- 0.289
- 0.377
- 0.274
- 0.362
- 0.293
- 0.294
- 0.326
- 0.318
- 0.312
- 0.305
- 0.313
- 0.325
- 0.276
- 0.284
- 0.298
- 0.371
- 0.306
- 0.337
unequal: 1
verbose: 1
