avg_train_accuracy: 0.758
avg_train_loss: 0.004
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.13138297872340426
- 0.20712765957446808
- 0.29521276595744683
- 0.3317553191489362
- 0.4341489361702128
- 0.3903723404255319
- 0.44377659574468087
- 0.473031914893617
- 0.5528191489361702
- 0.5790425531914893
- 0.5884574468085106
- 0.5568617021276596
- 0.5234574468085106
- 0.5759042553191489
- 0.5554255319148936
- 0.5859574468085106
- 0.6273936170212766
- 0.5872872340425532
- 0.6051063829787234
- 0.5968617021276595
- 0.5915957446808511
- 0.574627659574468
- 0.5607978723404256
- 0.6206382978723404
- 0.6122872340425531
- 0.6132446808510639
- 0.6367553191489361
- 0.6805851063829788
- 0.62
- 0.6413297872340425
- 0.6823404255319149
- 0.6791489361702128
- 0.6392021276595745
- 0.6442553191489362
- 0.6513829787234042
- 0.6462765957446809
- 0.6430851063829788
- 0.6176063829787234
- 0.6089361702127659
- 0.636968085106383
- 0.6524468085106383
- 0.673031914893617
- 0.6708510638297872
- 0.6194148936170213
- 0.6473936170212766
- 0.6756382978723404
- 0.6989893617021277
- 0.6459574468085106
- 0.6210106382978723
- 0.6728191489361702
- 0.6701595744680852
- 0.6452127659574468
- 0.6328723404255319
- 0.7023404255319149
- 0.6949468085106383
- 0.6392553191489362
- 0.7032446808510638
- 0.674468085106383
- 0.6696276595744681
- 0.6954787234042553
- 0.6401595744680851
- 0.6812765957446808
- 0.7190425531914894
- 0.6368085106382979
- 0.6588829787234043
- 0.6376063829787234
- 0.6856382978723404
- 0.704095744680851
- 0.7054787234042553
- 0.6504255319148936
- 0.6953191489361702
- 0.6519148936170213
- 0.6898404255319149
- 0.7159042553191489
- 0.7030851063829787
- 0.7008510638297872
- 0.7069148936170213
- 0.681968085106383
- 0.7011170212765957
- 0.6755851063829788
- 0.6687765957446808
- 0.6465425531914893
- 0.6947872340425532
- 0.6984574468085106
- 0.6893085106382979
- 0.678031914893617
- 0.7210106382978724
- 0.6902659574468085
- 0.7091489361702128
- 0.6430851063829788
- 0.6534574468085106
- 0.6865425531914894
- 0.7278191489361702
- 0.6900531914893617
- 0.6764893617021277
- 0.7031914893617022
- 0.6424468085106383
- 0.6786702127659574
- 0.6570744680851064
- 0.6507446808510639
test_loss_list:
- 531.4925012588501
- 471.92024421691895
- 398.90838503837585
- 366.50519728660583
- 306.50639510154724
- 302.7540783882141
- 288.2390971183777
- 263.3333090543747
- 224.10605800151825
- 210.68204247951508
- 201.99471831321716
- 200.93646669387817
- 222.37737035751343
- 208.77523601055145
- 199.99182856082916
- 188.31100642681122
- 172.23402416706085
- 178.76899307966232
- 177.04733097553253
- 180.0866031050682
- 183.74602031707764
- 176.5203914642334
- 175.45452857017517
- 161.58268976211548
- 158.58772456645966
- 173.6201251745224
- 156.78303134441376
- 142.25575119256973
- 169.97883266210556
- 155.77195316553116
- 138.8740491271019
- 137.31223684549332
- 162.56542390584946
- 152.5291382074356
- 150.63515406847
- 150.59954303503036
- 153.135995388031
- 164.59622138738632
- 166.24198228120804
- 165.6697084903717
- 151.69475960731506
- 132.41393649578094
- 133.24041002988815
- 159.8152061700821
- 148.2972930073738
- 142.2588717341423
- 139.11559480428696
- 152.67420601844788
- 161.64354199171066
- 138.28689473867416
- 142.59201765060425
- 157.43246364593506
- 160.61570757627487
- 133.32480907440186
- 141.44200587272644
- 148.0358488559723
- 135.5893280506134
- 141.2659792304039
- 143.60688084363937
- 125.18393343687057
- 158.09332394599915
- 133.62665897607803
- 121.2741768360138
- 157.3953137397766
- 150.26200687885284
- 156.99768894910812
- 125.15484988689423
- 126.49868875741959
- 124.43430507183075
- 145.10548198223114
- 126.03793203830719
- 158.15049493312836
- 136.092303276062
- 126.20017606019974
- 126.3592381477356
- 128.57940107584
- 135.58945733308792
- 132.1562655568123
- 124.05224633216858
- 133.54878026247025
- 144.014918923378
- 143.45010340213776
- 141.82613945007324
- 123.53882282972336
- 131.86976128816605
- 131.00651520490646
- 125.02300810813904
- 129.88392531871796
- 126.08013850450516
- 145.36718279123306
- 146.51314705610275
- 126.19715440273285
- 119.38917398452759
- 134.21386855840683
- 140.3246589899063
- 125.82132321596146
- 146.91125351190567
- 133.57873678207397
- 147.3559027314186
- 145.07430577278137
train_accuracy:
- 0.587
- 0.0
- 0.0
- 0.0
- 0.512
- 0.346
- 0.975
- 0.713
- 0.19
- 0.764
- 0.409
- 0.675
- 0.683
- 0.086
- 0.922
- 0.581
- 0.492
- 0.175
- 0.809
- 0.01
- 0.678
- 0.929
- 0.967
- 0.288
- 0.828
- 0.25
- 0.567
- 0.626
- 0.826
- 0.737
- 0.773
- 0.706
- 0.6
- 0.555
- 0.783
- 0.817
- 0.333
- 0.914
- 0.491
- 0.723
- 0.474
- 0.821
- 0.975
- 0.954
- 0.95
- 0.618
- 0.925
- 0.868
- 0.478
- 0.309
- 0.917
- 0.763
- 0.537
- 0.827
- 0.505
- 0.493
- 0.725
- 0.675
- 0.862
- 0.794
- 0.857
- 0.35
- 0.722
- 0.467
- 0.841
- 0.0
- 0.95
- 0.93
- 0.714
- 0.612
- 0.369
- 0.183
- 0.653
- 0.838
- 0.8
- 0.419
- 0.8
- 0.656
- 0.823
- 0.914
- 0.76
- 0.684
- 0.528
- 0.688
- 0.808
- 0.422
- 0.6
- 0.854
- 0.758
- 0.525
- 0.762
- 0.838
- 0.963
- 0.583
- 0.756
- 0.881
- 0.65
- 0.693
- 0.582
- 0.758
train_loss:
- 1.094
- 0.763
- 0.67
- 0.506
- 0.612
- 0.624
- 0.524
- 0.53
- 0.454
- 0.395
- 0.525
- 0.469
- 0.46
- 0.53
- 0.5
- 0.476
- 0.439
- 0.317
- 0.435
- 0.345
- 0.442
- 0.456
- 0.441
- 0.357
- 0.37
- 0.398
- 0.344
- 0.408
- 0.369
- 0.39
- 0.329
- 0.389
- 0.429
- 0.324
- 0.301
- 0.403
- 0.372
- 0.357
- 0.404
- 0.442
- 0.399
- 0.326
- 0.294
- 0.459
- 0.321
- 0.245
- 0.278
- 0.432
- 0.455
- 0.385
- 0.376
- 0.408
- 0.362
- 0.352
- 0.35
- 0.32
- 0.332
- 0.454
- 0.375
- 0.408
- 0.344
- 0.281
- 0.363
- 0.381
- 0.334
- 0.319
- 0.3
- 0.345
- 0.269
- 0.383
- 0.359
- 0.294
- 0.392
- 0.35
- 0.239
- 0.349
- 0.392
- 0.277
- 0.328
- 0.311
- 0.345
- 0.344
- 0.329
- 0.325
- 0.245
- 0.359
- 0.276
- 0.352
- 0.277
- 0.4
- 0.328
- 0.341
- 0.355
- 0.324
- 0.407
- 0.353
- 0.396
- 0.332
- 0.328
- 0.38
unequal: 1
verbose: 1
