avg_train_accuracy: 0.482
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.15840425531914892
- 0.22563829787234044
- 0.32207446808510637
- 0.37851063829787235
- 0.40547872340425534
- 0.4175531914893617
- 0.4991489361702128
- 0.5577127659574468
- 0.5542021276595744
- 0.5204255319148936
- 0.49882978723404253
- 0.5370744680851064
- 0.5510106382978723
- 0.5348404255319149
- 0.5988829787234042
- 0.5957446808510638
- 0.5543085106382979
- 0.5973936170212766
- 0.5942021276595745
- 0.6087765957446809
- 0.6216489361702128
- 0.6018085106382979
- 0.6237234042553191
- 0.6525
- 0.595
- 0.6119148936170212
- 0.6054255319148936
- 0.6168085106382979
- 0.6595212765957447
- 0.6396808510638298
- 0.6714361702127659
- 0.6079787234042553
- 0.6584042553191489
- 0.6497872340425532
- 0.6137765957446808
- 0.6031382978723404
- 0.6677127659574468
- 0.6536170212765957
- 0.600531914893617
- 0.6587234042553192
- 0.6054787234042553
- 0.6439893617021276
- 0.6136170212765958
- 0.6482978723404256
- 0.5728191489361703
- 0.663563829787234
- 0.647127659574468
- 0.6256382978723404
- 0.5546808510638298
- 0.6425531914893617
- 0.6334574468085107
- 0.6201595744680851
- 0.6397340425531914
- 0.6815425531914894
- 0.6522872340425532
- 0.6678191489361702
- 0.6668085106382978
- 0.6313829787234042
- 0.6496276595744681
- 0.6804787234042553
- 0.6987765957446809
- 0.6847872340425532
- 0.6098936170212766
- 0.6494148936170213
- 0.6398404255319149
- 0.6172872340425531
- 0.6311170212765957
- 0.6728191489361702
- 0.6751063829787234
- 0.6975531914893617
- 0.6528723404255319
- 0.663031914893617
- 0.6870744680851064
- 0.6522872340425532
- 0.6686170212765957
- 0.6662234042553191
- 0.6363829787234042
- 0.6851595744680851
- 0.6986170212765958
- 0.7117021276595744
- 0.6885106382978723
- 0.6722340425531915
- 0.6725
- 0.6845212765957447
- 0.6645744680851063
- 0.6462234042553191
- 0.6620744680851064
- 0.704627659574468
- 0.7070212765957447
- 0.6803191489361702
- 0.6843085106382979
- 0.6604255319148936
- 0.6662234042553191
- 0.7082446808510638
- 0.7026595744680851
- 0.6936702127659574
- 0.6860106382978723
- 0.6851595744680851
- 0.6523404255319148
- 0.638563829787234
test_loss_list:
- 539.3133816719055
- 456.0250012874603
- 370.58207511901855
- 325.9035921096802
- 297.8956789970398
- 278.67701756954193
- 244.822394490242
- 217.8781840801239
- 210.34542632102966
- 222.08896815776825
- 221.038400888443
- 204.0144703388214
- 199.91114127635956
- 214.68874061107635
- 180.53173565864563
- 178.12967282533646
- 185.88745152950287
- 174.13888496160507
- 177.221062541008
- 168.98894000053406
- 176.68838673830032
- 169.38465505838394
- 162.72429633140564
- 148.927903175354
- 184.49404805898666
- 178.4975101351738
- 164.3091008067131
- 163.79193091392517
- 150.9742437005043
- 161.61039143800735
- 145.4978152513504
- 164.39449256658554
- 151.6089786887169
- 154.76552087068558
- 155.55401170253754
- 186.52584117650986
- 139.99951392412186
- 145.2062644958496
- 163.6652975678444
- 148.20726829767227
- 169.69425189495087
- 150.63278138637543
- 160.45680785179138
- 150.32143664360046
- 169.46478617191315
- 143.7825847864151
- 146.44298672676086
- 155.2080253958702
- 178.19052356481552
- 148.94319516420364
- 153.2252013683319
- 145.36448293924332
- 140.42708933353424
- 138.14389330148697
- 144.457239985466
- 144.45268058776855
- 138.32410830259323
- 153.5979785323143
- 148.70756840705872
- 135.9657437801361
- 128.86465620994568
- 136.6922978758812
- 164.07985317707062
- 144.60696703195572
- 148.27566748857498
- 164.36496490240097
- 151.6332716345787
- 136.1259604692459
- 135.7281529903412
- 127.56972426176071
- 144.41434812545776
- 141.7498270869255
- 128.49796223640442
- 142.4967524409294
- 146.27730238437653
- 143.1283563375473
- 145.9064680337906
- 134.10156285762787
- 130.38328635692596
- 129.56335270404816
- 129.67752522230148
- 140.64956539869308
- 137.41490018367767
- 139.42609083652496
- 143.5379320383072
- 142.46110516786575
- 137.87058156728745
- 122.8017925620079
- 121.26658177375793
- 133.6877075433731
- 135.34704768657684
- 138.7090647816658
- 144.64769977331161
- 126.22026091814041
- 131.52953219413757
- 138.10245633125305
- 136.84201508760452
- 143.90913927555084
- 157.20252966880798
- 148.71277004480362
train_accuracy:
- 0.0
- 0.234
- 0.14
- 0.4
- 0.612
- 0.495
- 0.8
- 0.057
- 0.714
- 0.842
- 0.383
- 0.304
- 0.396
- 0.868
- 0.787
- 0.533
- 0.731
- 0.53
- 0.8
- 0.638
- 0.654
- 0.675
- 0.158
- 0.963
- 0.971
- 0.732
- 0.989
- 0.975
- 0.685
- 0.938
- 0.8
- 0.817
- 0.855
- 0.481
- 0.538
- 0.167
- 0.279
- 0.61
- 0.443
- 0.663
- 0.613
- 0.45
- 0.662
- 0.987
- 0.708
- 0.15
- 0.653
- 0.542
- 0.79
- 0.633
- 0.441
- 0.942
- 0.3
- 0.721
- 0.82
- 0.979
- 0.457
- 0.383
- 0.439
- 0.519
- 0.883
- 0.853
- 0.547
- 0.863
- 0.447
- 0.569
- 0.167
- 0.894
- 0.867
- 0.497
- 0.679
- 0.892
- 0.527
- 0.532
- 0.791
- 0.647
- 0.643
- 0.931
- 0.388
- 0.938
- 0.823
- 0.9
- 0.873
- 0.925
- 0.832
- 0.577
- 0.336
- 0.781
- 0.754
- 0.764
- 0.841
- 0.835
- 0.777
- 0.59
- 0.786
- 0.453
- 0.342
- 0.592
- 0.809
- 0.482
train_loss:
- 1.089
- 0.946
- 0.68
- 0.607
- 0.565
- 0.526
- 0.488
- 0.437
- 0.447
- 0.408
- 0.534
- 0.475
- 0.487
- 0.463
- 0.385
- 0.336
- 0.439
- 0.447
- 0.397
- 0.427
- 0.441
- 0.418
- 0.468
- 0.365
- 0.355
- 0.437
- 0.33
- 0.312
- 0.423
- 0.257
- 0.379
- 0.351
- 0.395
- 0.368
- 0.364
- 0.381
- 0.371
- 0.321
- 0.398
- 0.399
- 0.349
- 0.451
- 0.391
- 0.313
- 0.381
- 0.262
- 0.387
- 0.288
- 0.359
- 0.336
- 0.304
- 0.368
- 0.306
- 0.485
- 0.322
- 0.313
- 0.319
- 0.362
- 0.32
- 0.314
- 0.286
- 0.418
- 0.294
- 0.255
- 0.46
- 0.296
- 0.475
- 0.352
- 0.354
- 0.336
- 0.351
- 0.333
- 0.351
- 0.325
- 0.32
- 0.463
- 0.326
- 0.326
- 0.246
- 0.323
- 0.35
- 0.273
- 0.422
- 0.331
- 0.292
- 0.276
- 0.27
- 0.34
- 0.327
- 0.353
- 0.295
- 0.28
- 0.321
- 0.286
- 0.297
- 0.339
- 0.282
- 0.346
- 0.275
- 0.301
unequal: 1
verbose: 1
