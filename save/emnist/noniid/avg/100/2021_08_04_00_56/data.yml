avg_train_accuracy: 0.592
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.10159574468085106
- 0.2226063829787234
- 0.28691489361702127
- 0.3390957446808511
- 0.42861702127659573
- 0.39122340425531915
- 0.47845744680851066
- 0.48138297872340424
- 0.5259574468085106
- 0.506968085106383
- 0.5209574468085106
- 0.5400531914893617
- 0.5377659574468086
- 0.5679255319148936
- 0.607872340425532
- 0.5888297872340426
- 0.5994148936170213
- 0.5278191489361702
- 0.6076595744680852
- 0.5772872340425532
- 0.608563829787234
- 0.5977127659574468
- 0.6188297872340426
- 0.6100531914893617
- 0.5962765957446808
- 0.6202659574468085
- 0.6315425531914893
- 0.5882978723404255
- 0.6392021276595745
- 0.6196808510638298
- 0.6454787234042553
- 0.6628191489361702
- 0.5875
- 0.6840425531914893
- 0.6788829787234043
- 0.6340425531914894
- 0.6463297872340426
- 0.6779787234042554
- 0.6893085106382979
- 0.676968085106383
- 0.6176063829787234
- 0.6824468085106383
- 0.6432978723404256
- 0.6725531914893617
- 0.6242021276595745
- 0.6665425531914894
- 0.6635106382978724
- 0.6232978723404256
- 0.661595744680851
- 0.7050531914893617
- 0.7165425531914894
- 0.6865425531914894
- 0.6290425531914894
- 0.6848404255319149
- 0.6467021276595745
- 0.6813829787234043
- 0.6854255319148936
- 0.6782978723404255
- 0.6670744680851064
- 0.7067021276595745
- 0.710531914893617
- 0.6668085106382978
- 0.7023936170212766
- 0.670904255319149
- 0.6841489361702128
- 0.6740425531914893
- 0.6650531914893617
- 0.6798404255319149
- 0.699468085106383
- 0.6708510638297872
- 0.6538297872340425
- 0.6743085106382979
- 0.6690425531914893
- 0.6778723404255319
- 0.7270744680851063
- 0.689627659574468
- 0.7023404255319149
- 0.7026595744680851
- 0.7070744680851064
- 0.7121276595744681
- 0.676063829787234
- 0.7186702127659574
- 0.6917021276595745
- 0.6940425531914893
- 0.6568085106382979
- 0.729468085106383
- 0.6996808510638298
- 0.6872340425531915
- 0.7057446808510638
- 0.6785638297872341
- 0.6792553191489362
- 0.7037765957446809
- 0.7087234042553191
- 0.6899468085106383
- 0.7003191489361702
- 0.6812234042553191
- 0.7011170212765957
- 0.6782446808510638
- 0.6943617021276596
- 0.6636170212765957
test_loss_list:
- 537.9846425056458
- 464.9841275215149
- 413.35427355766296
- 369.98589158058167
- 308.56764030456543
- 317.5375510454178
- 274.2569556236267
- 252.56939923763275
- 233.64079129695892
- 241.79969203472137
- 227.61302542686462
- 213.37513887882233
- 208.8599375486374
- 187.08326077461243
- 171.53356730937958
- 181.7798724770546
- 194.05841946601868
- 207.8540964126587
- 170.10710471868515
- 174.7297694683075
- 164.3516827225685
- 183.79596328735352
- 171.47738182544708
- 173.7447070479393
- 171.64380651712418
- 160.76669055223465
- 157.7552061676979
- 181.22557789087296
- 165.94074416160583
- 159.5172614455223
- 149.55315494537354
- 148.3680875301361
- 179.55380195379257
- 145.37786799669266
- 145.92914164066315
- 164.75356769561768
- 147.52493792772293
- 147.66641265153885
- 151.33642721176147
- 140.06149554252625
- 160.5651491880417
- 141.41742992401123
- 145.11429530382156
- 135.35248470306396
- 157.8691610097885
- 139.74097657203674
- 141.94033056497574
- 152.23181545734406
- 147.40032106637955
- 124.95696431398392
- 125.88332718610764
- 134.4343505501747
- 158.8842589855194
- 134.7080666422844
- 147.94641816616058
- 141.5327033996582
- 139.91041177511215
- 138.3383533358574
- 141.70157396793365
- 123.03839361667633
- 122.81382805109024
- 140.24545991420746
- 123.524877846241
- 138.49556827545166
- 135.4928461909294
- 136.76354175806046
- 152.134859085083
- 135.25801318883896
- 127.65160924196243
- 133.92850369215012
- 144.99959671497345
- 156.54689413309097
- 148.44371056556702
- 129.67612260580063
- 114.7153491973877
- 132.91235047578812
- 117.2810263633728
- 125.73927640914917
- 126.27637726068497
- 130.03050816059113
- 131.8602972626686
- 128.40877431631088
- 130.4402111172676
- 125.86125153303146
- 144.0189253091812
- 115.98986440896988
- 141.37634539604187
- 128.06354451179504
- 126.26078766584396
- 139.62621372938156
- 143.76725804805756
- 129.96254014968872
- 125.85857194662094
- 132.7780933380127
- 129.35284900665283
- 147.1231678724289
- 124.99952054023743
- 133.43029183149338
- 125.93793368339539
- 130.1820752620697
train_accuracy:
- 0.0
- 0.557
- 0.775
- 0.36
- 0.418
- 0.45
- 0.683
- 0.697
- 0.696
- 0.846
- 0.689
- 0.45
- 0.85
- 0.156
- 0.689
- 0.98
- 0.217
- 0.209
- 0.662
- 0.2
- 0.786
- 0.49
- 0.944
- 0.806
- 0.459
- 0.89
- 0.935
- 0.635
- 0.732
- 0.732
- 0.75
- 0.88
- 0.677
- 0.888
- 0.792
- 0.637
- 0.927
- 0.3
- 0.963
- 0.361
- 0.573
- 0.9
- 0.312
- 0.956
- 0.65
- 0.864
- 0.857
- 0.769
- 0.609
- 0.896
- 0.262
- 0.688
- 0.493
- 0.809
- 0.727
- 0.833
- 0.579
- 0.816
- 0.49
- 0.532
- 0.765
- 0.868
- 0.582
- 0.93
- 0.844
- 0.735
- 0.365
- 0.505
- 0.904
- 0.794
- 0.96
- 0.638
- 0.482
- 0.37
- 0.308
- 0.714
- 0.816
- 0.653
- 0.661
- 0.772
- 0.787
- 0.815
- 0.443
- 0.621
- 0.534
- 0.556
- 0.839
- 0.794
- 0.924
- 0.831
- 0.536
- 0.963
- 0.903
- 0.838
- 0.8
- 0.933
- 0.76
- 0.932
- 0.553
- 0.592
train_loss:
- 1.068
- 0.836
- 0.683
- 0.764
- 0.571
- 0.54
- 0.47
- 0.486
- 0.521
- 0.475
- 0.531
- 0.451
- 0.442
- 0.379
- 0.391
- 0.456
- 0.345
- 0.337
- 0.469
- 0.4
- 0.492
- 0.468
- 0.407
- 0.391
- 0.419
- 0.46
- 0.414
- 0.353
- 0.468
- 0.403
- 0.406
- 0.36
- 0.427
- 0.337
- 0.352
- 0.387
- 0.339
- 0.346
- 0.237
- 0.373
- 0.397
- 0.344
- 0.391
- 0.374
- 0.327
- 0.42
- 0.4
- 0.404
- 0.43
- 0.377
- 0.347
- 0.399
- 0.323
- 0.314
- 0.407
- 0.351
- 0.364
- 0.381
- 0.33
- 0.345
- 0.306
- 0.381
- 0.366
- 0.357
- 0.371
- 0.35
- 0.355
- 0.41
- 0.378
- 0.398
- 0.35
- 0.359
- 0.388
- 0.418
- 0.319
- 0.326
- 0.347
- 0.376
- 0.398
- 0.369
- 0.375
- 0.325
- 0.37
- 0.358
- 0.35
- 0.386
- 0.321
- 0.371
- 0.329
- 0.338
- 0.282
- 0.323
- 0.355
- 0.332
- 0.403
- 0.35
- 0.36
- 0.341
- 0.319
- 0.312
unequal: 1
verbose: 1
