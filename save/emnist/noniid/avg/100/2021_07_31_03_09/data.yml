avg_train_accuracy: 0.495
avg_train_loss: 0.004
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1296276595744681
- 0.23324468085106384
- 0.2878191489361702
- 0.395531914893617
- 0.4013829787234043
- 0.4486702127659574
- 0.4999468085106383
- 0.5035106382978723
- 0.5635638297872341
- 0.5040425531914894
- 0.6073936170212766
- 0.62
- 0.5443085106382979
- 0.5576595744680851
- 0.5711170212765957
- 0.5181382978723404
- 0.5707446808510638
- 0.5947872340425532
- 0.5534574468085106
- 0.6252659574468085
- 0.6438297872340426
- 0.6172872340425531
- 0.6247340425531915
- 0.6047872340425532
- 0.6358510638297873
- 0.606436170212766
- 0.609095744680851
- 0.6495212765957447
- 0.6278723404255319
- 0.6132446808510639
- 0.6432446808510638
- 0.6243617021276596
- 0.6280851063829788
- 0.6232978723404256
- 0.6197340425531915
- 0.6399468085106383
- 0.6470212765957447
- 0.6625531914893616
- 0.6160106382978724
- 0.6436170212765957
- 0.621436170212766
- 0.6654255319148936
- 0.6582446808510638
- 0.6212765957446809
- 0.6528191489361702
- 0.6338297872340426
- 0.6694148936170212
- 0.6540957446808511
- 0.6784574468085106
- 0.6276063829787234
- 0.6628723404255319
- 0.7045744680851064
- 0.658563829787234
- 0.6529787234042553
- 0.6588829787234043
- 0.6818617021276596
- 0.6393617021276595
- 0.6640957446808511
- 0.6952659574468085
- 0.6587234042553192
- 0.6622872340425532
- 0.6967553191489362
- 0.6872340425531915
- 0.724095744680851
- 0.6407978723404255
- 0.6825531914893617
- 0.6485106382978724
- 0.6434042553191489
- 0.7287765957446809
- 0.7183510638297872
- 0.6849468085106383
- 0.6656914893617021
- 0.6598404255319149
- 0.7063297872340426
- 0.6676063829787234
- 0.6138829787234042
- 0.7000531914893617
- 0.6900531914893617
- 0.6847872340425532
- 0.6857978723404256
- 0.6670212765957447
- 0.6757978723404255
- 0.7237234042553191
- 0.713936170212766
- 0.696063829787234
- 0.6698404255319149
- 0.680372340425532
- 0.6867553191489362
- 0.7081914893617022
- 0.6929787234042554
- 0.6688829787234043
- 0.6121808510638298
- 0.6721276595744681
- 0.6978723404255319
- 0.6724468085106383
- 0.6875
- 0.6457978723404255
- 0.6568617021276596
- 0.683031914893617
- 0.6990957446808511
test_loss_list:
- 533.5017035007477
- 442.19934725761414
- 387.33422350883484
- 325.66479754447937
- 297.5508931875229
- 265.1425793170929
- 241.31607377529144
- 240.27065861225128
- 200.44512701034546
- 235.79584157466888
- 188.32682311534882
- 175.55041253566742
- 200.2325723171234
- 186.87958562374115
- 183.5838878750801
- 213.9489129781723
- 186.65821945667267
- 181.91713547706604
- 198.37803637981415
- 150.9989451766014
- 161.2333574295044
- 166.81952863931656
- 159.30775380134583
- 171.29563730955124
- 165.94983434677124
- 166.22647333145142
- 172.9916501045227
- 149.4208807349205
- 159.69838440418243
- 156.22079694271088
- 143.89489048719406
- 164.38752156496048
- 156.19004690647125
- 162.6460257768631
- 171.22368186712265
- 162.35294860601425
- 157.9742556810379
- 152.22715651988983
- 165.73941099643707
- 152.6309803724289
- 161.3606320619583
- 150.4243187904358
- 155.27551305294037
- 172.1197828054428
- 143.568101644516
- 148.26619619131088
- 137.63399869203568
- 139.38998770713806
- 139.9546309709549
- 157.30483275651932
- 144.18483972549438
- 129.23232448101044
- 158.7414903640747
- 151.82104921340942
- 153.62812519073486
- 138.8051866889
- 157.11502361297607
- 145.57607275247574
- 125.6421285867691
- 138.8113778233528
- 136.56610673666
- 129.45945090055466
- 134.86550587415695
- 130.5640953183174
- 154.73958259820938
- 136.7324699163437
- 139.10602396726608
- 138.45595091581345
- 116.43637561798096
- 116.65335273742676
- 130.17511588335037
- 136.1372091770172
- 148.1339060664177
- 129.54043436050415
- 141.11900866031647
- 159.73650348186493
- 130.97997397184372
- 131.43732619285583
- 132.79100388288498
- 127.61112409830093
- 138.26493686437607
- 133.77989333868027
- 121.65761113166809
- 121.20847797393799
- 122.51957815885544
- 137.787100315094
- 135.76395165920258
- 130.42272460460663
- 122.48515862226486
- 126.70776057243347
- 135.01024723052979
- 151.23675048351288
- 128.23660916090012
- 126.44519621133804
- 141.2558588385582
- 130.0712463259697
- 153.74275815486908
- 149.01128989458084
- 131.13072419166565
- 128.36387765407562
train_accuracy:
- 0.286
- 0.07
- 0.0
- 0.472
- 0.105
- 0.446
- 0.95
- 0.11
- 0.395
- 0.582
- 0.337
- 0.95
- 0.74
- 0.814
- 0.695
- 1.0
- 0.657
- 0.562
- 1.0
- 0.586
- 0.406
- 0.707
- 0.453
- 0.515
- 0.93
- 0.413
- 0.596
- 0.817
- 0.983
- 0.211
- 0.525
- 0.235
- 0.46
- 0.306
- 0.2
- 0.875
- 0.95
- 0.862
- 0.843
- 0.783
- 0.84
- 0.6
- 0.683
- 0.5
- 0.361
- 0.862
- 0.735
- 0.462
- 0.775
- 0.925
- 0.554
- 0.636
- 0.389
- 0.593
- 0.862
- 0.765
- 0.616
- 0.758
- 0.961
- 0.583
- 0.22
- 0.78
- 0.833
- 0.597
- 0.214
- 0.9
- 0.8
- 0.888
- 0.7
- 0.75
- 0.18
- 0.841
- 0.417
- 0.65
- 0.963
- 0.483
- 0.737
- 0.329
- 0.2
- 0.557
- 0.25
- 1.0
- 0.712
- 0.907
- 0.732
- 0.133
- 0.656
- 0.963
- 0.872
- 0.656
- 0.42
- 0.321
- 0.92
- 0.556
- 0.628
- 0.914
- 0.859
- 0.497
- 0.911
- 0.495
train_loss:
- 1.302
- 0.858
- 0.68
- 0.589
- 0.65
- 0.481
- 0.531
- 0.446
- 0.405
- 0.397
- 0.448
- 0.451
- 0.48
- 0.459
- 0.437
- 0.39
- 0.364
- 0.44
- 0.384
- 0.351
- 0.408
- 0.36
- 0.374
- 0.381
- 0.392
- 0.364
- 0.339
- 0.357
- 0.392
- 0.39
- 0.348
- 0.383
- 0.356
- 0.348
- 0.33
- 0.323
- 0.373
- 0.323
- 0.265
- 0.396
- 0.404
- 0.267
- 0.362
- 0.428
- 0.34
- 0.349
- 0.369
- 0.316
- 0.303
- 0.356
- 0.383
- 0.344
- 0.32
- 0.292
- 0.318
- 0.378
- 0.329
- 0.382
- 0.325
- 0.29
- 0.345
- 0.376
- 0.425
- 0.285
- 0.332
- 0.344
- 0.37
- 0.366
- 0.451
- 0.292
- 0.258
- 0.34
- 0.325
- 0.345
- 0.289
- 0.346
- 0.291
- 0.265
- 0.34
- 0.251
- 0.209
- 0.303
- 0.269
- 0.301
- 0.36
- 0.296
- 0.361
- 0.323
- 0.296
- 0.309
- 0.293
- 0.332
- 0.377
- 0.328
- 0.366
- 0.362
- 0.29
- 0.343
- 0.295
- 0.355
unequal: 1
verbose: 1
