avg_train_accuracy: 0.3
avg_train_loss: 0.004
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.09702127659574468
- 0.2680851063829787
- 0.29675531914893616
- 0.37611702127659574
- 0.4302127659574468
- 0.4181382978723404
- 0.42164893617021276
- 0.5084042553191489
- 0.5106914893617022
- 0.5281382978723405
- 0.5347872340425532
- 0.541595744680851
- 0.5859574468085106
- 0.5983510638297872
- 0.6046808510638297
- 0.5767553191489362
- 0.6181382978723404
- 0.600531914893617
- 0.5527659574468086
- 0.6037234042553191
- 0.5900531914893618
- 0.629468085106383
- 0.6602659574468085
- 0.5950531914893618
- 0.6206382978723404
- 0.6457978723404255
- 0.6364893617021277
- 0.6227127659574468
- 0.6275531914893617
- 0.665
- 0.6604255319148936
- 0.6537765957446808
- 0.6558510638297872
- 0.6469148936170213
- 0.6777127659574468
- 0.6373936170212766
- 0.6499468085106384
- 0.664468085106383
- 0.6419148936170213
- 0.68
- 0.6557978723404255
- 0.6339893617021276
- 0.6073404255319149
- 0.629468085106383
- 0.6781914893617021
- 0.6570744680851064
- 0.6708510638297872
- 0.6545744680851063
- 0.6621276595744681
- 0.6642553191489362
- 0.6677659574468086
- 0.6502127659574468
- 0.7074468085106383
- 0.6676595744680851
- 0.7066489361702127
- 0.6979787234042554
- 0.6950531914893617
- 0.7035638297872341
- 0.7052127659574469
- 0.7039893617021277
- 0.7253723404255319
- 0.7143085106382979
- 0.7053723404255319
- 0.6609574468085107
- 0.6637765957446808
- 0.6481382978723405
- 0.6460638297872341
- 0.6688829787234043
- 0.7014361702127659
- 0.6773404255319149
- 0.6775531914893617
- 0.6748404255319149
- 0.7023404255319149
- 0.7035638297872341
- 0.6719148936170213
- 0.6690957446808511
- 0.6881382978723404
- 0.6783510638297873
- 0.6903723404255319
- 0.6668617021276596
- 0.6765425531914894
- 0.6796808510638298
- 0.6348404255319149
- 0.6404255319148936
- 0.642127659574468
- 0.6988829787234042
- 0.7248936170212766
- 0.7112234042553192
- 0.6744148936170212
- 0.6962234042553191
- 0.6721808510638297
- 0.676968085106383
- 0.6819148936170213
- 0.6890957446808511
- 0.6511170212765958
- 0.6762234042553191
- 0.6997872340425532
- 0.7017021276595745
- 0.6918617021276596
- 0.6748404255319149
test_loss_list:
- 534.8697710037231
- 438.85985136032104
- 372.04832649230957
- 317.4471822977066
- 290.49058413505554
- 267.1874929666519
- 250.76030266284943
- 244.3352106809616
- 231.87139356136322
- 217.6063233613968
- 209.39761996269226
- 211.45874452590942
- 184.06791722774506
- 173.44839143753052
- 174.0571681857109
- 187.26418697834015
- 164.66004550457
- 170.01307326555252
- 199.18417072296143
- 167.77549731731415
- 180.68821436166763
- 165.01857268810272
- 152.28655564785004
- 170.13421273231506
- 160.77323693037033
- 154.08987975120544
- 150.89429360628128
- 152.78430658578873
- 154.47819286584854
- 146.0689280629158
- 141.30105209350586
- 147.91290372610092
- 154.89231193065643
- 152.16375207901
- 139.52395701408386
- 160.2240884900093
- 146.48972260951996
- 137.93801802396774
- 140.6963371038437
- 136.47652155160904
- 143.78054058551788
- 146.38108468055725
- 157.6232293844223
- 148.68505334854126
- 139.06208163499832
- 145.33025777339935
- 145.27913630008698
- 153.28803098201752
- 151.4461990594864
- 142.88782757520676
- 140.55513662099838
- 150.79171931743622
- 122.67519551515579
- 128.8376823067665
- 126.98183262348175
- 134.86488544940948
- 122.42170095443726
- 127.31389033794403
- 127.48348355293274
- 130.44656455516815
- 113.50462967157364
- 119.48742002248764
- 125.81710034608841
- 130.22017335891724
- 134.78730058670044
- 147.2076273560524
- 166.3338726758957
- 134.53100031614304
- 119.4716289639473
- 127.03113377094269
- 137.22572940587997
- 135.83765321969986
- 126.0532215833664
- 128.73337131738663
- 153.81696212291718
- 144.65163278579712
- 122.08379447460175
- 131.62102049589157
- 130.20761024951935
- 136.63303065299988
- 135.6341502070427
- 132.7901463508606
- 149.31314289569855
- 146.0837517976761
- 145.32690864801407
- 121.62284481525421
- 126.59898519515991
- 124.43755275011063
- 139.96739500761032
- 123.49480456113815
- 133.67450016736984
- 122.8585154414177
- 129.70949161052704
- 136.05147832632065
- 144.98899459838867
- 140.083946287632
- 131.74948781728745
- 129.73066419363022
- 131.77796864509583
- 132.29754531383514
train_accuracy:
- 0.247
- 0.271
- 0.267
- 0.319
- 0.718
- 0.975
- 0.542
- 0.15
- 0.158
- 0.22
- 0.225
- 0.34
- 0.716
- 0.28
- 0.725
- 0.03
- 0.356
- 0.513
- 0.861
- 0.525
- 0.24
- 0.25
- 0.839
- 0.9
- 0.76
- 0.658
- 0.43
- 0.364
- 0.775
- 0.433
- 0.95
- 0.764
- 0.573
- 0.729
- 0.647
- 0.675
- 0.067
- 0.957
- 0.871
- 0.69
- 0.881
- 0.933
- 0.439
- 0.75
- 0.61
- 0.925
- 0.386
- 0.814
- 0.47
- 0.594
- 0.912
- 0.618
- 0.595
- 0.909
- 0.786
- 0.9
- 0.925
- 0.77
- 0.889
- 0.055
- 0.946
- 0.983
- 0.725
- 0.538
- 0.4
- 0.69
- 0.535
- 0.75
- 0.613
- 0.083
- 0.859
- 0.875
- 0.54
- 0.879
- 0.779
- 0.395
- 0.67
- 0.392
- 0.779
- 0.683
- 0.9
- 0.886
- 0.44
- 0.839
- 0.868
- 0.65
- 0.762
- 0.24
- 0.817
- 0.631
- 0.621
- 0.769
- 0.344
- 0.71
- 0.59
- 0.468
- 0.9
- 0.969
- 0.573
- 0.3
train_loss:
- 1.22
- 0.944
- 0.701
- 0.533
- 0.622
- 0.592
- 0.416
- 0.573
- 0.517
- 0.465
- 0.467
- 0.391
- 0.557
- 0.332
- 0.309
- 0.321
- 0.357
- 0.441
- 0.448
- 0.411
- 0.406
- 0.419
- 0.427
- 0.487
- 0.395
- 0.377
- 0.393
- 0.369
- 0.42
- 0.341
- 0.319
- 0.363
- 0.441
- 0.351
- 0.335
- 0.355
- 0.41
- 0.406
- 0.363
- 0.293
- 0.346
- 0.382
- 0.342
- 0.307
- 0.42
- 0.361
- 0.323
- 0.298
- 0.299
- 0.319
- 0.384
- 0.358
- 0.426
- 0.302
- 0.299
- 0.384
- 0.365
- 0.382
- 0.323
- 0.32
- 0.331
- 0.361
- 0.388
- 0.344
- 0.328
- 0.318
- 0.278
- 0.353
- 0.283
- 0.267
- 0.284
- 0.381
- 0.31
- 0.353
- 0.294
- 0.312
- 0.308
- 0.363
- 0.397
- 0.267
- 0.359
- 0.371
- 0.331
- 0.316
- 0.358
- 0.341
- 0.348
- 0.257
- 0.332
- 0.323
- 0.311
- 0.381
- 0.314
- 0.39
- 0.336
- 0.352
- 0.297
- 0.351
- 0.335
- 0.406
unequal: 1
verbose: 1
