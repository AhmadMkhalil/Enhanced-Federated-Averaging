avg_train_accuracy: 0.825
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.09074468085106382
- 0.21851063829787234
- 0.3323936170212766
- 0.4161170212765957
- 0.3784574468085106
- 0.4324468085106383
- 0.5082446808510638
- 0.45914893617021274
- 0.5145744680851064
- 0.4972872340425532
- 0.543936170212766
- 0.5265425531914893
- 0.5635638297872341
- 0.5913297872340425
- 0.5627659574468085
- 0.5846808510638298
- 0.6307446808510638
- 0.5622872340425532
- 0.6282978723404256
- 0.6236702127659575
- 0.6376595744680851
- 0.6027659574468085
- 0.6543085106382979
- 0.6461702127659574
- 0.6395212765957446
- 0.6370212765957447
- 0.6438297872340426
- 0.5987765957446809
- 0.6496808510638298
- 0.6255851063829787
- 0.6353191489361703
- 0.6579787234042553
- 0.6262765957446809
- 0.6032446808510639
- 0.6477659574468085
- 0.6351595744680851
- 0.7081382978723404
- 0.670904255319149
- 0.671968085106383
- 0.6586702127659575
- 0.6801063829787234
- 0.6507978723404255
- 0.6392553191489362
- 0.6050531914893617
- 0.6362765957446809
- 0.6656382978723404
- 0.6675531914893617
- 0.7046808510638298
- 0.6898936170212766
- 0.7054255319148937
- 0.6976063829787233
- 0.6473404255319148
- 0.6523936170212766
- 0.6776063829787234
- 0.6778723404255319
- 0.6926595744680851
- 0.669468085106383
- 0.6443617021276595
- 0.6146276595744681
- 0.6523404255319148
- 0.7044148936170213
- 0.6486702127659575
- 0.7045744680851064
- 0.6801595744680851
- 0.671595744680851
- 0.6431914893617021
- 0.6575
- 0.6775531914893617
- 0.6678191489361702
- 0.6730851063829787
- 0.6467021276595745
- 0.6883510638297873
- 0.686595744680851
- 0.7055851063829788
- 0.6762234042553191
- 0.6768617021276596
- 0.6854787234042553
- 0.689468085106383
- 0.6706382978723404
- 0.731436170212766
- 0.6970212765957446
- 0.7053191489361702
- 0.6504787234042553
- 0.6807446808510639
- 0.6725531914893617
- 0.6590425531914894
- 0.6567553191489361
- 0.7012765957446808
- 0.6594148936170213
- 0.6629787234042553
- 0.68
- 0.6692553191489362
- 0.6727659574468086
- 0.6746808510638298
- 0.6606914893617021
- 0.6553723404255319
- 0.6736702127659574
- 0.6721276595744681
- 0.631436170212766
- 0.7144148936170213
test_loss_list:
- 535.5497176647186
- 466.0185844898224
- 399.5471420288086
- 341.1239113807678
- 333.28209841251373
- 287.92333722114563
- 257.0161018371582
- 271.0670120716095
- 230.16268384456635
- 237.93277442455292
- 218.50873637199402
- 220.9853618144989
- 218.88469350337982
- 182.81382644176483
- 198.875150680542
- 192.89361107349396
- 169.3682622909546
- 191.08710193634033
- 165.45096629858017
- 172.94508355855942
- 162.3731147646904
- 183.35056591033936
- 159.23652291297913
- 150.11396557092667
- 163.67393106222153
- 151.68846565485
- 153.5404418706894
- 165.37298041582108
- 149.69702279567719
- 160.04836112260818
- 180.29223024845123
- 148.74796772003174
- 174.5256849527359
- 173.63895761966705
- 161.80352288484573
- 156.78794521093369
- 130.7819184064865
- 144.87858790159225
- 152.43580830097198
- 157.3027948141098
- 143.84462648630142
- 150.01824116706848
- 151.28501415252686
- 178.28809028863907
- 149.33573919534683
- 146.00489401817322
- 142.9081426858902
- 128.94984018802643
- 126.74586242437363
- 125.3359255194664
- 134.989852309227
- 140.80127483606339
- 143.05424004793167
- 137.70247781276703
- 132.01077926158905
- 125.89796030521393
- 140.50000673532486
- 153.04752606153488
- 159.39965122938156
- 148.85083329677582
- 128.78441458940506
- 151.45033484697342
- 130.76113444566727
- 132.21092504262924
- 144.0579937696457
- 154.63849651813507
- 143.7746461033821
- 139.52523523569107
- 149.95434093475342
- 147.6094889640808
- 147.01173001527786
- 129.94062560796738
- 141.99686563014984
- 124.2980278134346
- 135.91564625501633
- 144.47061967849731
- 129.07918047904968
- 126.21053063869476
- 129.12725973129272
- 119.61603373289108
- 128.40865528583527
- 129.650737285614
- 140.68680953979492
- 131.96119278669357
- 131.60718780755997
- 140.06980222463608
- 157.062295794487
- 128.577949821949
- 136.504558801651
- 144.49751955270767
- 131.293654859066
- 140.7408491373062
- 127.39788573980331
- 139.921914935112
- 159.36889827251434
- 146.50509935617447
- 148.73552334308624
- 139.3334293961525
- 149.58480155467987
- 124.23389720916748
train_accuracy:
- 0.0
- 0.468
- 0.475
- 0.453
- 0.45
- 0.131
- 0.78
- 0.717
- 0.5
- 0.675
- 0.655
- 0.172
- 0.325
- 0.89
- 0.88
- 0.836
- 0.579
- 0.238
- 0.447
- 0.525
- 0.7
- 0.92
- 0.967
- 0.883
- 0.747
- 0.917
- 0.742
- 0.658
- 0.71
- 0.912
- 0.538
- 0.821
- 0.647
- 0.925
- 0.75
- 0.917
- 0.9
- 0.9
- 0.585
- 0.513
- 0.383
- 0.318
- 0.208
- 0.867
- 1.0
- 0.7
- 0.95
- 0.731
- 0.87
- 0.92
- 0.53
- 0.22
- 0.95
- 0.6
- 0.78
- 0.85
- 0.71
- 0.0
- 0.379
- 0.9
- 0.715
- 0.754
- 0.47
- 0.925
- 0.914
- 0.874
- 0.755
- 0.443
- 0.714
- 0.471
- 0.823
- 0.643
- 0.817
- 0.692
- 0.96
- 0.686
- 0.497
- 0.789
- 0.375
- 0.7
- 0.65
- 0.933
- 0.765
- 0.538
- 0.838
- 0.95
- 0.445
- 0.518
- 0.49
- 0.518
- 0.89
- 0.395
- 0.819
- 0.514
- 0.629
- 0.753
- 0.654
- 0.685
- 0.825
- 0.825
train_loss:
- 0.98
- 0.828
- 0.754
- 0.591
- 0.527
- 0.569
- 0.394
- 0.476
- 0.417
- 0.479
- 0.47
- 0.487
- 0.36
- 0.45
- 0.433
- 0.376
- 0.381
- 0.471
- 0.415
- 0.371
- 0.41
- 0.377
- 0.471
- 0.285
- 0.402
- 0.38
- 0.322
- 0.384
- 0.316
- 0.308
- 0.404
- 0.347
- 0.365
- 0.391
- 0.322
- 0.381
- 0.297
- 0.364
- 0.363
- 0.311
- 0.443
- 0.373
- 0.401
- 0.277
- 0.402
- 0.389
- 0.368
- 0.258
- 0.353
- 0.238
- 0.349
- 0.36
- 0.352
- 0.412
- 0.374
- 0.263
- 0.301
- 0.278
- 0.257
- 0.317
- 0.332
- 0.361
- 0.347
- 0.338
- 0.35
- 0.397
- 0.374
- 0.368
- 0.408
- 0.269
- 0.303
- 0.322
- 0.328
- 0.396
- 0.269
- 0.336
- 0.354
- 0.433
- 0.341
- 0.297
- 0.318
- 0.283
- 0.392
- 0.257
- 0.394
- 0.34
- 0.425
- 0.358
- 0.363
- 0.297
- 0.346
- 0.322
- 0.309
- 0.275
- 0.255
- 0.315
- 0.278
- 0.375
- 0.34
- 0.311
unequal: 1
verbose: 1
