avg_train_accuracy: 0.744
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1751595744680851
- 0.20303191489361702
- 0.3078723404255319
- 0.3608510638297872
- 0.46335106382978725
- 0.40856382978723405
- 0.42090425531914893
- 0.5143085106382979
- 0.4572340425531915
- 0.4607978723404255
- 0.5681382978723404
- 0.5254255319148936
- 0.5692021276595745
- 0.5545212765957447
- 0.5720744680851064
- 0.5907446808510638
- 0.5974468085106382
- 0.611968085106383
- 0.6326595744680851
- 0.6482446808510638
- 0.5653723404255319
- 0.6111702127659574
- 0.6129787234042553
- 0.6118617021276596
- 0.6336702127659575
- 0.6017021276595744
- 0.636968085106383
- 0.6294148936170213
- 0.6098404255319149
- 0.6017553191489362
- 0.6103191489361702
- 0.5982446808510639
- 0.6355851063829787
- 0.6036170212765958
- 0.603563829787234
- 0.6732978723404255
- 0.6856914893617021
- 0.6186170212765958
- 0.6204255319148936
- 0.6051063829787234
- 0.5999468085106383
- 0.6446808510638298
- 0.6786702127659574
- 0.6704787234042553
- 0.686595744680851
- 0.6539893617021276
- 0.6432446808510638
- 0.6495212765957447
- 0.6477127659574468
- 0.6534042553191489
- 0.6650531914893617
- 0.5959574468085106
- 0.6719148936170213
- 0.6543617021276595
- 0.6716489361702128
- 0.7110106382978724
- 0.6877659574468085
- 0.6916489361702127
- 0.6437765957446808
- 0.6346808510638298
- 0.6400531914893617
- 0.6830851063829787
- 0.688936170212766
- 0.6168085106382979
- 0.6714893617021277
- 0.6489893617021276
- 0.678936170212766
- 0.6519680851063829
- 0.6157446808510638
- 0.6295212765957446
- 0.7006382978723404
- 0.6731382978723405
- 0.683031914893617
- 0.6972872340425532
- 0.703031914893617
- 0.6854255319148936
- 0.6510638297872341
- 0.6863297872340426
- 0.6540957446808511
- 0.6662765957446809
- 0.6938829787234042
- 0.668936170212766
- 0.6537765957446808
- 0.6601595744680852
- 0.6596276595744681
- 0.6429255319148937
- 0.6638297872340425
- 0.6873404255319149
- 0.6769148936170213
- 0.6459574468085106
- 0.6916489361702127
- 0.6775
- 0.6727659574468086
- 0.6897340425531915
- 0.6832446808510638
- 0.6662765957446809
- 0.668936170212766
- 0.6639893617021276
- 0.6893085106382979
- 0.6957446808510638
test_loss_list:
- 527.8105537891388
- 455.350200176239
- 375.75410890579224
- 323.5689070224762
- 272.5479372739792
- 295.9533029794693
- 278.24482023715973
- 235.8960655927658
- 240.7408685684204
- 251.8840538263321
- 212.0053368806839
- 210.14752328395844
- 189.18469840288162
- 191.97553026676178
- 195.91157591342926
- 178.32921493053436
- 172.8399852514267
- 173.9707236289978
- 163.3936641216278
- 162.1527597308159
- 192.68632566928864
- 174.93446815013885
- 164.04411762952805
- 171.71148663759232
- 154.30562323331833
- 171.79406195878983
- 159.60630297660828
- 158.73794031143188
- 167.8871118426323
- 163.92872458696365
- 158.29246485233307
- 159.50123977661133
- 146.92782825231552
- 177.73021012544632
- 168.56865787506104
- 137.4355878829956
- 146.57084983587265
- 152.98839282989502
- 161.9933408498764
- 164.50292325019836
- 151.9366734623909
- 151.45727998018265
- 136.1275023818016
- 139.04476314783096
- 133.1879798769951
- 145.5368528366089
- 145.1101717352867
- 143.58391588926315
- 151.04831236600876
- 140.49801343679428
- 146.9500417113304
- 175.2000675201416
- 134.8827309012413
- 142.50891256332397
- 138.0096788406372
- 120.86913859844208
- 133.32040125131607
- 129.02916437387466
- 146.06680834293365
- 152.88261723518372
- 145.98219239711761
- 123.86320948600769
- 130.31688177585602
- 166.8310490846634
- 135.88473731279373
- 145.12687319517136
- 137.20651882886887
- 158.77059268951416
- 173.00097113847733
- 148.20246106386185
- 129.82963919639587
- 144.17092734575272
- 128.61391657590866
- 124.68824422359467
- 131.9391669034958
- 130.9145064353943
- 145.07913917303085
- 135.12491643428802
- 138.55208176374435
- 139.03050827980042
- 132.49652582406998
- 136.55296671390533
- 146.33530312776566
- 146.60831570625305
- 144.38721579313278
- 152.40256130695343
- 155.89570313692093
- 124.49377113580704
- 129.72612810134888
- 137.55627328157425
- 129.0576022863388
- 139.5552058815956
- 143.33855485916138
- 131.07426398992538
- 128.6131910085678
- 133.51616233587265
- 136.60198628902435
- 139.00927329063416
- 130.00848680734634
- 120.25492244958878
train_accuracy:
- 0.3
- 0.0
- 0.214
- 0.382
- 0.679
- 0.433
- 0.83
- 0.017
- 0.706
- 0.192
- 0.65
- 0.95
- 0.883
- 0.473
- 0.343
- 0.925
- 0.667
- 0.853
- 0.85
- 0.494
- 0.496
- 0.442
- 0.925
- 0.918
- 0.659
- 0.647
- 0.464
- 0.9
- 0.619
- 0.619
- 0.65
- 0.45
- 0.309
- 0.92
- 0.808
- 0.961
- 0.87
- 0.561
- 0.95
- 0.907
- 0.489
- 0.782
- 0.95
- 0.665
- 0.967
- 0.897
- 0.12
- 0.864
- 0.542
- 0.386
- 0.346
- 0.692
- 0.975
- 0.211
- 0.565
- 0.784
- 0.561
- 0.694
- 0.821
- 0.37
- 0.893
- 0.493
- 0.817
- 0.427
- 0.678
- 0.715
- 0.696
- 0.833
- 0.975
- 0.894
- 0.806
- 0.882
- 0.817
- 0.864
- 0.933
- 0.935
- 0.559
- 0.875
- 0.458
- 0.625
- 0.624
- 0.459
- 0.3
- 0.8
- 0.95
- 0.967
- 0.933
- 0.586
- 0.674
- 0.234
- 0.62
- 0.875
- 0.88
- 0.685
- 0.933
- 0.667
- 0.559
- 0.284
- 0.929
- 0.744
train_loss:
- 1.238
- 0.786
- 0.7
- 0.662
- 0.554
- 0.48
- 0.597
- 0.437
- 0.41
- 0.527
- 0.492
- 0.401
- 0.387
- 0.421
- 0.415
- 0.412
- 0.406
- 0.464
- 0.369
- 0.314
- 0.391
- 0.39
- 0.395
- 0.38
- 0.364
- 0.345
- 0.364
- 0.293
- 0.396
- 0.425
- 0.392
- 0.394
- 0.375
- 0.303
- 0.315
- 0.339
- 0.356
- 0.348
- 0.286
- 0.459
- 0.394
- 0.36
- 0.346
- 0.273
- 0.304
- 0.399
- 0.355
- 0.365
- 0.295
- 0.344
- 0.353
- 0.323
- 0.339
- 0.366
- 0.325
- 0.31
- 0.359
- 0.374
- 0.379
- 0.368
- 0.328
- 0.388
- 0.299
- 0.253
- 0.303
- 0.347
- 0.352
- 0.321
- 0.269
- 0.374
- 0.325
- 0.491
- 0.271
- 0.357
- 0.225
- 0.363
- 0.404
- 0.281
- 0.345
- 0.31
- 0.296
- 0.349
- 0.336
- 0.299
- 0.34
- 0.324
- 0.291
- 0.328
- 0.358
- 0.349
- 0.326
- 0.329
- 0.295
- 0.302
- 0.326
- 0.257
- 0.282
- 0.314
- 0.371
- 0.29
unequal: 1
verbose: 1
