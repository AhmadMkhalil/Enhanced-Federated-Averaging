avg_train_accuracy: 0.858
avg_train_loss: 0.004
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1425531914893617
- 0.26420212765957446
- 0.3095212765957447
- 0.3227127659574468
- 0.37122340425531913
- 0.46170212765957447
- 0.38601063829787235
- 0.47569148936170214
- 0.43425531914893617
- 0.47569148936170214
- 0.45335106382978724
- 0.49409574468085105
- 0.5845744680851064
- 0.5690957446808511
- 0.5693617021276596
- 0.6340957446808511
- 0.5841489361702128
- 0.5788829787234042
- 0.6227127659574468
- 0.611968085106383
- 0.6105851063829787
- 0.6215425531914893
- 0.5390957446808511
- 0.6059574468085106
- 0.6298936170212766
- 0.5888829787234042
- 0.604468085106383
- 0.6020744680851063
- 0.595531914893617
- 0.6161170212765957
- 0.5761170212765957
- 0.6511170212765958
- 0.6622872340425532
- 0.6150531914893617
- 0.6773936170212767
- 0.5884574468085106
- 0.6435106382978724
- 0.6569148936170213
- 0.589627659574468
- 0.6195744680851064
- 0.5931914893617021
- 0.6260638297872341
- 0.6693617021276596
- 0.6330851063829788
- 0.6099468085106383
- 0.6291489361702127
- 0.6732978723404255
- 0.6570212765957447
- 0.6518085106382979
- 0.681063829787234
- 0.7
- 0.6697340425531915
- 0.6645212765957447
- 0.6310106382978723
- 0.6770744680851064
- 0.6722872340425532
- 0.6619148936170213
- 0.6322872340425532
- 0.666968085106383
- 0.6463297872340426
- 0.6346808510638298
- 0.6154255319148936
- 0.6706914893617021
- 0.7040425531914893
- 0.6870744680851064
- 0.6748936170212766
- 0.634468085106383
- 0.6784042553191489
- 0.7004787234042553
- 0.6753191489361702
- 0.7245212765957447
- 0.6753191489361702
- 0.6302127659574468
- 0.6348404255319149
- 0.6229255319148936
- 0.6259574468085106
- 0.6811702127659575
- 0.7027127659574468
- 0.6842553191489362
- 0.6917021276595745
- 0.6656382978723404
- 0.6363829787234042
- 0.6539893617021276
- 0.6973936170212766
- 0.65
- 0.6683510638297873
- 0.6287765957446808
- 0.6566489361702128
- 0.6431382978723404
- 0.6582978723404256
- 0.6662234042553191
- 0.694468085106383
- 0.6972340425531914
- 0.6770212765957446
- 0.6648936170212766
- 0.7073936170212766
- 0.7023404255319149
- 0.6943617021276596
- 0.6472872340425532
- 0.6975
test_loss_list:
- 534.724755525589
- 455.69685220718384
- 402.29043984413147
- 373.74568819999695
- 325.6017407178879
- 266.5608365535736
- 293.61200749874115
- 253.53515791893005
- 271.78986072540283
- 238.1609367132187
- 233.26645731925964
- 222.13552510738373
- 190.88865679502487
- 196.49018716812134
- 193.50460028648376
- 169.5165039896965
- 200.9121025800705
- 184.72131633758545
- 164.70291179418564
- 171.51069456338882
- 165.78601974248886
- 182.61392694711685
- 191.7671493291855
- 169.70362949371338
- 165.58434736728668
- 175.36357927322388
- 170.78830206394196
- 167.39191955327988
- 162.26154911518097
- 160.5283253788948
- 175.13075107336044
- 162.25308042764664
- 149.98703014850616
- 167.99446338415146
- 146.6260638833046
- 169.76882964372635
- 150.88747853040695
- 141.27718830108643
- 158.70010423660278
- 152.2676140666008
- 161.24329435825348
- 162.81063950061798
- 136.56438237428665
- 153.7581768631935
- 168.73987311124802
- 149.64276134967804
- 138.04954981803894
- 141.51209384202957
- 154.47485864162445
- 142.02675515413284
- 140.24398285150528
- 129.16902953386307
- 157.0070300102234
- 151.983103454113
- 144.00094830989838
- 141.73263812065125
- 136.5795984864235
- 153.4054138660431
- 137.6838777065277
- 150.4153909087181
- 150.96609675884247
- 165.86564725637436
- 136.65638422966003
- 124.84390765428543
- 134.43777072429657
- 137.31998538970947
- 146.23793733119965
- 135.6703062057495
- 126.00279092788696
- 135.15011954307556
- 122.0310400724411
- 134.00217962265015
- 158.54066133499146
- 159.37615472078323
- 160.46836298704147
- 151.1294903755188
- 147.99571979045868
- 128.54017913341522
- 137.20124852657318
- 137.5432989001274
- 141.58541297912598
- 150.50632482767105
- 149.5755912065506
- 123.07126355171204
- 146.9836277961731
- 134.02798026800156
- 148.85046541690826
- 136.92801016569138
- 150.45676916837692
- 150.53531658649445
- 139.06070184707642
- 125.11849892139435
- 122.27245628833771
- 131.37076383829117
- 142.90144383907318
- 118.43705981969833
- 122.97114878892899
- 130.9709963798523
- 160.32967692613602
- 125.60189521312714
train_accuracy:
- 0.0
- 0.443
- 0.379
- 0.812
- 0.02
- 0.119
- 0.1
- 0.845
- 0.612
- 0.629
- 0.668
- 0.786
- 0.35
- 0.067
- 0.958
- 0.538
- 0.516
- 0.785
- 0.0
- 0.911
- 0.606
- 0.808
- 0.264
- 0.409
- 0.705
- 0.412
- 0.8
- 0.277
- 0.55
- 0.0
- 0.9
- 0.4
- 0.978
- 0.603
- 0.737
- 0.59
- 0.871
- 0.793
- 0.033
- 0.904
- 0.57
- 0.665
- 0.067
- 0.729
- 0.967
- 0.908
- 0.344
- 0.494
- 0.568
- 0.941
- 0.91
- 0.95
- 0.505
- 0.79
- 0.633
- 0.592
- 0.733
- 0.811
- 0.713
- 0.671
- 0.655
- 0.474
- 0.875
- 0.763
- 0.781
- 0.906
- 0.623
- 0.79
- 0.212
- 0.85
- 0.842
- 0.65
- 0.35
- 0.541
- 0.921
- 0.963
- 0.718
- 0.827
- 0.723
- 0.827
- 0.754
- 0.935
- 0.633
- 0.819
- 0.989
- 0.258
- 0.635
- 0.75
- 0.904
- 0.75
- 0.829
- 0.556
- 0.733
- 0.737
- 0.627
- 0.679
- 0.8
- 0.9
- 0.185
- 0.858
train_loss:
- 1.12
- 0.729
- 0.613
- 0.613
- 0.601
- 0.425
- 0.484
- 0.457
- 0.52
- 0.485
- 0.479
- 0.416
- 0.446
- 0.401
- 0.385
- 0.39
- 0.371
- 0.469
- 0.383
- 0.318
- 0.424
- 0.437
- 0.404
- 0.332
- 0.357
- 0.415
- 0.424
- 0.362
- 0.309
- 0.412
- 0.303
- 0.286
- 0.326
- 0.328
- 0.317
- 0.312
- 0.36
- 0.376
- 0.317
- 0.296
- 0.366
- 0.318
- 0.377
- 0.366
- 0.331
- 0.352
- 0.319
- 0.31
- 0.381
- 0.319
- 0.323
- 0.299
- 0.347
- 0.26
- 0.326
- 0.369
- 0.303
- 0.312
- 0.359
- 0.391
- 0.304
- 0.352
- 0.316
- 0.321
- 0.411
- 0.285
- 0.354
- 0.262
- 0.272
- 0.353
- 0.326
- 0.348
- 0.292
- 0.331
- 0.304
- 0.369
- 0.345
- 0.322
- 0.302
- 0.263
- 0.316
- 0.378
- 0.357
- 0.283
- 0.342
- 0.303
- 0.364
- 0.351
- 0.327
- 0.318
- 0.323
- 0.345
- 0.372
- 0.269
- 0.409
- 0.252
- 0.276
- 0.232
- 0.226
- 0.365
unequal: 1
verbose: 1
