avg_train_accuracy: 0.938
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1295212765957447
- 0.23031914893617023
- 0.2982446808510638
- 0.36638297872340425
- 0.42164893617021276
- 0.3968617021276596
- 0.4848936170212766
- 0.4871808510638298
- 0.5547872340425531
- 0.49122340425531913
- 0.5169680851063829
- 0.565
- 0.5805851063829788
- 0.6066489361702128
- 0.6181382978723404
- 0.6138829787234042
- 0.5711702127659575
- 0.6076595744680852
- 0.5765957446808511
- 0.5666489361702127
- 0.6601595744680852
- 0.6163829787234043
- 0.5942021276595745
- 0.6463829787234042
- 0.5621276595744681
- 0.576063829787234
- 0.6184574468085107
- 0.6075
- 0.6802127659574468
- 0.6579255319148937
- 0.6040425531914894
- 0.5849468085106383
- 0.6442553191489362
- 0.6295212765957446
- 0.6691489361702128
- 0.6289361702127659
- 0.6454787234042553
- 0.6750531914893617
- 0.6533510638297872
- 0.7148936170212766
- 0.6843617021276596
- 0.6639893617021276
- 0.6498404255319149
- 0.6538297872340425
- 0.6434042553191489
- 0.6188297872340426
- 0.6251595744680851
- 0.666968085106383
- 0.6405851063829787
- 0.6889893617021277
- 0.6869148936170213
- 0.6895744680851064
- 0.6818085106382978
- 0.6463829787234042
- 0.6952659574468085
- 0.6902127659574468
- 0.6648936170212766
- 0.6950531914893617
- 0.6617021276595745
- 0.6493085106382979
- 0.6619680851063829
- 0.6938829787234042
- 0.6210638297872341
- 0.6775531914893617
- 0.6925531914893617
- 0.6849468085106383
- 0.6555851063829787
- 0.6555319148936171
- 0.6355851063829787
- 0.6575
- 0.6747872340425531
- 0.7132978723404255
- 0.7006914893617021
- 0.7127659574468085
- 0.7147872340425532
- 0.6843085106382979
- 0.6599468085106382
- 0.695531914893617
- 0.6953723404255319
- 0.7178723404255319
- 0.6978191489361703
- 0.7097340425531915
- 0.6821808510638298
- 0.7097340425531915
- 0.7132978723404255
- 0.6776063829787234
- 0.7152659574468085
- 0.6870212765957446
- 0.6651595744680852
- 0.6738297872340425
- 0.6793085106382979
- 0.6842553191489362
- 0.665904255319149
- 0.663031914893617
- 0.6789893617021276
- 0.6726063829787234
- 0.6921808510638298
- 0.6627659574468086
- 0.6668617021276596
- 0.6893617021276596
test_loss_list:
- 534.4445238113403
- 448.0859341621399
- 385.45672130584717
- 342.222779750824
- 296.7152945995331
- 281.815168261528
- 246.39707589149475
- 256.16303837299347
- 206.23774552345276
- 227.8742549419403
- 220.87493443489075
- 203.68320429325104
- 192.59101796150208
- 187.5452840924263
- 170.1692432165146
- 171.6017460823059
- 195.87512075901031
- 178.42495769262314
- 177.1972878575325
- 188.15858244895935
- 167.07432341575623
- 169.04334771633148
- 180.3701406121254
- 157.50554978847504
- 184.24554347991943
- 191.77333855628967
- 162.9594457745552
- 163.4433267712593
- 142.16488987207413
- 145.25874388217926
- 177.10297161340714
- 193.69914436340332
- 145.68246871232986
- 156.93220037221909
- 149.48000973463058
- 153.6414675116539
- 161.17134642601013
- 133.86903721094131
- 140.8005753159523
- 126.52043032646179
- 137.19093024730682
- 144.30029368400574
- 158.24602115154266
- 138.6950644850731
- 152.46237367391586
- 166.8554052710533
- 163.04879069328308
- 142.80735379457474
- 141.63237196207047
- 130.81866347789764
- 137.50919753313065
- 128.98542815446854
- 135.58655726909637
- 143.35943859815598
- 132.7985102534294
- 127.70805609226227
- 144.4991010427475
- 137.82338869571686
- 133.77805387973785
- 149.6792985200882
- 134.64737486839294
- 122.17144072055817
- 167.44970148801804
- 133.8305270075798
- 129.91391170024872
- 130.07798039913177
- 144.4327186346054
- 149.76922225952148
- 164.32925444841385
- 149.59011763334274
- 135.80671966075897
- 128.38292360305786
- 127.9589895606041
- 121.33507990837097
- 120.60337519645691
- 146.63129419088364
- 144.26862585544586
- 124.35536992549896
- 127.10108959674835
- 134.96360105276108
- 121.89265239238739
- 138.88142812252045
- 137.94073921442032
- 134.1307816505432
- 120.47035485506058
- 140.5308622121811
- 125.75036764144897
- 126.27795475721359
- 144.77160149812698
- 137.60675424337387
- 130.87010049819946
- 123.19947075843811
- 135.85919338464737
- 135.2871703505516
- 125.5037772655487
- 127.86779361963272
- 117.81913697719574
- 130.17745596170425
- 131.3168185353279
- 134.1196946501732
train_accuracy:
- 0.007
- 0.09
- 0.089
- 0.25
- 0.016
- 0.771
- 0.473
- 0.331
- 0.17
- 0.27
- 0.732
- 0.747
- 0.875
- 0.478
- 0.43
- 0.306
- 0.666
- 0.317
- 0.542
- 0.286
- 0.787
- 0.75
- 0.694
- 0.593
- 0.857
- 0.518
- 0.581
- 0.9
- 1.0
- 0.356
- 0.911
- 0.331
- 0.767
- 0.4
- 0.283
- 0.825
- 0.817
- 0.671
- 0.81
- 0.275
- 0.875
- 0.763
- 0.633
- 0.53
- 0.95
- 0.975
- 0.0
- 0.908
- 0.8
- 0.47
- 0.873
- 0.831
- 0.567
- 0.763
- 0.808
- 0.975
- 0.41
- 0.9
- 0.477
- 0.855
- 0.863
- 0.98
- 0.378
- 0.68
- 0.835
- 0.791
- 0.342
- 0.52
- 0.867
- 0.786
- 0.91
- 0.98
- 0.783
- 0.847
- 1.0
- 0.642
- 0.846
- 0.715
- 0.356
- 0.942
- 0.221
- 0.417
- 0.98
- 0.8
- 0.65
- 0.492
- 0.85
- 0.243
- 0.814
- 0.917
- 0.917
- 0.783
- 0.8
- 0.987
- 0.92
- 0.894
- 0.69
- 0.408
- 0.838
- 0.938
train_loss:
- 1.033
- 0.829
- 0.63
- 0.592
- 0.56
- 0.512
- 0.549
- 0.53
- 0.425
- 0.49
- 0.419
- 0.396
- 0.41
- 0.363
- 0.43
- 0.383
- 0.353
- 0.46
- 0.456
- 0.472
- 0.314
- 0.349
- 0.415
- 0.439
- 0.403
- 0.384
- 0.399
- 0.389
- 0.405
- 0.347
- 0.375
- 0.42
- 0.375
- 0.336
- 0.352
- 0.354
- 0.272
- 0.351
- 0.323
- 0.291
- 0.325
- 0.334
- 0.3
- 0.385
- 0.312
- 0.266
- 0.289
- 0.364
- 0.347
- 0.294
- 0.339
- 0.295
- 0.293
- 0.344
- 0.347
- 0.317
- 0.329
- 0.334
- 0.384
- 0.312
- 0.385
- 0.25
- 0.342
- 0.369
- 0.278
- 0.331
- 0.39
- 0.323
- 0.331
- 0.323
- 0.343
- 0.26
- 0.366
- 0.377
- 0.319
- 0.302
- 0.383
- 0.274
- 0.27
- 0.258
- 0.333
- 0.207
- 0.327
- 0.335
- 0.242
- 0.363
- 0.29
- 0.33
- 0.319
- 0.286
- 0.329
- 0.303
- 0.291
- 0.283
- 0.344
- 0.356
- 0.264
- 0.343
- 0.275
- 0.327
unequal: 1
verbose: 1
