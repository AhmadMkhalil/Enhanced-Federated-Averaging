avg_train_accuracy: 0.806
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1225
- 0.23648936170212767
- 0.34904255319148936
- 0.3243617021276596
- 0.43340425531914895
- 0.42281914893617023
- 0.46164893617021274
- 0.4959574468085106
- 0.4640957446808511
- 0.574468085106383
- 0.5026595744680851
- 0.5467553191489362
- 0.5709042553191489
- 0.6057446808510638
- 0.5609574468085107
- 0.5972872340425532
- 0.6053191489361702
- 0.6009042553191489
- 0.6160638297872341
- 0.5635106382978723
- 0.6248936170212765
- 0.620904255319149
- 0.6263297872340425
- 0.6119148936170212
- 0.595531914893617
- 0.5820744680851064
- 0.6677127659574468
- 0.6524468085106383
- 0.6313297872340425
- 0.6540425531914894
- 0.629095744680851
- 0.6461170212765958
- 0.5821276595744681
- 0.631436170212766
- 0.5930851063829787
- 0.6403723404255319
- 0.6549468085106382
- 0.6499468085106384
- 0.6684042553191489
- 0.6361702127659574
- 0.6390425531914894
- 0.6875531914893617
- 0.6895744680851064
- 0.6573936170212766
- 0.6586702127659575
- 0.6584042553191489
- 0.6314893617021277
- 0.6498936170212766
- 0.676968085106383
- 0.6707446808510639
- 0.6606382978723404
- 0.6651595744680852
- 0.679468085106383
- 0.6775
- 0.6610106382978723
- 0.6489893617021276
- 0.6525
- 0.6932446808510638
- 0.6931914893617022
- 0.6756382978723404
- 0.6986702127659574
- 0.6975531914893617
- 0.6738297872340425
- 0.6328723404255319
- 0.696063829787234
- 0.6569680851063829
- 0.6616489361702128
- 0.6616489361702128
- 0.6807978723404255
- 0.6400531914893617
- 0.6332446808510638
- 0.6707446808510639
- 0.6305851063829787
- 0.68
- 0.6832446808510638
- 0.6365425531914893
- 0.6815425531914894
- 0.6762234042553191
- 0.6972872340425532
- 0.6841489361702128
- 0.705531914893617
- 0.7147340425531915
- 0.7084574468085106
- 0.6707978723404255
- 0.7165425531914894
- 0.7085106382978723
- 0.7008510638297872
- 0.6914361702127659
- 0.7072340425531914
- 0.7104255319148937
- 0.7293617021276596
- 0.6891489361702128
- 0.6757446808510639
- 0.6751063829787234
- 0.7092021276595745
- 0.7029255319148936
- 0.6989893617021277
- 0.7092553191489361
- 0.6801063829787234
- 0.7220212765957447
test_loss_list:
- 537.8582911491394
- 433.91482496261597
- 362.9829182624817
- 351.3104827404022
- 294.8259621858597
- 294.8490160703659
- 247.34323477745056
- 241.354478597641
- 241.60411405563354
- 204.29147100448608
- 226.93028485774994
- 208.49905729293823
- 192.76799488067627
- 188.22059035301208
- 203.873211145401
- 185.38335514068604
- 180.83123016357422
- 169.47359365224838
- 173.56613570451736
- 184.8054340481758
- 167.63631922006607
- 163.04825115203857
- 166.39810824394226
- 179.83331233263016
- 174.8203485608101
- 174.23330122232437
- 147.5729489326477
- 155.214783847332
- 157.090172290802
- 155.50795328617096
- 158.8846651315689
- 153.8631586432457
- 171.6615988612175
- 150.3828929066658
- 184.7710044980049
- 162.54333341121674
- 151.85003447532654
- 157.59115117788315
- 146.53192800283432
- 154.63480204343796
- 147.91815066337585
- 136.65578001737595
- 135.86881113052368
- 151.50900942087173
- 142.81015515327454
- 148.53301239013672
- 163.46564608812332
- 152.76388442516327
- 142.69004648923874
- 157.3508362174034
- 154.58900207281113
- 140.34819501638412
- 137.17002069950104
- 129.4737366437912
- 146.26868414878845
- 157.64778232574463
- 140.78966289758682
- 135.61641252040863
- 134.32671880722046
- 139.63306337594986
- 135.84307169914246
- 122.53218561410904
- 137.9626965522766
- 155.72985011339188
- 125.59540945291519
- 141.51492702960968
- 142.2265437245369
- 141.03378170728683
- 135.92226469516754
- 145.27371299266815
- 151.86011046171188
- 147.468568444252
- 154.9601667523384
- 126.78803086280823
- 128.37873929738998
- 143.25592321157455
- 134.27220141887665
- 137.77012413740158
- 132.2099022269249
- 131.18033927679062
- 122.05295759439468
- 123.46287822723389
- 123.830182492733
- 137.22678977251053
- 118.91113358736038
- 123.35633164644241
- 131.70142424106598
- 129.77717876434326
- 124.79686218500137
- 140.67151695489883
- 119.72404217720032
- 140.31683206558228
- 146.658800303936
- 139.76284128427505
- 134.43095880746841
- 130.85386323928833
- 127.13458859920502
- 120.14636343717575
- 127.72251415252686
- 115.65489268302917
train_accuracy:
- 0.241
- 0.019
- 0.282
- 0.337
- 0.9
- 0.275
- 0.45
- 0.475
- 0.883
- 0.577
- 0.481
- 0.574
- 0.655
- 0.45
- 0.963
- 0.517
- 0.225
- 0.518
- 0.815
- 0.618
- 0.403
- 0.742
- 0.75
- 0.647
- 0.225
- 0.543
- 0.567
- 0.771
- 0.868
- 0.85
- 0.708
- 0.835
- 0.2
- 0.708
- 0.888
- 0.828
- 0.583
- 0.705
- 0.4
- 0.88
- 0.632
- 0.909
- 0.35
- 0.748
- 0.959
- 0.712
- 0.613
- 0.542
- 0.731
- 0.923
- 0.581
- 0.963
- 0.633
- 0.921
- 0.96
- 0.637
- 0.95
- 0.5
- 0.672
- 0.779
- 0.675
- 0.635
- 0.843
- 0.914
- 0.792
- 0.925
- 0.573
- 0.786
- 0.867
- 0.388
- 0.604
- 0.833
- 0.461
- 0.408
- 0.74
- 0.911
- 0.65
- 0.67
- 0.793
- 0.462
- 0.858
- 0.654
- 0.42
- 0.83
- 0.413
- 0.825
- 0.9
- 0.893
- 0.964
- 0.942
- 0.45
- 0.388
- 0.342
- 0.83
- 0.663
- 0.91
- 0.553
- 0.782
- 0.918
- 0.806
train_loss:
- 1.257
- 0.853
- 0.679
- 0.605
- 0.445
- 0.569
- 0.544
- 0.487
- 0.487
- 0.448
- 0.49
- 0.458
- 0.408
- 0.491
- 0.4
- 0.38
- 0.422
- 0.466
- 0.406
- 0.382
- 0.43
- 0.458
- 0.39
- 0.391
- 0.36
- 0.405
- 0.39
- 0.329
- 0.457
- 0.407
- 0.366
- 0.261
- 0.346
- 0.362
- 0.351
- 0.343
- 0.341
- 0.384
- 0.363
- 0.31
- 0.4
- 0.388
- 0.254
- 0.408
- 0.367
- 0.418
- 0.371
- 0.358
- 0.398
- 0.386
- 0.338
- 0.319
- 0.395
- 0.341
- 0.338
- 0.345
- 0.272
- 0.444
- 0.357
- 0.333
- 0.4
- 0.365
- 0.364
- 0.385
- 0.28
- 0.279
- 0.362
- 0.341
- 0.337
- 0.393
- 0.367
- 0.301
- 0.354
- 0.368
- 0.329
- 0.32
- 0.378
- 0.334
- 0.404
- 0.342
- 0.283
- 0.297
- 0.374
- 0.283
- 0.322
- 0.301
- 0.381
- 0.375
- 0.337
- 0.4
- 0.297
- 0.34
- 0.355
- 0.329
- 0.334
- 0.37
- 0.373
- 0.344
- 0.338
- 0.3
unequal: 1
verbose: 1
