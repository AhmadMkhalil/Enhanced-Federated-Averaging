avg_train_accuracy: 0.87
avg_train_loss: 0.003
avg_type: avg
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.12292553191489362
- 0.23675531914893616
- 0.37382978723404253
- 0.4035106382978723
- 0.43303191489361703
- 0.4277127659574468
- 0.46638297872340423
- 0.46382978723404256
- 0.4901595744680851
- 0.603563829787234
- 0.5446808510638298
- 0.5462234042553191
- 0.593563829787234
- 0.5817553191489362
- 0.5820212765957447
- 0.5806382978723404
- 0.5735638297872341
- 0.5817021276595745
- 0.5912234042553192
- 0.6370212765957447
- 0.5907446808510638
- 0.5913297872340425
- 0.6431914893617021
- 0.6838297872340425
- 0.6124468085106383
- 0.6602127659574468
- 0.661595744680851
- 0.6248404255319149
- 0.6562765957446809
- 0.6385106382978724
- 0.6479787234042553
- 0.6334042553191489
- 0.6392553191489362
- 0.6031914893617021
- 0.6490425531914894
- 0.5923936170212766
- 0.6173936170212766
- 0.6284042553191489
- 0.6787234042553192
- 0.6498404255319149
- 0.6743085106382979
- 0.6296276595744681
- 0.6561170212765958
- 0.6702659574468085
- 0.7044680851063829
- 0.6473404255319148
- 0.6925
- 0.699468085106383
- 0.7050531914893617
- 0.6811702127659575
- 0.670372340425532
- 0.6681382978723405
- 0.6542553191489362
- 0.6534574468085106
- 0.695531914893617
- 0.6936170212765957
- 0.6538297872340425
- 0.6832446808510638
- 0.6704787234042553
- 0.6520744680851064
- 0.6826063829787234
- 0.6796808510638298
- 0.6667021276595745
- 0.6573404255319149
- 0.7028723404255319
- 0.7020212765957446
- 0.6961702127659575
- 0.7011170212765957
- 0.6987234042553192
- 0.7207446808510638
- 0.6866489361702127
- 0.6676063829787234
- 0.6870744680851064
- 0.7120744680851064
- 0.6857978723404256
- 0.6721808510638297
- 0.7130319148936171
- 0.6821276595744681
- 0.7059042553191489
- 0.6860106382978723
- 0.6965425531914894
- 0.7207978723404256
- 0.6827127659574468
- 0.7210106382978724
- 0.7193617021276596
- 0.6944148936170212
- 0.7078191489361703
- 0.686595744680851
- 0.7136170212765958
- 0.7265425531914894
- 0.7165425531914894
- 0.6940425531914893
- 0.6631382978723405
- 0.6947340425531915
- 0.6816489361702127
- 0.7023936170212766
- 0.708031914893617
- 0.6907446808510638
- 0.7106914893617021
- 0.7061702127659575
test_loss_list:
- 537.4990332126617
- 456.03613114356995
- 376.4212923049927
- 338.063773393631
- 309.44432532787323
- 277.92621552944183
- 278.38099586963654
- 246.99664628505707
- 239.85557329654694
- 187.36208403110504
- 221.1661115884781
- 207.77198159694672
- 188.44131290912628
- 183.9020447731018
- 189.6516625881195
- 173.48424220085144
- 185.22633969783783
- 177.44682759046555
- 184.3709909915924
- 165.0460206270218
- 181.51297461986542
- 175.6605041027069
- 165.54133141040802
- 144.6519848704338
- 166.90667736530304
- 160.58673816919327
- 150.09852838516235
- 160.29378062486649
- 145.87416315078735
- 147.02695924043655
- 147.92689275741577
- 153.4810929298401
- 149.7076061964035
- 165.3798570036888
- 148.54297667741776
- 164.5309001803398
- 169.9391409754753
- 162.4107502102852
- 133.4233290553093
- 142.0964499115944
- 136.4807722568512
- 151.96310651302338
- 143.83307057619095
- 138.90784347057343
- 121.36536437273026
- 151.27842217683792
- 137.2518737912178
- 147.90867966413498
- 122.70964550971985
- 135.2432965040207
- 138.32241946458817
- 138.56624203920364
- 139.97077083587646
- 154.58670222759247
- 144.82889479398727
- 129.5873927474022
- 163.2444747686386
- 133.47086894512177
- 139.04410272836685
- 156.43897104263306
- 143.77216744422913
- 133.150550365448
- 139.92621034383774
- 147.6973878145218
- 128.3299439549446
- 120.93993306159973
- 132.8321123123169
- 124.40134257078171
- 128.95603454113007
- 118.34135895967484
- 127.46335220336914
- 133.36267310380936
- 123.28613263368607
- 124.22849184274673
- 139.4711936712265
- 144.67179429531097
- 125.17011421918869
- 138.20758777856827
- 129.38279455900192
- 137.56301271915436
- 129.58477920293808
- 115.87853717803955
- 132.89751994609833
- 122.29006767272949
- 121.71155053377151
- 123.95307070016861
- 120.9593396782875
- 128.77831357717514
- 127.00853997468948
- 109.82540035247803
- 123.25001585483551
- 137.5064617395401
- 145.58827245235443
- 124.77920871973038
- 132.3076193332672
- 126.54402947425842
- 116.62893557548523
- 139.68384033441544
- 124.27418178319931
- 120.51959788799286
train_accuracy:
- 0.011
- 0.461
- 0.0
- 0.355
- 0.23
- 0.95
- 0.476
- 0.477
- 0.333
- 0.583
- 0.594
- 0.9
- 0.72
- 0.283
- 0.456
- 0.433
- 0.858
- 0.663
- 0.794
- 0.642
- 0.792
- 0.656
- 0.8
- 0.423
- 0.59
- 0.911
- 0.53
- 0.445
- 0.641
- 0.667
- 0.667
- 0.053
- 0.872
- 0.659
- 0.777
- 0.425
- 0.803
- 0.465
- 0.911
- 0.479
- 0.527
- 0.55
- 0.97
- 0.85
- 0.642
- 0.684
- 0.632
- 0.223
- 0.5
- 0.414
- 0.694
- 0.592
- 0.479
- 0.711
- 0.405
- 0.725
- 0.12
- 0.625
- 0.621
- 0.765
- 0.879
- 0.6
- 0.815
- 0.609
- 0.861
- 0.839
- 0.785
- 0.8
- 0.787
- 0.712
- 0.833
- 0.718
- 0.821
- 0.21
- 0.763
- 0.355
- 0.795
- 0.247
- 0.677
- 0.765
- 0.964
- 0.683
- 0.892
- 0.878
- 0.459
- 0.694
- 0.87
- 0.417
- 0.569
- 0.925
- 0.507
- 0.663
- 0.586
- 0.844
- 0.794
- 0.571
- 0.95
- 0.914
- 0.642
- 0.87
train_loss:
- 1.081
- 0.85
- 0.631
- 0.612
- 0.556
- 0.428
- 0.543
- 0.524
- 0.463
- 0.522
- 0.435
- 0.476
- 0.51
- 0.414
- 0.428
- 0.427
- 0.466
- 0.436
- 0.428
- 0.399
- 0.424
- 0.416
- 0.388
- 0.43
- 0.405
- 0.393
- 0.345
- 0.368
- 0.409
- 0.411
- 0.391
- 0.357
- 0.402
- 0.465
- 0.4
- 0.398
- 0.373
- 0.352
- 0.372
- 0.358
- 0.395
- 0.368
- 0.328
- 0.338
- 0.355
- 0.35
- 0.321
- 0.35
- 0.381
- 0.405
- 0.308
- 0.394
- 0.379
- 0.383
- 0.318
- 0.353
- 0.321
- 0.348
- 0.4
- 0.357
- 0.309
- 0.247
- 0.323
- 0.37
- 0.357
- 0.364
- 0.364
- 0.326
- 0.374
- 0.291
- 0.394
- 0.327
- 0.342
- 0.302
- 0.345
- 0.329
- 0.343
- 0.354
- 0.345
- 0.373
- 0.293
- 0.34
- 0.39
- 0.384
- 0.326
- 0.33
- 0.383
- 0.328
- 0.346
- 0.297
- 0.326
- 0.283
- 0.364
- 0.284
- 0.283
- 0.386
- 0.318
- 0.309
- 0.291
- 0.336
unequal: 1
verbose: 1
