avg_train_accuracy: 0.675
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.16702127659574467
- 0.27101063829787236
- 0.3364893617021277
- 0.3232978723404255
- 0.4473404255319149
- 0.46962765957446806
- 0.4920744680851064
- 0.4800531914893617
- 0.5289893617021276
- 0.5184042553191489
- 0.515904255319149
- 0.5057446808510638
- 0.564468085106383
- 0.558404255319149
- 0.5696808510638298
- 0.5475
- 0.5572872340425532
- 0.5982446808510639
- 0.6248404255319149
- 0.6420744680851064
- 0.565531914893617
- 0.5275
- 0.596968085106383
- 0.5915957446808511
- 0.606436170212766
- 0.603404255319149
- 0.5988297872340426
- 0.6462765957446809
- 0.6289361702127659
- 0.6295744680851064
- 0.6499468085106384
- 0.637340425531915
- 0.6287765957446808
- 0.6210106382978723
- 0.646595744680851
- 0.6337234042553191
- 0.637340425531915
- 0.6520744680851064
- 0.571968085106383
- 0.6597872340425532
- 0.6179255319148936
- 0.6475
- 0.6078191489361702
- 0.6549468085106382
- 0.5888829787234042
- 0.6855851063829788
- 0.6701063829787234
- 0.6410106382978723
- 0.6751063829787234
- 0.6301595744680851
- 0.6206382978723404
- 0.6776595744680851
- 0.6386702127659575
- 0.6609574468085107
- 0.5902127659574468
- 0.6517553191489361
- 0.5785638297872341
- 0.6196808510638298
- 0.658031914893617
- 0.6373936170212766
- 0.651595744680851
- 0.6520744680851064
- 0.6470212765957447
- 0.6420212765957447
- 0.6577659574468085
- 0.6908510638297872
- 0.6177659574468085
- 0.675159574468085
- 0.6306914893617022
- 0.6302659574468085
- 0.6654787234042553
- 0.6499468085106384
- 0.6287765957446808
- 0.671968085106383
- 0.6898936170212766
- 0.6442553191489362
- 0.5972872340425532
- 0.6234042553191489
- 0.6620212765957447
- 0.6735638297872341
- 0.6698404255319149
- 0.701968085106383
- 0.6488297872340425
- 0.6369148936170212
- 0.6584042553191489
- 0.6452127659574468
- 0.6365425531914893
- 0.6354787234042554
- 0.6708510638297872
- 0.64
- 0.6337234042553191
- 0.681063829787234
- 0.685372340425532
- 0.6758510638297872
- 0.6048936170212766
- 0.6716489361702128
- 0.6351063829787233
- 0.5986170212765958
- 0.6914361702127659
- 0.6527659574468085
test_loss_list:
- 522.2076346874237
- 428.3755724430084
- 363.4391396045685
- 388.15033769607544
- 315.8546984195709
- 273.4325089454651
- 245.31914007663727
- 246.81078457832336
- 228.65634095668793
- 225.10394513607025
- 229.5854583978653
- 214.0119287967682
- 195.13855385780334
- 184.22023355960846
- 183.30274468660355
- 185.08154678344727
- 195.41183042526245
- 179.27523934841156
- 165.49454194307327
- 159.01696890592575
- 196.563108086586
- 210.38154011964798
- 175.1615588068962
- 171.36136895418167
- 177.26569217443466
- 170.74240601062775
- 187.08314967155457
- 152.9832227230072
- 160.62804752588272
- 154.32726460695267
- 143.2007376551628
- 153.25603783130646
- 171.63572698831558
- 179.99340671300888
- 166.28901129961014
- 164.94835758209229
- 163.7242866754532
- 144.85746186971664
- 178.9706039428711
- 143.99876725673676
- 168.65223270654678
- 159.29918152093887
- 152.1980295777321
- 141.26876372098923
- 172.18321776390076
- 136.2370480298996
- 139.28940778970718
- 163.92588698863983
- 139.7478947043419
- 157.83507132530212
- 150.26932561397552
- 135.9647331237793
- 154.69248819351196
- 148.62813419103622
- 170.50678884983063
- 144.21240562200546
- 199.8608918786049
- 168.10997658967972
- 139.17032498121262
- 156.9184198975563
- 142.1268104314804
- 144.10703271627426
- 154.37267571687698
- 140.59710228443146
- 139.69274228811264
- 136.2328495979309
- 148.037683904171
- 148.90422916412354
- 154.25705367326736
- 164.7304309606552
- 137.00526422262192
- 165.34242475032806
- 149.5468304157257
- 141.3222900032997
- 139.57731068134308
- 155.3636918067932
- 175.59642273187637
- 152.07135158777237
- 146.6255355477333
- 158.44243574142456
- 136.04723191261292
- 133.78232657909393
- 157.60098427534103
- 152.08924067020416
- 158.12630581855774
- 148.16364312171936
- 156.90145468711853
- 148.78022772073746
- 123.4231806397438
- 155.95030337572098
- 157.8254222869873
- 133.6842678785324
- 137.0948435664177
- 139.7755628824234
- 166.08796632289886
- 142.92586892843246
- 154.37832844257355
- 162.22413390874863
- 126.66962856054306
- 146.05673331022263
train_accuracy:
- 0.0
- 0.144
- 0.16
- 0.0
- 0.39
- 0.24
- 0.573
- 0.675
- 0.775
- 0.817
- 0.773
- 0.95
- 0.577
- 0.438
- 0.457
- 0.495
- 0.947
- 0.0
- 0.567
- 0.522
- 0.629
- 0.444
- 0.544
- 0.639
- 0.466
- 0.416
- 0.9
- 0.809
- 0.975
- 0.975
- 0.888
- 0.533
- 0.531
- 0.261
- 0.915
- 0.453
- 0.494
- 0.692
- 0.489
- 0.518
- 0.775
- 0.656
- 0.567
- 0.415
- 0.581
- 0.668
- 0.544
- 0.271
- 0.841
- 0.93
- 0.68
- 0.679
- 0.95
- 0.538
- 0.929
- 0.62
- 0.736
- 0.928
- 0.406
- 0.958
- 0.91
- 0.395
- 0.967
- 0.865
- 0.613
- 0.806
- 0.443
- 0.462
- 0.983
- 0.456
- 0.503
- 0.967
- 0.581
- 0.878
- 0.786
- 0.891
- 0.8
- 0.542
- 0.8
- 0.204
- 0.958
- 0.783
- 0.767
- 0.608
- 0.341
- 0.938
- 0.755
- 0.576
- 0.9
- 0.653
- 0.777
- 0.655
- 0.917
- 0.864
- 0.691
- 0.665
- 0.513
- 0.663
- 0.05
- 0.675
train_loss:
- 1.05
- 0.876
- 0.682
- 0.514
- 0.649
- 0.567
- 0.516
- 0.49
- 0.516
- 0.409
- 0.444
- 0.442
- 0.422
- 0.355
- 0.439
- 0.406
- 0.437
- 0.387
- 0.343
- 0.422
- 0.426
- 0.37
- 0.379
- 0.434
- 0.383
- 0.39
- 0.366
- 0.356
- 0.322
- 0.301
- 0.378
- 0.364
- 0.372
- 0.326
- 0.344
- 0.36
- 0.334
- 0.347
- 0.308
- 0.346
- 0.332
- 0.423
- 0.388
- 0.32
- 0.388
- 0.297
- 0.342
- 0.422
- 0.356
- 0.396
- 0.316
- 0.369
- 0.307
- 0.311
- 0.332
- 0.295
- 0.289
- 0.337
- 0.339
- 0.371
- 0.321
- 0.296
- 0.285
- 0.326
- 0.354
- 0.367
- 0.383
- 0.4
- 0.295
- 0.387
- 0.303
- 0.344
- 0.336
- 0.311
- 0.355
- 0.322
- 0.346
- 0.345
- 0.292
- 0.339
- 0.329
- 0.277
- 0.36
- 0.319
- 0.343
- 0.379
- 0.321
- 0.311
- 0.353
- 0.361
- 0.298
- 0.311
- 0.311
- 0.275
- 0.331
- 0.348
- 0.341
- 0.327
- 0.307
- 0.334
unequal: 1
verbose: 1
