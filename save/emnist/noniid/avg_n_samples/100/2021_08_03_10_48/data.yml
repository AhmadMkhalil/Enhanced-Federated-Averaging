avg_train_accuracy: 0.663
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1575531914893617
- 0.2252127659574468
- 0.3227659574468085
- 0.36792553191489363
- 0.33026595744680853
- 0.39420212765957446
- 0.46595744680851064
- 0.4722340425531915
- 0.4919148936170213
- 0.45728723404255317
- 0.5493085106382979
- 0.5052127659574468
- 0.647127659574468
- 0.5686170212765957
- 0.4862234042553191
- 0.5618085106382978
- 0.5745744680851064
- 0.5846808510638298
- 0.6470212765957447
- 0.6149468085106383
- 0.5800531914893617
- 0.600531914893617
- 0.6357978723404255
- 0.5876063829787234
- 0.6337234042553191
- 0.6277127659574468
- 0.5431382978723405
- 0.5476595744680851
- 0.619095744680851
- 0.6067021276595744
- 0.6377659574468085
- 0.5907978723404256
- 0.6580851063829787
- 0.5921808510638298
- 0.6375531914893617
- 0.5781382978723404
- 0.5747872340425532
- 0.5945744680851064
- 0.620531914893617
- 0.6286702127659575
- 0.6584042553191489
- 0.6125
- 0.6614361702127659
- 0.6372872340425532
- 0.6871276595744681
- 0.6792553191489362
- 0.6189361702127659
- 0.6754255319148936
- 0.6781382978723405
- 0.6140425531914894
- 0.6585106382978724
- 0.6393085106382979
- 0.6395744680851064
- 0.6659574468085107
- 0.6326063829787234
- 0.612659574468085
- 0.6384042553191489
- 0.6226595744680851
- 0.6434574468085107
- 0.6417021276595745
- 0.6408510638297872
- 0.6429787234042553
- 0.635904255319149
- 0.6843085106382979
- 0.6831914893617022
- 0.6520744680851064
- 0.6879787234042554
- 0.671063829787234
- 0.6207978723404255
- 0.6663297872340426
- 0.6434042553191489
- 0.6459574468085106
- 0.5926063829787234
- 0.6860106382978723
- 0.664468085106383
- 0.6057446808510638
- 0.6752127659574468
- 0.6520744680851064
- 0.6708510638297872
- 0.6726595744680851
- 0.6214893617021277
- 0.6376595744680851
- 0.6967021276595745
- 0.6856914893617021
- 0.6819148936170213
- 0.6968617021276595
- 0.6419680851063829
- 0.670372340425532
- 0.6620212765957447
- 0.7027659574468085
- 0.6776063829787234
- 0.6604255319148936
- 0.6648936170212766
- 0.6215425531914893
- 0.6628723404255319
- 0.6598936170212766
- 0.6342553191489362
- 0.6389893617021276
- 0.6583510638297873
- 0.6587234042553192
test_loss_list:
- 537.0192339420319
- 445.83737564086914
- 387.4507327079773
- 354.8921730518341
- 349.08258271217346
- 320.5439488887787
- 276.50388538837433
- 258.70948457717896
- 241.43123519420624
- 252.14767730236053
- 201.9131679534912
- 227.8159283399582
- 171.20545369386673
- 198.20919489860535
- 216.89903926849365
- 189.07762730121613
- 185.4220106601715
- 184.54260921478271
- 153.35346746444702
- 168.40411919355392
- 184.18579679727554
- 179.6582493185997
- 159.36077499389648
- 172.79937100410461
- 169.63383030891418
- 163.76958829164505
- 192.9157897233963
- 198.45959270000458
- 173.2439449429512
- 183.75063025951385
- 172.49476611614227
- 163.2573446035385
- 150.69550144672394
- 183.30694526433945
- 158.8259288072586
- 180.79671746492386
- 182.4910175204277
- 178.0291702747345
- 169.90254300832748
- 164.09757268428802
- 147.4157549738884
- 176.14432126283646
- 145.52974212169647
- 180.64883035421371
- 131.98529869318008
- 141.60089361667633
- 168.1829496026039
- 133.24246686697006
- 132.44073647260666
- 157.69596618413925
- 139.91768711805344
- 147.52757996320724
- 149.81710332632065
- 147.3546306490898
- 140.47579264640808
- 154.4986732006073
- 159.3394426703453
- 151.29101872444153
- 143.92594003677368
- 141.584243953228
- 146.54251915216446
- 141.22899335622787
- 148.1359880566597
- 139.38788652420044
- 131.15953981876373
- 141.2563834786415
- 136.662417948246
- 129.76656955480576
- 152.01541441679
- 133.39271849393845
- 143.02317720651627
- 139.39466732740402
- 175.86247438192368
- 130.99400788545609
- 139.48749899864197
- 170.21211034059525
- 138.51015371084213
- 140.46935296058655
- 140.783829331398
- 149.2058938741684
- 170.12488585710526
- 143.6470890045166
- 125.09895598888397
- 133.71462434530258
- 131.65105617046356
- 129.6516593694687
- 142.8953133225441
- 139.3199268579483
- 144.58134096860886
- 133.85809367895126
- 129.08051389455795
- 140.2239158153534
- 143.0046364068985
- 168.3561528325081
- 151.28337687253952
- 146.58651667833328
- 152.30466997623444
- 143.2901886701584
- 136.02451074123383
- 136.38996493816376
train_accuracy:
- 0.28
- 0.1
- 0.255
- 0.225
- 0.775
- 0.193
- 0.832
- 0.525
- 0.58
- 0.1
- 0.25
- 0.035
- 0.682
- 0.842
- 0.217
- 0.843
- 0.341
- 0.439
- 0.7
- 0.895
- 0.85
- 0.879
- 0.655
- 0.567
- 0.647
- 0.7
- 0.633
- 0.625
- 0.757
- 0.761
- 0.55
- 0.525
- 0.659
- 0.0
- 0.0
- 0.76
- 0.492
- 0.354
- 0.843
- 0.867
- 0.825
- 0.967
- 0.836
- 0.413
- 0.788
- 0.541
- 0.558
- 0.583
- 0.739
- 0.632
- 0.125
- 0.683
- 0.133
- 0.687
- 0.654
- 0.4
- 0.546
- 0.557
- 0.885
- 0.742
- 0.93
- 0.57
- 0.935
- 0.827
- 0.99
- 0.471
- 0.811
- 0.817
- 0.97
- 0.676
- 0.236
- 0.706
- 0.365
- 0.929
- 0.875
- 0.118
- 0.808
- 0.958
- 0.821
- 0.607
- 0.95
- 0.76
- 0.721
- 0.653
- 0.987
- 0.742
- 0.91
- 0.762
- 0.39
- 0.67
- 0.711
- 0.675
- 0.257
- 0.337
- 0.585
- 0.883
- 0.517
- 0.291
- 0.854
- 0.663
train_loss:
- 1.248
- 0.866
- 0.522
- 0.611
- 0.499
- 0.409
- 0.54
- 0.49
- 0.532
- 0.411
- 0.425
- 0.469
- 0.509
- 0.508
- 0.444
- 0.476
- 0.437
- 0.441
- 0.44
- 0.397
- 0.307
- 0.402
- 0.382
- 0.366
- 0.408
- 0.389
- 0.427
- 0.332
- 0.424
- 0.405
- 0.31
- 0.396
- 0.355
- 0.37
- 0.328
- 0.377
- 0.354
- 0.384
- 0.361
- 0.377
- 0.343
- 0.273
- 0.405
- 0.338
- 0.353
- 0.34
- 0.357
- 0.314
- 0.392
- 0.441
- 0.349
- 0.335
- 0.335
- 0.34
- 0.366
- 0.36
- 0.336
- 0.364
- 0.329
- 0.333
- 0.275
- 0.264
- 0.315
- 0.341
- 0.328
- 0.305
- 0.344
- 0.294
- 0.31
- 0.315
- 0.392
- 0.33
- 0.327
- 0.317
- 0.357
- 0.372
- 0.316
- 0.292
- 0.387
- 0.337
- 0.26
- 0.305
- 0.281
- 0.338
- 0.328
- 0.39
- 0.332
- 0.391
- 0.323
- 0.303
- 0.332
- 0.309
- 0.323
- 0.351
- 0.33
- 0.301
- 0.335
- 0.363
- 0.304
- 0.307
unequal: 1
verbose: 1
