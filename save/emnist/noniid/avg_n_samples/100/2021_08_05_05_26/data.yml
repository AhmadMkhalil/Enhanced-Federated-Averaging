avg_train_accuracy: 0.8
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1529255319148936
- 0.2374468085106383
- 0.3402659574468085
- 0.40058510638297873
- 0.3886702127659574
- 0.4401595744680851
- 0.4120744680851064
- 0.4795744680851064
- 0.4718085106382979
- 0.4870744680851064
- 0.4675531914893617
- 0.4408510638297872
- 0.49569148936170215
- 0.48696808510638295
- 0.5895212765957447
- 0.5597872340425532
- 0.5236702127659575
- 0.5938829787234042
- 0.5909042553191489
- 0.5502659574468085
- 0.5922340425531915
- 0.540904255319149
- 0.5663829787234043
- 0.6067021276595744
- 0.5723404255319149
- 0.6334574468085107
- 0.6439893617021276
- 0.6023936170212766
- 0.5946808510638298
- 0.5971808510638298
- 0.6071276595744681
- 0.5956382978723405
- 0.5386170212765957
- 0.6534042553191489
- 0.6501595744680851
- 0.6267553191489361
- 0.6227659574468085
- 0.6127659574468085
- 0.6547340425531915
- 0.6451063829787234
- 0.5957978723404256
- 0.6478191489361702
- 0.6486702127659575
- 0.6788829787234043
- 0.5566489361702127
- 0.6597872340425532
- 0.6731382978723405
- 0.6320212765957447
- 0.6375531914893617
- 0.602872340425532
- 0.6384042553191489
- 0.6011170212765957
- 0.5922340425531915
- 0.6009042553191489
- 0.6478723404255319
- 0.6698404255319149
- 0.6581382978723405
- 0.637127659574468
- 0.6570744680851064
- 0.6776063829787234
- 0.706063829787234
- 0.6721276595744681
- 0.6507978723404255
- 0.635
- 0.6597340425531915
- 0.6617553191489361
- 0.6329787234042553
- 0.6446808510638298
- 0.6129255319148936
- 0.6502127659574468
- 0.6707446808510639
- 0.6578191489361702
- 0.6840425531914893
- 0.6782446808510638
- 0.6431382978723404
- 0.6554787234042553
- 0.6577659574468085
- 0.6222340425531915
- 0.6567021276595745
- 0.6596808510638298
- 0.6903191489361702
- 0.6372872340425532
- 0.6804787234042553
- 0.6213829787234042
- 0.643563829787234
- 0.676063829787234
- 0.7083510638297872
- 0.6394148936170213
- 0.6754787234042553
- 0.6797872340425531
- 0.6923936170212766
- 0.7041489361702128
- 0.6688829787234043
- 0.7088829787234042
- 0.6546276595744681
- 0.7170744680851063
- 0.6813297872340426
- 0.6557978723404255
- 0.6090425531914894
- 0.6167021276595744
test_loss_list:
- 526.5483436584473
- 454.8589324951172
- 372.3055431842804
- 335.5378954410553
- 319.4809750318527
- 306.82469296455383
- 315.0834001302719
- 286.57259929180145
- 275.6563308238983
- 245.42844009399414
- 259.44126439094543
- 252.6718269586563
- 227.72574019432068
- 233.87863624095917
- 193.05920749902725
- 200.66032898426056
- 213.30318069458008
- 179.11992514133453
- 178.8008873462677
- 183.0426430106163
- 174.83786582946777
- 192.77757728099823
- 196.5906662940979
- 165.62583816051483
- 178.16264700889587
- 161.6565368771553
- 155.99228471517563
- 180.31723457574844
- 172.98946928977966
- 182.01321905851364
- 174.60911184549332
- 165.9200222492218
- 194.6449966430664
- 147.10443234443665
- 148.94728273153305
- 166.404198884964
- 171.25476133823395
- 153.5173854827881
- 143.88280987739563
- 150.21005350351334
- 163.83915430307388
- 139.70684248209
- 145.60943126678467
- 135.07735389471054
- 193.47453832626343
- 143.64617544412613
- 130.38785761594772
- 152.002670109272
- 155.9654803276062
- 182.93835318088531
- 150.16593050956726
- 163.6261715888977
- 191.8096536397934
- 165.67023807764053
- 148.47759747505188
- 144.500213265419
- 146.3162499666214
- 178.0745514035225
- 153.86359637975693
- 144.02968227863312
- 138.85988342761993
- 142.7502703666687
- 144.38403296470642
- 148.3732808828354
- 147.55416214466095
- 142.2186854481697
- 163.77604550123215
- 156.501414000988
- 164.4317997097969
- 152.66934406757355
- 134.77500146627426
- 151.35174870491028
- 142.80465245246887
- 127.17886120080948
- 151.0436264872551
- 156.42623990774155
- 150.33175802230835
- 180.881167948246
- 146.17101937532425
- 161.82720255851746
- 129.9082419872284
- 138.15288245677948
- 132.4462814927101
- 158.03129637241364
- 139.77208936214447
- 139.33034682273865
- 123.01331639289856
- 142.94650220870972
- 130.11795037984848
- 126.26004731655121
- 133.9138000011444
- 128.36861145496368
- 144.07881355285645
- 121.28370326757431
- 150.26015025377274
- 122.53840506076813
- 143.9329068660736
- 143.67491388320923
- 176.7873871922493
- 170.433926820755
train_accuracy:
- 0.15
- 0.0
- 0.238
- 0.457
- 0.017
- 0.331
- 0.433
- 0.0
- 0.628
- 0.653
- 0.391
- 0.98
- 0.39
- 0.382
- 0.059
- 0.575
- 0.95
- 0.75
- 0.382
- 0.611
- 0.667
- 0.969
- 0.807
- 0.664
- 0.193
- 0.358
- 0.67
- 0.8
- 0.7
- 0.579
- 0.092
- 0.821
- 0.44
- 0.714
- 0.896
- 0.525
- 0.786
- 0.604
- 0.843
- 0.474
- 0.912
- 0.755
- 0.718
- 0.41
- 0.525
- 0.841
- 0.621
- 0.9
- 0.731
- 0.32
- 0.796
- 0.838
- 0.271
- 0.703
- 0.821
- 0.663
- 0.612
- 0.457
- 0.515
- 0.188
- 0.685
- 0.688
- 0.917
- 0.733
- 0.885
- 0.682
- 0.607
- 0.764
- 0.821
- 0.954
- 0.73
- 0.92
- 0.44
- 0.871
- 0.825
- 0.84
- 0.369
- 0.5
- 0.517
- 0.703
- 0.569
- 0.765
- 0.925
- 0.59
- 0.904
- 0.835
- 0.689
- 0.571
- 0.478
- 0.568
- 0.711
- 0.735
- 0.718
- 0.85
- 0.94
- 0.697
- 0.777
- 0.904
- 0.513
- 0.8
train_loss:
- 1.33
- 0.785
- 0.727
- 0.534
- 0.571
- 0.535
- 0.543
- 0.442
- 0.446
- 0.518
- 0.46
- 0.443
- 0.407
- 0.399
- 0.452
- 0.457
- 0.35
- 0.442
- 0.387
- 0.447
- 0.454
- 0.343
- 0.395
- 0.369
- 0.389
- 0.328
- 0.391
- 0.436
- 0.449
- 0.348
- 0.335
- 0.39
- 0.356
- 0.331
- 0.32
- 0.338
- 0.373
- 0.425
- 0.382
- 0.351
- 0.285
- 0.364
- 0.323
- 0.341
- 0.332
- 0.414
- 0.34
- 0.268
- 0.326
- 0.334
- 0.319
- 0.386
- 0.353
- 0.362
- 0.386
- 0.349
- 0.388
- 0.304
- 0.333
- 0.355
- 0.363
- 0.344
- 0.365
- 0.257
- 0.371
- 0.333
- 0.283
- 0.342
- 0.322
- 0.366
- 0.371
- 0.299
- 0.32
- 0.284
- 0.299
- 0.279
- 0.347
- 0.297
- 0.391
- 0.322
- 0.35
- 0.312
- 0.343
- 0.343
- 0.411
- 0.365
- 0.321
- 0.371
- 0.313
- 0.285
- 0.338
- 0.346
- 0.346
- 0.375
- 0.385
- 0.28
- 0.35
- 0.332
- 0.362
- 0.282
unequal: 1
verbose: 1
