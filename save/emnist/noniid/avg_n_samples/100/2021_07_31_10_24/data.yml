avg_train_accuracy: 0.838
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.18728723404255318
- 0.2653723404255319
- 0.3547872340425532
- 0.4277659574468085
- 0.5038297872340426
- 0.4247872340425532
- 0.43069148936170215
- 0.45351063829787236
- 0.4748404255319149
- 0.5042021276595745
- 0.5132446808510638
- 0.520531914893617
- 0.4854787234042553
- 0.5229787234042553
- 0.595531914893617
- 0.6389361702127659
- 0.5811702127659575
- 0.5907446808510638
- 0.5505851063829788
- 0.6029787234042553
- 0.5873404255319149
- 0.5857446808510638
- 0.5719148936170213
- 0.5287234042553192
- 0.5938829787234042
- 0.5779787234042553
- 0.6246276595744681
- 0.6003191489361702
- 0.5976595744680852
- 0.6229787234042553
- 0.6382978723404256
- 0.5912234042553192
- 0.614095744680851
- 0.5962765957446808
- 0.6197340425531915
- 0.5952127659574468
- 0.5903723404255319
- 0.6358510638297873
- 0.5861702127659575
- 0.5877127659574468
- 0.6156382978723405
- 0.611968085106383
- 0.6159574468085106
- 0.6197872340425532
- 0.6531914893617021
- 0.6840957446808511
- 0.6619148936170213
- 0.6303723404255319
- 0.6736702127659574
- 0.6713829787234042
- 0.6448404255319149
- 0.6362765957446809
- 0.6210106382978723
- 0.6326595744680851
- 0.6186702127659575
- 0.6153723404255319
- 0.6184042553191489
- 0.6210106382978723
- 0.6160106382978724
- 0.6263829787234042
- 0.6427127659574469
- 0.6917553191489362
- 0.6631914893617021
- 0.6352659574468085
- 0.6550531914893617
- 0.6723404255319149
- 0.6401595744680851
- 0.6484042553191489
- 0.619468085106383
- 0.5880319148936171
- 0.6398936170212766
- 0.6653191489361702
- 0.6367021276595745
- 0.6370744680851064
- 0.622872340425532
- 0.6283510638297872
- 0.620904255319149
- 0.6485106382978724
- 0.6260638297872341
- 0.6901595744680851
- 0.6325
- 0.66
- 0.6177659574468085
- 0.5841489361702128
- 0.6190425531914894
- 0.6706914893617021
- 0.6225
- 0.6242021276595745
- 0.7118085106382979
- 0.6093617021276596
- 0.6313297872340425
- 0.7060106382978724
- 0.680531914893617
- 0.6236170212765958
- 0.6253191489361702
- 0.6896808510638298
- 0.656063829787234
- 0.6706382978723404
- 0.665372340425532
- 0.6343085106382979
test_loss_list:
- 522.0291285514832
- 437.5405547618866
- 375.23253774642944
- 324.8084123134613
- 289.2162221670151
- 276.0761933326721
- 277.2490339279175
- 269.82931113243103
- 241.71871304512024
- 238.26821959018707
- 231.2835762500763
- 229.72130405902863
- 242.86912786960602
- 197.784193277359
- 177.61016470193863
- 165.28830540180206
- 188.67750561237335
- 188.41016125679016
- 198.95004320144653
- 183.75026094913483
- 176.72290343046188
- 186.5859124660492
- 185.11489951610565
- 202.49756288528442
- 171.2799795269966
- 174.17682319879532
- 161.97736710309982
- 190.2078395485878
- 181.0000279545784
- 172.31432712078094
- 154.72053331136703
- 164.11916756629944
- 158.53108549118042
- 180.68912941217422
- 154.67720979452133
- 181.00757777690887
- 174.50634783506393
- 146.51972597837448
- 165.30317836999893
- 161.62734246253967
- 176.58295905590057
- 167.64257568120956
- 153.5469994544983
- 157.10439383983612
- 145.3461611866951
- 133.55493688583374
- 156.67269319295883
- 147.14122462272644
- 146.0454785823822
- 143.27793246507645
- 145.8461240530014
- 157.25247889757156
- 159.77324068546295
- 153.03022491931915
- 162.49954515695572
- 164.53450852632523
- 162.09282004833221
- 160.85716527700424
- 168.76610946655273
- 164.2310175895691
- 166.31746900081635
- 144.82018274068832
- 147.75243455171585
- 153.68739914894104
- 136.2216225862503
- 132.00928151607513
- 141.89588636159897
- 156.32939195632935
- 155.0280660390854
- 177.5006361603737
- 160.91772204637527
- 153.13023722171783
- 157.98494946956635
- 155.8836150765419
- 156.09718090295792
- 163.77690398693085
- 165.15319961309433
- 148.01349592208862
- 151.89022028446198
- 124.53751695156097
- 145.06546169519424
- 150.52778589725494
- 152.42955881357193
- 173.97651106119156
- 161.64400708675385
- 139.33402186632156
- 172.91995948553085
- 153.85282397270203
- 122.15794551372528
- 164.0298861861229
- 169.93579530715942
- 128.03989791870117
- 135.15743327140808
- 153.00075167417526
- 151.04296189546585
- 133.3565627336502
- 138.74308478832245
- 131.5819861292839
- 148.2951119542122
- 151.25212442874908
train_accuracy:
- 0.644
- 0.394
- 0.275
- 0.306
- 0.0
- 0.277
- 0.368
- 0.65
- 0.05
- 0.389
- 0.758
- 0.63
- 0.436
- 0.75
- 0.463
- 0.631
- 0.972
- 0.319
- 0.311
- 0.638
- 0.678
- 0.783
- 0.464
- 0.557
- 0.35
- 0.092
- 0.945
- 0.787
- 0.579
- 0.46
- 0.394
- 0.95
- 0.238
- 0.3
- 0.494
- 0.9
- 0.671
- 0.669
- 0.888
- 0.519
- 0.625
- 0.8
- 0.977
- 0.6
- 0.591
- 0.845
- 0.912
- 0.983
- 0.661
- 1.0
- 0.815
- 0.513
- 0.697
- 0.863
- 0.955
- 0.358
- 0.543
- 0.181
- 0.568
- 0.281
- 0.192
- 0.327
- 0.65
- 0.583
- 0.95
- 0.175
- 0.783
- 0.888
- 0.375
- 0.956
- 0.142
- 0.875
- 0.337
- 0.707
- 0.562
- 0.84
- 0.372
- 0.603
- 0.875
- 0.929
- 0.45
- 0.657
- 0.317
- 0.842
- 0.508
- 0.838
- 0.6
- 0.426
- 0.778
- 0.581
- 0.663
- 0.637
- 0.83
- 0.825
- 0.022
- 0.369
- 0.754
- 0.93
- 0.9
- 0.838
train_loss:
- 1.307
- 0.696
- 0.561
- 0.544
- 0.57
- 0.467
- 0.359
- 0.558
- 0.465
- 0.433
- 0.378
- 0.408
- 0.448
- 0.398
- 0.381
- 0.408
- 0.41
- 0.444
- 0.39
- 0.421
- 0.347
- 0.362
- 0.386
- 0.383
- 0.368
- 0.368
- 0.346
- 0.325
- 0.338
- 0.395
- 0.412
- 0.312
- 0.452
- 0.344
- 0.319
- 0.348
- 0.386
- 0.285
- 0.328
- 0.371
- 0.31
- 0.343
- 0.256
- 0.348
- 0.314
- 0.31
- 0.292
- 0.355
- 0.414
- 0.272
- 0.283
- 0.344
- 0.374
- 0.24
- 0.314
- 0.329
- 0.309
- 0.397
- 0.29
- 0.273
- 0.335
- 0.326
- 0.368
- 0.326
- 0.276
- 0.362
- 0.314
- 0.31
- 0.32
- 0.297
- 0.283
- 0.314
- 0.328
- 0.305
- 0.364
- 0.311
- 0.374
- 0.332
- 0.298
- 0.253
- 0.333
- 0.304
- 0.367
- 0.294
- 0.319
- 0.319
- 0.27
- 0.374
- 0.352
- 0.325
- 0.288
- 0.374
- 0.307
- 0.286
- 0.329
- 0.366
- 0.398
- 0.294
- 0.269
- 0.276
unequal: 1
verbose: 1
