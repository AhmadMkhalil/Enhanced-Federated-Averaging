avg_train_accuracy: 0.41
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1579787234042553
- 0.20297872340425532
- 0.28117021276595744
- 0.3551595744680851
- 0.420531914893617
- 0.45904255319148934
- 0.45053191489361705
- 0.4476063829787234
- 0.48606382978723406
- 0.4815957446808511
- 0.48090425531914893
- 0.530904255319149
- 0.5272872340425532
- 0.5822340425531914
- 0.6001063829787234
- 0.5732978723404255
- 0.6588297872340425
- 0.5511702127659575
- 0.5558510638297872
- 0.5878191489361703
- 0.5967021276595744
- 0.5332978723404256
- 0.607872340425532
- 0.5882978723404255
- 0.5464361702127659
- 0.555
- 0.6061170212765957
- 0.6336170212765957
- 0.6180851063829788
- 0.6057978723404255
- 0.6079255319148936
- 0.6536702127659575
- 0.6543085106382979
- 0.6445212765957447
- 0.6109042553191489
- 0.6246276595744681
- 0.6224468085106383
- 0.6339361702127659
- 0.6311170212765957
- 0.6182978723404255
- 0.6160638297872341
- 0.6115957446808511
- 0.663936170212766
- 0.6011702127659575
- 0.5364361702127659
- 0.606968085106383
- 0.5932446808510639
- 0.6323404255319149
- 0.6453723404255319
- 0.629095744680851
- 0.5971276595744681
- 0.6076063829787234
- 0.6372872340425532
- 0.6261170212765957
- 0.6592021276595744
- 0.6292553191489362
- 0.631968085106383
- 0.5929255319148936
- 0.6253723404255319
- 0.6843617021276596
- 0.6524468085106383
- 0.6320744680851064
- 0.6098936170212766
- 0.6023404255319149
- 0.6578723404255319
- 0.648936170212766
- 0.665
- 0.6490957446808511
- 0.6041489361702128
- 0.6239361702127659
- 0.6420212765957447
- 0.675159574468085
- 0.6353723404255319
- 0.6617553191489361
- 0.6433510638297872
- 0.6321808510638298
- 0.5996808510638297
- 0.6519680851063829
- 0.6288297872340426
- 0.6215957446808511
- 0.6849468085106383
- 0.6466489361702128
- 0.6506382978723404
- 0.6356382978723404
- 0.6263829787234042
- 0.6065957446808511
- 0.6156382978723405
- 0.6407978723404255
- 0.6590957446808511
- 0.6580851063829787
- 0.6454255319148936
- 0.6876595744680851
- 0.6399468085106383
- 0.693031914893617
- 0.6709574468085107
- 0.6505851063829787
- 0.6276595744680851
- 0.6342021276595745
- 0.6677659574468086
- 0.6667021276595745
test_loss_list:
- 524.612113237381
- 453.4973495006561
- 388.5481336116791
- 341.8835566043854
- 306.57176172733307
- 284.65437364578247
- 265.8571399450302
- 259.83331048488617
- 244.76103484630585
- 223.5713188648224
- 246.36991965770721
- 210.66349267959595
- 214.62933707237244
- 192.53530931472778
- 186.66385704278946
- 189.18008428812027
- 156.92685770988464
- 193.35937577486038
- 196.86308228969574
- 195.7492891550064
- 173.88990151882172
- 189.6484351158142
- 178.39952158927917
- 164.87859582901
- 200.42863035202026
- 199.22107887268066
- 181.63698011636734
- 173.66639560461044
- 171.8646012544632
- 160.65907633304596
- 170.87982350587845
- 151.0524256825447
- 174.6745709180832
- 142.14834833145142
- 163.239059984684
- 164.27145218849182
- 153.40745145082474
- 177.94857335090637
- 163.93157559633255
- 150.90236937999725
- 169.08549988269806
- 167.7396045923233
- 148.33124965429306
- 201.36934912204742
- 188.21168494224548
- 169.0219703912735
- 181.813536465168
- 146.9416338801384
- 150.89696794748306
- 157.64336347579956
- 179.45821648836136
- 159.55953139066696
- 151.5310240983963
- 144.34504741430283
- 137.93542337417603
- 162.9788693189621
- 165.32211536169052
- 184.8245177268982
- 163.5237898826599
- 141.54124575853348
- 162.15332239866257
- 157.1100074648857
- 164.91552037000656
- 158.49428248405457
- 142.39701759815216
- 156.70095229148865
- 142.87395918369293
- 157.0103565454483
- 168.78255248069763
- 165.34797185659409
- 144.110474050045
- 133.57320219278336
- 170.00901466608047
- 149.02347266674042
- 159.56405591964722
- 141.0828418135643
- 156.87771910429
- 143.75072652101517
- 157.42497301101685
- 163.68056464195251
- 135.53217566013336
- 160.35080605745316
- 154.19768166542053
- 172.02483236789703
- 156.4036493897438
- 190.63541388511658
- 149.47916519641876
- 140.1642850637436
- 141.2796322107315
- 153.1015049815178
- 144.61517125368118
- 137.13598841428757
- 166.80021733045578
- 119.66055101156235
- 132.97365510463715
- 158.59108835458755
- 166.2902416586876
- 174.74916940927505
- 135.1485157608986
- 135.47167325019836
train_accuracy:
- 0.0
- 0.304
- 0.54
- 0.468
- 0.369
- 0.104
- 0.145
- 0.007
- 0.05
- 0.483
- 0.725
- 0.36
- 0.757
- 0.458
- 0.492
- 0.655
- 0.7
- 0.527
- 0.786
- 0.575
- 0.533
- 0.5
- 0.344
- 0.356
- 0.705
- 0.825
- 0.482
- 0.643
- 0.642
- 0.382
- 0.836
- 0.3
- 0.37
- 0.485
- 0.814
- 0.477
- 0.636
- 0.913
- 0.639
- 0.797
- 0.59
- 0.929
- 0.55
- 0.35
- 0.7
- 0.8
- 0.822
- 0.893
- 0.246
- 0.894
- 0.336
- 0.343
- 0.264
- 0.414
- 0.794
- 0.839
- 0.933
- 0.728
- 0.527
- 0.514
- 0.783
- 0.976
- 0.487
- 0.608
- 0.686
- 0.59
- 0.95
- 0.892
- 0.533
- 0.485
- 0.646
- 0.317
- 0.8
- 0.516
- 0.938
- 0.505
- 0.615
- 0.911
- 0.95
- 0.885
- 0.629
- 0.578
- 0.442
- 0.386
- 0.241
- 0.843
- 0.54
- 0.638
- 0.385
- 0.832
- 0.744
- 0.695
- 0.54
- 0.589
- 0.438
- 0.971
- 0.817
- 0.581
- 0.933
- 0.41
train_loss:
- 1.077
- 0.745
- 0.651
- 0.601
- 0.615
- 0.588
- 0.527
- 0.481
- 0.428
- 0.429
- 0.431
- 0.377
- 0.512
- 0.475
- 0.398
- 0.385
- 0.392
- 0.333
- 0.357
- 0.386
- 0.377
- 0.339
- 0.412
- 0.367
- 0.339
- 0.34
- 0.296
- 0.399
- 0.359
- 0.412
- 0.386
- 0.384
- 0.348
- 0.405
- 0.301
- 0.432
- 0.389
- 0.349
- 0.328
- 0.309
- 0.384
- 0.376
- 0.371
- 0.284
- 0.338
- 0.308
- 0.312
- 0.347
- 0.361
- 0.323
- 0.289
- 0.412
- 0.41
- 0.364
- 0.335
- 0.315
- 0.371
- 0.336
- 0.362
- 0.316
- 0.279
- 0.328
- 0.351
- 0.356
- 0.384
- 0.347
- 0.351
- 0.321
- 0.324
- 0.364
- 0.311
- 0.335
- 0.334
- 0.395
- 0.303
- 0.316
- 0.353
- 0.298
- 0.352
- 0.296
- 0.3
- 0.378
- 0.307
- 0.341
- 0.313
- 0.307
- 0.331
- 0.332
- 0.301
- 0.347
- 0.351
- 0.345
- 0.28
- 0.312
- 0.29
- 0.317
- 0.319
- 0.309
- 0.372
- 0.306
unequal: 1
verbose: 1
