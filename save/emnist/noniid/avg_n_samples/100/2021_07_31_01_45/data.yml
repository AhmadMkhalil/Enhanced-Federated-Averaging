avg_train_accuracy: 0.719
avg_train_loss: 0.004
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.11425531914893618
- 0.2404255319148936
- 0.3197872340425532
- 0.43148936170212765
- 0.40845744680851065
- 0.4271276595744681
- 0.45867021276595743
- 0.5158510638297872
- 0.5534574468085106
- 0.5107978723404255
- 0.5279255319148937
- 0.5527659574468086
- 0.5414893617021277
- 0.5145212765957446
- 0.5193617021276595
- 0.5336170212765957
- 0.5142553191489362
- 0.5557446808510639
- 0.5632446808510638
- 0.6049468085106383
- 0.6167021276595744
- 0.6020744680851063
- 0.6061170212765957
- 0.6136702127659575
- 0.588404255319149
- 0.6498404255319149
- 0.553031914893617
- 0.6166489361702128
- 0.6526595744680851
- 0.6027127659574468
- 0.6369148936170212
- 0.6648936170212766
- 0.6748404255319149
- 0.6302127659574468
- 0.6297340425531915
- 0.6270744680851064
- 0.608563829787234
- 0.6787234042553192
- 0.6620744680851064
- 0.6462765957446809
- 0.6063829787234043
- 0.6293617021276596
- 0.629468085106383
- 0.6954787234042553
- 0.6038829787234042
- 0.676968085106383
- 0.6604787234042553
- 0.6490957446808511
- 0.6700531914893617
- 0.6062765957446808
- 0.5999468085106383
- 0.6160106382978724
- 0.6542553191489362
- 0.6264893617021277
- 0.6390425531914894
- 0.6536170212765957
- 0.6429255319148937
- 0.6391489361702127
- 0.6437234042553192
- 0.6770212765957446
- 0.6543617021276595
- 0.6573936170212766
- 0.6503723404255319
- 0.6250531914893617
- 0.6913829787234043
- 0.7020212765957446
- 0.6601595744680852
- 0.7153191489361702
- 0.708404255319149
- 0.6627659574468086
- 0.6449468085106383
- 0.6341489361702127
- 0.6806382978723404
- 0.6909574468085107
- 0.6571808510638298
- 0.6561170212765958
- 0.6425
- 0.6530851063829787
- 0.6356382978723404
- 0.6398404255319149
- 0.6914893617021277
- 0.6704255319148936
- 0.620531914893617
- 0.6433510638297872
- 0.6835106382978723
- 0.6322872340425532
- 0.6904255319148936
- 0.670372340425532
- 0.6875531914893617
- 0.651436170212766
- 0.6953191489361702
- 0.6698404255319149
- 0.6856914893617021
- 0.6651595744680852
- 0.7101063829787234
- 0.6854787234042553
- 0.7042553191489361
- 0.6722872340425532
- 0.6490425531914894
- 0.7360638297872341
test_loss_list:
- 534.0998084545135
- 434.2682206630707
- 374.56423926353455
- 306.32714688777924
- 302.9449499845505
- 288.13467395305634
- 284.7703130245209
- 249.55498147010803
- 216.58932876586914
- 213.34601593017578
- 214.37154269218445
- 207.07889640331268
- 205.84434163570404
- 224.06624710559845
- 221.95464050769806
- 210.9712781906128
- 219.29049682617188
- 189.87990820407867
- 186.79753202199936
- 174.7421414256096
- 164.53674882650375
- 170.8599596619606
- 170.5283161997795
- 167.56067204475403
- 168.86920249462128
- 148.28928142786026
- 189.32212090492249
- 162.05612206459045
- 142.7685604095459
- 163.78933155536652
- 161.39014238119125
- 153.22850263118744
- 148.39617031812668
- 156.62423717975616
- 147.38026350736618
- 160.6101706624031
- 170.3315127491951
- 144.1700857281685
- 153.2885826230049
- 154.47082835435867
- 154.93829041719437
- 145.86363518238068
- 151.15824615955353
- 129.2666972875595
- 170.07025676965714
- 143.54125559329987
- 144.1306272149086
- 147.8247777223587
- 145.98446714878082
- 152.73581564426422
- 170.4008902311325
- 154.24178194999695
- 149.21467196941376
- 147.06730502843857
- 154.01031082868576
- 139.64273166656494
- 148.40657526254654
- 145.86828190088272
- 148.0369114279747
- 128.1974087357521
- 149.48050397634506
- 146.5950476527214
- 145.73739689588547
- 151.6870436668396
- 128.5869687795639
- 132.587926030159
- 147.02890771627426
- 121.07069110870361
- 128.73161208629608
- 144.84285527467728
- 155.8581165075302
- 176.96916157007217
- 136.89420408010483
- 133.99719560146332
- 148.46235609054565
- 148.44873237609863
- 137.245585501194
- 148.8268871307373
- 151.8291437625885
- 160.42646914720535
- 128.5787888765335
- 134.53043657541275
- 159.34087854623795
- 156.17236632108688
- 132.8787466287613
- 157.12702131271362
- 131.9230653643608
- 145.59003591537476
- 144.3962962627411
- 138.55175763368607
- 125.10101556777954
- 134.3567050099373
- 142.0967516899109
- 137.42875170707703
- 129.00583654642105
- 133.34108632802963
- 122.45197123289108
- 135.7961181998253
- 143.262364923954
- 113.87945026159286
train_accuracy:
- 0.164
- 0.077
- 0.727
- 0.567
- 0.714
- 0.843
- 0.471
- 0.406
- 0.213
- 0.181
- 0.661
- 0.275
- 0.067
- 0.617
- 0.3
- 0.92
- 0.772
- 0.167
- 0.477
- 0.617
- 0.844
- 0.51
- 0.73
- 0.797
- 0.7
- 0.972
- 0.95
- 0.98
- 0.767
- 0.417
- 0.657
- 0.856
- 0.927
- 1.0
- 0.94
- 0.375
- 0.967
- 0.955
- 0.5
- 0.64
- 0.641
- 0.919
- 0.327
- 0.881
- 0.935
- 0.725
- 0.042
- 0.725
- 0.779
- 0.859
- 0.242
- 0.708
- 0.942
- 0.532
- 0.888
- 0.75
- 0.517
- 0.65
- 0.979
- 0.95
- 0.89
- 0.561
- 0.491
- 0.2
- 0.462
- 0.9
- 0.774
- 0.929
- 0.83
- 0.574
- 0.97
- 0.925
- 0.933
- 0.432
- 0.518
- 0.15
- 0.94
- 0.604
- 0.823
- 0.617
- 0.863
- 0.732
- 0.747
- 0.8
- 0.656
- 0.625
- 0.597
- 0.692
- 0.864
- 0.553
- 0.805
- 0.8
- 0.812
- 0.682
- 0.733
- 0.967
- 0.925
- 0.716
- 0.917
- 0.719
train_loss:
- 1.215
- 0.826
- 0.592
- 0.51
- 0.536
- 0.547
- 0.543
- 0.522
- 0.484
- 0.452
- 0.451
- 0.379
- 0.426
- 0.466
- 0.348
- 0.365
- 0.451
- 0.372
- 0.407
- 0.41
- 0.483
- 0.361
- 0.402
- 0.387
- 0.314
- 0.427
- 0.34
- 0.339
- 0.373
- 0.341
- 0.414
- 0.366
- 0.354
- 0.371
- 0.382
- 0.355
- 0.29
- 0.365
- 0.399
- 0.396
- 0.33
- 0.296
- 0.388
- 0.344
- 0.299
- 0.386
- 0.354
- 0.39
- 0.29
- 0.303
- 0.332
- 0.405
- 0.327
- 0.399
- 0.374
- 0.267
- 0.409
- 0.344
- 0.298
- 0.35
- 0.354
- 0.442
- 0.319
- 0.297
- 0.387
- 0.275
- 0.35
- 0.4
- 0.369
- 0.322
- 0.282
- 0.303
- 0.362
- 0.337
- 0.277
- 0.329
- 0.373
- 0.363
- 0.316
- 0.302
- 0.329
- 0.387
- 0.376
- 0.315
- 0.348
- 0.273
- 0.349
- 0.33
- 0.307
- 0.392
- 0.353
- 0.393
- 0.319
- 0.315
- 0.344
- 0.336
- 0.29
- 0.279
- 0.324
- 0.376
unequal: 1
verbose: 1
