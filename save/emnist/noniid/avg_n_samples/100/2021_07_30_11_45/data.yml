avg_train_accuracy: 0.45
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.12936170212765957
- 0.2399468085106383
- 0.346968085106383
- 0.41856382978723405
- 0.4001063829787234
- 0.4402127659574468
- 0.49186170212765956
- 0.4743085106382979
- 0.5020744680851064
- 0.5460106382978723
- 0.5831382978723404
- 0.559468085106383
- 0.5909574468085106
- 0.5725
- 0.5437765957446808
- 0.5615425531914894
- 0.620904255319149
- 0.5186702127659575
- 0.5358510638297872
- 0.5829255319148936
- 0.6569148936170213
- 0.5353191489361702
- 0.5821276595744681
- 0.6015957446808511
- 0.6068617021276596
- 0.6402127659574468
- 0.6683510638297873
- 0.6282978723404256
- 0.5535638297872341
- 0.6039361702127659
- 0.6197340425531915
- 0.6354255319148936
- 0.6237234042553191
- 0.65
- 0.6976063829787233
- 0.6434042553191489
- 0.6392021276595745
- 0.5845744680851064
- 0.6579787234042553
- 0.6876595744680851
- 0.6591489361702128
- 0.666968085106383
- 0.6537765957446808
- 0.6253191489361702
- 0.6526595744680851
- 0.6553191489361702
- 0.6959574468085107
- 0.6378191489361702
- 0.6180851063829788
- 0.6579787234042553
- 0.6770744680851064
- 0.656595744680851
- 0.6943617021276596
- 0.6593617021276595
- 0.6778191489361702
- 0.6814361702127659
- 0.648031914893617
- 0.605
- 0.633031914893617
- 0.696968085106383
- 0.6206382978723404
- 0.7052127659574469
- 0.6550531914893617
- 0.6573936170212766
- 0.6663829787234042
- 0.666968085106383
- 0.6427127659574469
- 0.6852659574468085
- 0.6798936170212766
- 0.6263297872340425
- 0.6852127659574468
- 0.6473936170212766
- 0.7223936170212766
- 0.6679787234042553
- 0.6749468085106383
- 0.6716489361702128
- 0.7059042553191489
- 0.6423936170212766
- 0.6822872340425532
- 0.6430851063829788
- 0.6834574468085106
- 0.6626063829787234
- 0.6644148936170213
- 0.6520212765957447
- 0.6284042553191489
- 0.6543617021276595
- 0.6526063829787234
- 0.6708510638297872
- 0.6749468085106383
- 0.681595744680851
- 0.7061170212765957
- 0.6713297872340426
- 0.6628723404255319
- 0.7182446808510639
- 0.6741489361702128
- 0.6947872340425532
- 0.6239893617021277
- 0.6793617021276596
- 0.6671276595744681
- 0.6520212765957447
test_loss_list:
- 534.6270654201508
- 449.38456106185913
- 360.7389409542084
- 306.893412232399
- 296.947012424469
- 291.7946536540985
- 256.20801305770874
- 245.77872395515442
- 236.8646891117096
- 208.76287126541138
- 188.60302072763443
- 202.80499374866486
- 184.57224464416504
- 200.92362141609192
- 196.27210366725922
- 200.48887383937836
- 186.18991005420685
- 199.89355885982513
- 199.10482144355774
- 181.855153799057
- 151.98734283447266
- 204.5943285226822
- 187.24205440282822
- 163.877306163311
- 170.72317916154861
- 152.34044152498245
- 141.93131536245346
- 151.51791870594025
- 187.3776822090149
- 164.96346777677536
- 164.0551813840866
- 153.2200666666031
- 162.8796021938324
- 154.9415071606636
- 136.57054018974304
- 158.40964007377625
- 155.98002582788467
- 171.6785894036293
- 144.71545791625977
- 130.37257331609726
- 147.54495358467102
- 136.32849270105362
- 140.63754856586456
- 160.16736751794815
- 148.92965227365494
- 145.3593049645424
- 133.99518364667892
- 159.22799521684647
- 167.67852568626404
- 157.9671425819397
- 129.49028211832047
- 153.11661440134048
- 138.99294435977936
- 144.13596284389496
- 145.9954867362976
- 135.20235919952393
- 146.43170350790024
- 176.87527430057526
- 143.91982889175415
- 134.05823010206223
- 145.6338227391243
- 131.50075429677963
- 146.1278970837593
- 140.60538816452026
- 140.35600531101227
- 142.439488530159
- 148.02399945259094
- 133.75275647640228
- 137.72389221191406
- 156.13177907466888
- 133.0592364668846
- 149.3222485780716
- 117.62823957204819
- 136.8372219800949
- 141.7487907409668
- 138.34914845228195
- 124.31899589300156
- 138.66033732891083
- 134.3168163895607
- 153.9365861415863
- 128.21670192480087
- 152.7196220755577
- 143.37148189544678
- 143.62763637304306
- 153.22235119342804
- 154.95473682880402
- 143.05916821956635
- 148.17203968763351
- 141.113339304924
- 130.7605654001236
- 123.29560631513596
- 140.86147785186768
- 137.54520976543427
- 118.73331362009048
- 133.10831898450851
- 129.6142606139183
- 151.31803423166275
- 133.61952793598175
- 141.8526970744133
- 141.192815721035
train_accuracy:
- 0.0
- 0.128
- 0.79
- 0.509
- 0.892
- 0.767
- 0.685
- 0.446
- 0.1
- 0.371
- 0.556
- 0.155
- 0.45
- 0.273
- 0.578
- 0.812
- 0.287
- 0.745
- 0.675
- 0.058
- 0.779
- 0.142
- 0.75
- 0.372
- 0.625
- 0.558
- 0.897
- 0.98
- 0.574
- 0.883
- 0.56
- 0.506
- 0.667
- 0.9
- 0.665
- 0.675
- 0.73
- 0.675
- 0.834
- 0.611
- 0.907
- 0.881
- 0.683
- 0.933
- 0.834
- 0.871
- 0.574
- 0.975
- 0.983
- 0.555
- 0.765
- 0.373
- 0.696
- 0.133
- 0.192
- 0.94
- 0.828
- 0.88
- 0.917
- 0.95
- 0.875
- 0.5
- 0.774
- 0.742
- 0.595
- 0.838
- 0.058
- 0.8
- 0.969
- 0.3
- 0.581
- 0.626
- 0.831
- 0.9
- 0.509
- 0.821
- 0.958
- 0.95
- 0.25
- 0.538
- 0.888
- 0.627
- 0.874
- 0.45
- 0.9
- 0.65
- 0.65
- 0.704
- 0.778
- 0.888
- 0.85
- 0.513
- 0.705
- 0.908
- 0.526
- 0.882
- 0.742
- 0.7
- 0.035
- 0.45
train_loss:
- 1.263
- 0.697
- 0.698
- 0.679
- 0.485
- 0.492
- 0.544
- 0.49
- 0.482
- 0.439
- 0.417
- 0.559
- 0.438
- 0.468
- 0.439
- 0.417
- 0.321
- 0.484
- 0.489
- 0.431
- 0.449
- 0.393
- 0.393
- 0.435
- 0.426
- 0.457
- 0.349
- 0.381
- 0.352
- 0.378
- 0.335
- 0.459
- 0.337
- 0.393
- 0.398
- 0.326
- 0.402
- 0.346
- 0.393
- 0.387
- 0.317
- 0.395
- 0.328
- 0.323
- 0.398
- 0.321
- 0.34
- 0.324
- 0.358
- 0.313
- 0.374
- 0.291
- 0.367
- 0.341
- 0.422
- 0.288
- 0.337
- 0.298
- 0.358
- 0.33
- 0.303
- 0.329
- 0.328
- 0.343
- 0.371
- 0.3
- 0.371
- 0.403
- 0.326
- 0.341
- 0.357
- 0.422
- 0.354
- 0.277
- 0.326
- 0.369
- 0.343
- 0.355
- 0.345
- 0.345
- 0.313
- 0.346
- 0.355
- 0.365
- 0.372
- 0.332
- 0.376
- 0.269
- 0.346
- 0.4
- 0.364
- 0.335
- 0.312
- 0.3
- 0.351
- 0.372
- 0.29
- 0.338
- 0.364
- 0.32
unequal: 1
verbose: 1
