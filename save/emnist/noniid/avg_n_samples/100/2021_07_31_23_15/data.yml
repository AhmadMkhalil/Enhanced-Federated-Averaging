avg_train_accuracy: 0.897
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.17585106382978724
- 0.27132978723404255
- 0.24675531914893617
- 0.4219148936170213
- 0.42686170212765956
- 0.4617553191489362
- 0.4373404255319149
- 0.4643617021276596
- 0.511968085106383
- 0.5143085106382979
- 0.5741489361702128
- 0.6021276595744681
- 0.5707978723404256
- 0.5721808510638298
- 0.5782978723404255
- 0.5642021276595744
- 0.5280851063829787
- 0.6117021276595744
- 0.599095744680851
- 0.5917553191489362
- 0.6139893617021277
- 0.6779787234042554
- 0.6150531914893617
- 0.5670212765957446
- 0.5988297872340426
- 0.608404255319149
- 0.6106914893617021
- 0.6570212765957447
- 0.6020744680851063
- 0.6150531914893617
- 0.6886702127659574
- 0.6440425531914894
- 0.5932446808510639
- 0.5629787234042554
- 0.6011170212765957
- 0.6647340425531915
- 0.6165957446808511
- 0.5904787234042553
- 0.6590957446808511
- 0.6558510638297872
- 0.6628723404255319
- 0.6384574468085107
- 0.6514893617021277
- 0.6852127659574468
- 0.5396808510638298
- 0.7103191489361702
- 0.6605851063829787
- 0.6590957446808511
- 0.6177127659574468
- 0.5467021276595745
- 0.6639893617021276
- 0.6443617021276595
- 0.62
- 0.6923404255319149
- 0.7000531914893617
- 0.6384574468085107
- 0.6688297872340425
- 0.6521808510638298
- 0.6701063829787234
- 0.6656382978723404
- 0.6209574468085106
- 0.6503191489361703
- 0.6548936170212766
- 0.6746808510638298
- 0.6853191489361702
- 0.653563829787234
- 0.6877127659574468
- 0.6921276595744681
- 0.6577659574468085
- 0.6577659574468085
- 0.6603191489361702
- 0.6592021276595744
- 0.6843617021276596
- 0.6951063829787234
- 0.6356914893617022
- 0.665372340425532
- 0.6533510638297872
- 0.668031914893617
- 0.6406914893617022
- 0.6234042553191489
- 0.6381914893617021
- 0.6683510638297873
- 0.7197872340425532
- 0.6827127659574468
- 0.6954787234042553
- 0.6552659574468085
- 0.6759042553191489
- 0.7196808510638298
- 0.6840957446808511
- 0.6909042553191489
- 0.6776063829787234
- 0.6797340425531915
- 0.6113829787234043
- 0.6683510638297873
- 0.6886170212765957
- 0.6528191489361702
- 0.6637765957446808
- 0.6992553191489361
- 0.6999468085106383
- 0.6959042553191489
test_loss_list:
- 525.9613029956818
- 437.04431676864624
- 395.9300158023834
- 334.3957757949829
- 277.4805428981781
- 275.722607254982
- 288.2670307159424
- 262.89268386363983
- 232.110564827919
- 240.0695402622223
- 183.80116480588913
- 176.8155658841133
- 184.7959983944893
- 182.09023612737656
- 195.32061058282852
- 193.51800554990768
- 202.3113329410553
- 167.5483990907669
- 177.77573412656784
- 172.6395138502121
- 168.38656741380692
- 148.09170824289322
- 158.39326441287994
- 176.36784374713898
- 170.41952520608902
- 171.6855770945549
- 166.7445428967476
- 147.08725786209106
- 182.11438584327698
- 162.57203650474548
- 138.41749304533005
- 144.74298179149628
- 169.62527734041214
- 186.19810539484024
- 172.48323142528534
- 142.71287125349045
- 153.27952533960342
- 172.8092076778412
- 155.71894204616547
- 153.30828428268433
- 152.48035329580307
- 166.07415384054184
- 154.44173872470856
- 135.7579070329666
- 215.55159330368042
- 137.53890770673752
- 149.8067106604576
- 137.6085034608841
- 152.63875597715378
- 197.09381341934204
- 138.42569649219513
- 149.13044655323029
- 167.15662622451782
- 133.1801872253418
- 131.2050705552101
- 138.35591173171997
- 141.46540236473083
- 138.2643040418625
- 145.65202516317368
- 140.5243899822235
- 168.8801342844963
- 157.6132406592369
- 145.14800423383713
- 137.5336799621582
- 137.8208109140396
- 146.75191569328308
- 144.24963986873627
- 135.05291950702667
- 134.3142718076706
- 136.83317917585373
- 149.7990119457245
- 137.3500980734825
- 128.26023375988007
- 124.62479585409164
- 154.20786237716675
- 143.88123267889023
- 142.0725213289261
- 142.5183431506157
- 142.34217190742493
- 151.88995325565338
- 153.3759937286377
- 133.32162189483643
- 119.0625130534172
- 130.04829543828964
- 125.31311815977097
- 157.58573925495148
- 142.89395743608475
- 120.4068011045456
- 146.81325387954712
- 140.0906829237938
- 138.22929269075394
- 140.2144563794136
- 183.8081437945366
- 137.79282587766647
- 125.49015194177628
- 149.2099466919899
- 140.4068511724472
- 121.67069041728973
- 124.83390480279922
- 136.53142827749252
train_accuracy:
- 0.268
- 0.0
- 0.65
- 0.221
- 0.035
- 0.97
- 0.017
- 0.237
- 0.49
- 0.285
- 0.665
- 0.953
- 0.392
- 0.592
- 0.558
- 0.64
- 0.878
- 0.892
- 0.854
- 0.007
- 0.608
- 0.925
- 0.336
- 0.257
- 0.482
- 0.694
- 0.155
- 0.833
- 0.61
- 0.562
- 0.543
- 0.104
- 0.925
- 0.847
- 0.715
- 0.931
- 0.671
- 0.732
- 0.562
- 0.825
- 0.684
- 0.925
- 0.7
- 0.832
- 0.327
- 0.846
- 0.462
- 0.753
- 0.612
- 0.587
- 0.522
- 0.581
- 0.911
- 0.922
- 0.675
- 0.55
- 0.588
- 0.8
- 0.568
- 0.675
- 0.862
- 0.612
- 0.49
- 0.744
- 0.167
- 0.864
- 0.727
- 0.492
- 0.304
- 0.45
- 0.88
- 0.487
- 0.85
- 0.6
- 0.675
- 0.229
- 0.883
- 0.679
- 0.4
- 0.921
- 0.625
- 0.912
- 0.917
- 0.725
- 0.912
- 0.44
- 0.0
- 0.564
- 0.867
- 0.917
- 0.8
- 0.4
- 0.717
- 0.456
- 0.641
- 0.65
- 0.518
- 0.761
- 0.914
- 0.897
train_loss:
- 1.236
- 0.816
- 0.681
- 0.505
- 0.59
- 0.415
- 0.417
- 0.484
- 0.487
- 0.498
- 0.428
- 0.493
- 0.433
- 0.402
- 0.417
- 0.313
- 0.475
- 0.386
- 0.437
- 0.4
- 0.381
- 0.402
- 0.363
- 0.38
- 0.375
- 0.343
- 0.374
- 0.34
- 0.432
- 0.317
- 0.374
- 0.358
- 0.404
- 0.382
- 0.367
- 0.377
- 0.36
- 0.409
- 0.368
- 0.336
- 0.354
- 0.346
- 0.38
- 0.399
- 0.332
- 0.429
- 0.381
- 0.336
- 0.416
- 0.369
- 0.401
- 0.314
- 0.334
- 0.352
- 0.396
- 0.354
- 0.351
- 0.287
- 0.364
- 0.317
- 0.339
- 0.341
- 0.299
- 0.381
- 0.307
- 0.384
- 0.272
- 0.351
- 0.34
- 0.389
- 0.331
- 0.365
- 0.288
- 0.356
- 0.351
- 0.272
- 0.332
- 0.325
- 0.281
- 0.338
- 0.331
- 0.336
- 0.327
- 0.316
- 0.347
- 0.321
- 0.338
- 0.403
- 0.349
- 0.344
- 0.378
- 0.328
- 0.25
- 0.284
- 0.355
- 0.327
- 0.34
- 0.363
- 0.352
- 0.325
unequal: 1
verbose: 1
