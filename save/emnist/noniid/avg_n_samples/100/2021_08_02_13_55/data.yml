avg_train_accuracy: 0.335
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.14191489361702128
- 0.2377127659574468
- 0.33904255319148935
- 0.4689893617021277
- 0.36436170212765956
- 0.4226063829787234
- 0.4875531914893617
- 0.47829787234042553
- 0.42946808510638296
- 0.4698404255319149
- 0.5088829787234043
- 0.4625
- 0.5925
- 0.5260638297872341
- 0.581968085106383
- 0.538563829787234
- 0.41212765957446806
- 0.5661702127659575
- 0.5587765957446809
- 0.5322872340425532
- 0.5628723404255319
- 0.5469148936170213
- 0.5968617021276595
- 0.5947872340425532
- 0.5939893617021277
- 0.6128191489361702
- 0.637127659574468
- 0.6282978723404256
- 0.6242553191489362
- 0.5779255319148936
- 0.5655851063829788
- 0.6261702127659574
- 0.6310638297872341
- 0.5879787234042553
- 0.6620744680851064
- 0.5940425531914894
- 0.606063829787234
- 0.6164893617021276
- 0.6657446808510639
- 0.5986170212765958
- 0.6038829787234042
- 0.5926595744680851
- 0.5932446808510639
- 0.6048936170212766
- 0.6342553191489362
- 0.685
- 0.6106914893617021
- 0.6723404255319149
- 0.6505851063829787
- 0.6198404255319149
- 0.647127659574468
- 0.6831914893617022
- 0.6986170212765958
- 0.6081382978723404
- 0.6317553191489361
- 0.6678723404255319
- 0.666595744680851
- 0.6540957446808511
- 0.613563829787234
- 0.608404255319149
- 0.6692553191489362
- 0.6438297872340426
- 0.6421808510638298
- 0.6513829787234042
- 0.6055851063829787
- 0.64
- 0.6256382978723404
- 0.6436702127659575
- 0.6307446808510638
- 0.6652127659574468
- 0.6836702127659574
- 0.6295212765957446
- 0.6657446808510639
- 0.6501063829787234
- 0.6237765957446808
- 0.6697340425531915
- 0.6773404255319149
- 0.6402127659574468
- 0.6605319148936171
- 0.6626595744680851
- 0.635531914893617
- 0.6758510638297872
- 0.6722340425531915
- 0.6602659574468085
- 0.6931382978723404
- 0.6746808510638298
- 0.6654787234042553
- 0.6376595744680851
- 0.6823404255319149
- 0.6426595744680851
- 0.6735106382978724
- 0.5912765957446808
- 0.6734574468085106
- 0.6739893617021276
- 0.6695744680851063
- 0.7106382978723405
- 0.6339893617021276
- 0.6525531914893618
- 0.6684042553191489
- 0.6428191489361702
test_loss_list:
- 532.6381046772003
- 432.1891915798187
- 364.28954434394836
- 287.7901906967163
- 301.63304364681244
- 270.9304687976837
- 242.87097942829132
- 250.53969657421112
- 253.14718902111053
- 249.7485053539276
- 216.1574901342392
- 233.18123614788055
- 201.00666522979736
- 224.23468220233917
- 192.34102815389633
- 208.62879371643066
- 279.6691725254059
- 182.05854833126068
- 196.5226857662201
- 202.29240787029266
- 176.9511974453926
- 196.4035758972168
- 168.6238860487938
- 184.580901324749
- 169.5798763036728
- 154.200361430645
- 148.32584393024445
- 163.76742589473724
- 170.49837601184845
- 188.2180758714676
- 184.87231862545013
- 172.68178236484528
- 161.4936889410019
- 173.33695524930954
- 146.85092425346375
- 179.09865593910217
- 179.62462109327316
- 169.0978137254715
- 144.7705585360527
- 183.3752116560936
- 166.27014988660812
- 161.66492944955826
- 168.2758134007454
- 161.70237863063812
- 158.26133847236633
- 138.35802137851715
- 169.06845605373383
- 134.2784759402275
- 158.1936873793602
- 161.33438235521317
- 150.33319532871246
- 132.37174659967422
- 137.87180680036545
- 172.12578630447388
- 152.20372653007507
- 133.8293769955635
- 143.24874490499496
- 146.06151914596558
- 152.58341455459595
- 171.47029823064804
- 140.06402760744095
- 147.40089803934097
- 144.14027070999146
- 150.90779995918274
- 171.0564780831337
- 147.96938407421112
- 155.35644018650055
- 152.2366418838501
- 154.95141184329987
- 139.5810043811798
- 133.11215579509735
- 150.1121529340744
- 142.53617411851883
- 149.23013353347778
- 156.1319898366928
- 137.81108385324478
- 133.60642224550247
- 152.53958696126938
- 141.59458154439926
- 143.93154680728912
- 155.69908434152603
- 133.35653406381607
- 137.31735682487488
- 136.83716320991516
- 125.91817677021027
- 124.29833698272705
- 139.68364256620407
- 149.18143504858017
- 131.9767256975174
- 146.99890780448914
- 141.27751207351685
- 172.61451476812363
- 135.0650645494461
- 132.2169440984726
- 148.87026488780975
- 122.84577476978302
- 147.9474744796753
- 145.9810763001442
- 140.56700855493546
- 147.14015305042267
train_accuracy:
- 0.187
- 0.397
- 0.496
- 0.458
- 0.244
- 0.88
- 0.292
- 0.05
- 0.312
- 0.382
- 0.758
- 0.214
- 0.608
- 0.975
- 0.888
- 0.0
- 0.45
- 0.517
- 0.521
- 0.963
- 0.665
- 0.683
- 0.8
- 0.881
- 0.712
- 0.603
- 0.865
- 0.715
- 0.821
- 0.45
- 0.0
- 0.633
- 0.758
- 0.329
- 0.733
- 0.888
- 0.756
- 0.661
- 0.824
- 0.57
- 0.35
- 0.529
- 0.582
- 0.938
- 0.85
- 0.927
- 0.888
- 0.913
- 0.337
- 0.496
- 0.618
- 0.137
- 0.457
- 0.961
- 0.371
- 0.942
- 0.392
- 0.782
- 0.442
- 0.283
- 0.787
- 0.925
- 0.515
- 0.75
- 0.596
- 0.5
- 0.925
- 0.667
- 0.429
- 0.938
- 0.495
- 0.8
- 0.67
- 0.0
- 0.0
- 0.897
- 0.435
- 0.756
- 0.765
- 0.0
- 0.137
- 0.658
- 0.579
- 0.395
- 0.806
- 0.563
- 0.647
- 0.689
- 0.795
- 0.25
- 0.288
- 0.667
- 0.812
- 0.869
- 0.704
- 0.113
- 0.579
- 0.817
- 0.521
- 0.335
train_loss:
- 1.212
- 0.838
- 0.618
- 0.543
- 0.577
- 0.6
- 0.498
- 0.423
- 0.477
- 0.462
- 0.455
- 0.524
- 0.429
- 0.35
- 0.432
- 0.459
- 0.348
- 0.371
- 0.42
- 0.431
- 0.385
- 0.432
- 0.401
- 0.417
- 0.299
- 0.426
- 0.407
- 0.366
- 0.383
- 0.341
- 0.4
- 0.312
- 0.341
- 0.376
- 0.293
- 0.354
- 0.443
- 0.423
- 0.334
- 0.35
- 0.384
- 0.349
- 0.308
- 0.359
- 0.377
- 0.297
- 0.313
- 0.315
- 0.364
- 0.358
- 0.351
- 0.385
- 0.326
- 0.286
- 0.304
- 0.391
- 0.302
- 0.348
- 0.36
- 0.317
- 0.226
- 0.392
- 0.321
- 0.378
- 0.313
- 0.325
- 0.287
- 0.313
- 0.345
- 0.314
- 0.335
- 0.353
- 0.318
- 0.381
- 0.33
- 0.268
- 0.346
- 0.424
- 0.35
- 0.267
- 0.25
- 0.286
- 0.296
- 0.333
- 0.343
- 0.418
- 0.355
- 0.36
- 0.339
- 0.305
- 0.304
- 0.374
- 0.283
- 0.353
- 0.341
- 0.279
- 0.237
- 0.344
- 0.329
- 0.286
unequal: 1
verbose: 1
