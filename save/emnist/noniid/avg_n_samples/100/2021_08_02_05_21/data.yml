avg_train_accuracy: 0.262
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1599468085106383
- 0.2246276595744681
- 0.33824468085106385
- 0.388563829787234
- 0.45361702127659576
- 0.4475
- 0.5032446808510638
- 0.5056914893617022
- 0.5227127659574468
- 0.5267021276595745
- 0.4290425531914894
- 0.48861702127659573
- 0.47606382978723405
- 0.5270212765957447
- 0.5183510638297872
- 0.5562234042553191
- 0.5637765957446809
- 0.5556382978723404
- 0.6202659574468085
- 0.6447340425531914
- 0.5954255319148937
- 0.6168085106382979
- 0.6174468085106383
- 0.5915425531914894
- 0.641436170212766
- 0.555372340425532
- 0.6161170212765957
- 0.6182978723404255
- 0.5740425531914893
- 0.6285106382978723
- 0.6700531914893617
- 0.6910106382978723
- 0.5980851063829787
- 0.6191489361702127
- 0.6395744680851064
- 0.573031914893617
- 0.644468085106383
- 0.6156382978723405
- 0.6293085106382978
- 0.5916489361702127
- 0.5806382978723404
- 0.5773936170212766
- 0.6592553191489362
- 0.6892021276595744
- 0.6209574468085106
- 0.6329255319148936
- 0.6020212765957447
- 0.6433510638297872
- 0.6786170212765957
- 0.660372340425532
- 0.6476063829787234
- 0.6921808510638298
- 0.6288829787234043
- 0.6713297872340426
- 0.6496276595744681
- 0.6461702127659574
- 0.6290425531914894
- 0.675372340425532
- 0.6526063829787234
- 0.6638829787234043
- 0.6278723404255319
- 0.6327659574468085
- 0.6437234042553192
- 0.6554255319148936
- 0.6480851063829787
- 0.6590957446808511
- 0.6179787234042553
- 0.597872340425532
- 0.6019148936170213
- 0.659468085106383
- 0.6318085106382979
- 0.631968085106383
- 0.5706914893617021
- 0.5738297872340425
- 0.6763297872340426
- 0.6722340425531915
- 0.6561170212765958
- 0.6606914893617021
- 0.6436702127659575
- 0.6751063829787234
- 0.6311170212765957
- 0.6001063829787234
- 0.5716489361702127
- 0.6823404255319149
- 0.6707446808510639
- 0.714095744680851
- 0.688404255319149
- 0.6836702127659574
- 0.6770744680851064
- 0.6161702127659574
- 0.6976595744680851
- 0.6877127659574468
- 0.6732446808510638
- 0.6514893617021277
- 0.68
- 0.6417021276595745
- 0.6525531914893618
- 0.6630851063829787
- 0.6660106382978723
- 0.6433510638297872
test_loss_list:
- 528.281197309494
- 432.7208549976349
- 354.49861907958984
- 318.4857895374298
- 268.3316842317581
- 271.1114341020584
- 252.90738368034363
- 234.2460390329361
- 235.9930763244629
- 213.2695870399475
- 269.33326053619385
- 238.774719953537
- 260.6118347644806
- 219.26839923858643
- 220.943723320961
- 193.360105574131
- 192.3520188331604
- 190.55530309677124
- 167.09246945381165
- 164.80691635608673
- 176.9052666425705
- 172.93735867738724
- 155.511454641819
- 178.68872249126434
- 157.78611052036285
- 230.62654864788055
- 178.7471560239792
- 165.28463810682297
- 182.425643324852
- 154.68887001276016
- 145.38336688280106
- 137.99036139249802
- 166.1033108830452
- 153.06562787294388
- 154.4347095489502
- 176.45318961143494
- 152.3665800690651
- 159.03577905893326
- 168.66006976366043
- 177.22364765405655
- 178.74018573760986
- 176.4732584953308
- 151.29732638597488
- 140.77866119146347
- 164.06606954336166
- 161.05997222661972
- 154.48115354776382
- 142.07566142082214
- 145.90848916769028
- 148.9860424399376
- 158.55048072338104
- 136.5356084704399
- 157.6832549571991
- 135.5851093530655
- 147.65804785490036
- 145.42850840091705
- 141.3945313692093
- 138.9057719707489
- 143.8578124642372
- 146.03581535816193
- 145.81465464830399
- 145.30791699886322
- 145.64280325174332
- 147.88128054141998
- 155.391231238842
- 142.72630268335342
- 164.54337710142136
- 200.92241197824478
- 169.89041393995285
- 134.84939938783646
- 144.61210238933563
- 158.37197095155716
- 173.21413099765778
- 164.49172180891037
- 135.05308729410172
- 139.731429874897
- 144.57187682390213
- 142.9124104976654
- 138.81202191114426
- 133.29521584510803
- 165.74455589056015
- 157.53737193346024
- 193.0457445383072
- 139.43295460939407
- 140.54094696044922
- 130.32928866147995
- 136.78689968585968
- 144.97352772951126
- 137.51159620285034
- 166.91191452741623
- 134.80609792470932
- 121.85411483049393
- 135.65353977680206
- 137.7710247039795
- 125.63969248533249
- 159.98913234472275
- 146.77787190675735
- 147.90626859664917
- 134.34614086151123
- 151.353065431118
train_accuracy:
- 0.018
- 0.003
- 0.012
- 0.722
- 0.0
- 0.422
- 0.26
- 0.57
- 0.833
- 0.169
- 0.697
- 0.545
- 0.612
- 0.112
- 0.221
- 0.033
- 0.611
- 0.638
- 0.5
- 0.569
- 0.315
- 0.957
- 0.477
- 0.438
- 0.755
- 0.688
- 0.417
- 0.883
- 0.246
- 0.7
- 0.792
- 0.917
- 0.8
- 0.567
- 0.23
- 0.9
- 0.889
- 0.871
- 0.293
- 0.312
- 0.95
- 0.617
- 0.238
- 0.797
- 0.58
- 0.142
- 0.896
- 0.45
- 0.712
- 0.586
- 0.215
- 0.563
- 0.465
- 0.94
- 0.883
- 0.877
- 0.821
- 0.547
- 0.265
- 0.95
- 0.356
- 0.567
- 0.0
- 0.903
- 0.306
- 0.783
- 1.0
- 0.221
- 1.0
- 0.283
- 0.696
- 0.676
- 0.833
- 0.622
- 0.759
- 0.585
- 0.495
- 0.786
- 0.665
- 0.75
- 0.981
- 0.3
- 0.723
- 0.73
- 0.55
- 0.791
- 0.845
- 0.527
- 0.665
- 0.686
- 0.917
- 0.938
- 0.377
- 0.344
- 0.906
- 0.431
- 0.5
- 0.744
- 0.6
- 0.262
train_loss:
- 1.192
- 0.823
- 0.696
- 0.583
- 0.597
- 0.448
- 0.544
- 0.441
- 0.5
- 0.427
- 0.44
- 0.406
- 0.402
- 0.406
- 0.417
- 0.376
- 0.396
- 0.422
- 0.421
- 0.417
- 0.368
- 0.434
- 0.319
- 0.382
- 0.353
- 0.393
- 0.386
- 0.415
- 0.405
- 0.325
- 0.339
- 0.314
- 0.311
- 0.264
- 0.364
- 0.332
- 0.35
- 0.344
- 0.371
- 0.324
- 0.356
- 0.342
- 0.338
- 0.348
- 0.207
- 0.363
- 0.33
- 0.36
- 0.336
- 0.299
- 0.336
- 0.271
- 0.311
- 0.332
- 0.336
- 0.331
- 0.426
- 0.256
- 0.348
- 0.324
- 0.369
- 0.384
- 0.366
- 0.357
- 0.363
- 0.318
- 0.364
- 0.32
- 0.324
- 0.371
- 0.349
- 0.362
- 0.295
- 0.35
- 0.355
- 0.324
- 0.359
- 0.378
- 0.403
- 0.279
- 0.294
- 0.341
- 0.33
- 0.301
- 0.33
- 0.271
- 0.286
- 0.265
- 0.29
- 0.302
- 0.275
- 0.305
- 0.336
- 0.318
- 0.319
- 0.249
- 0.333
- 0.289
- 0.387
- 0.305
unequal: 1
verbose: 1
