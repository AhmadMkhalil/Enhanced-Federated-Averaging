avg_train_accuracy: 0.682
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.15015957446808512
- 0.25909574468085106
- 0.30840425531914895
- 0.3381914893617021
- 0.3699468085106383
- 0.3389893617021277
- 0.49611702127659574
- 0.4675
- 0.49367021276595746
- 0.4792021276595745
- 0.4773404255319149
- 0.48569148936170214
- 0.4745744680851064
- 0.4804255319148936
- 0.5524468085106383
- 0.5818617021276595
- 0.5260638297872341
- 0.5456382978723404
- 0.6261702127659574
- 0.5903723404255319
- 0.6628723404255319
- 0.593563829787234
- 0.5695212765957447
- 0.576063829787234
- 0.5275
- 0.6112234042553192
- 0.5739893617021277
- 0.6229787234042553
- 0.6477127659574468
- 0.5830851063829787
- 0.6532978723404256
- 0.5687234042553192
- 0.619468085106383
- 0.6351595744680851
- 0.600531914893617
- 0.6143085106382978
- 0.6238829787234043
- 0.6336702127659575
- 0.5728191489361703
- 0.6390425531914894
- 0.5963829787234043
- 0.6125
- 0.6231914893617021
- 0.6397340425531914
- 0.5789893617021277
- 0.5975531914893617
- 0.6667021276595745
- 0.6649468085106383
- 0.6281914893617021
- 0.6095744680851064
- 0.6100531914893617
- 0.6459574468085106
- 0.6274468085106383
- 0.6171808510638298
- 0.6442553191489362
- 0.6131914893617021
- 0.655904255319149
- 0.6802659574468085
- 0.6723936170212766
- 0.6346276595744681
- 0.6576595744680851
- 0.6404787234042553
- 0.641436170212766
- 0.6028191489361702
- 0.6287234042553191
- 0.5886170212765958
- 0.6617021276595745
- 0.6139361702127659
- 0.6247872340425532
- 0.645904255319149
- 0.6506382978723404
- 0.686063829787234
- 0.6773936170212767
- 0.6757978723404255
- 0.6286170212765958
- 0.6061702127659574
- 0.6438297872340426
- 0.6676595744680851
- 0.7148404255319148
- 0.6631914893617021
- 0.67
- 0.6862234042553191
- 0.6575531914893618
- 0.6673404255319149
- 0.6405851063829787
- 0.6507978723404255
- 0.6460106382978723
- 0.703031914893617
- 0.6508510638297872
- 0.6537234042553192
- 0.6656382978723404
- 0.6081914893617021
- 0.670372340425532
- 0.6365425531914893
- 0.6382446808510638
- 0.6400531914893617
- 0.6982978723404255
- 0.698031914893617
- 0.6863297872340426
- 0.6373936170212766
test_loss_list:
- 524.1104545593262
- 428.6768252849579
- 386.05441546440125
- 346.923508644104
- 319.3528689146042
- 336.5200732946396
- 258.15924966335297
- 257.22901153564453
- 237.19847464561462
- 232.95505952835083
- 256.97326135635376
- 234.97470033168793
- 229.60678732395172
- 232.94803202152252
- 192.74469232559204
- 187.57051241397858
- 213.88041496276855
- 197.53941941261292
- 161.27871918678284
- 183.02566212415695
- 150.03752958774567
- 173.72391480207443
- 186.1131809949875
- 188.2939503788948
- 209.28364157676697
- 163.17584991455078
- 182.31341075897217
- 169.21529322862625
- 156.94721657037735
- 175.65212857723236
- 152.61283481121063
- 187.2484393119812
- 158.370046377182
- 163.19646835327148
- 169.9999548792839
- 159.11715131998062
- 167.49168246984482
- 168.27877181768417
- 174.7916870713234
- 170.0408399105072
- 172.710608959198
- 175.90325158834457
- 162.15343672037125
- 142.8578714132309
- 175.7749752998352
- 168.27369356155396
- 134.04897010326385
- 146.32158958911896
- 169.88632452487946
- 168.13755595684052
- 153.00629729032516
- 153.22554522752762
- 148.8010356426239
- 178.7717113494873
- 153.4927470088005
- 165.2971030473709
- 145.79927378892899
- 138.30040925741196
- 138.36067521572113
- 160.2161151766777
- 141.50081074237823
- 139.3726146221161
- 149.9808863401413
- 169.29260402917862
- 148.36301684379578
- 170.78579884767532
- 150.54552805423737
- 161.55470198392868
- 161.48165047168732
- 178.4382026195526
- 137.54102218151093
- 140.56991058588028
- 136.8329340815544
- 139.09857869148254
- 148.6434070467949
- 169.67482268810272
- 145.86423099040985
- 135.2583258152008
- 122.51912820339203
- 136.3657402396202
- 136.63401347398758
- 141.8566071987152
- 151.82348161935806
- 148.8528926372528
- 147.3570009469986
- 147.98506718873978
- 144.28795319795609
- 124.72029864788055
- 142.13606387376785
- 140.43421334028244
- 138.83562642335892
- 162.19069987535477
- 138.2093661427498
- 154.57134521007538
- 154.56650972366333
- 150.2444707751274
- 124.78334218263626
- 124.1848760843277
- 136.9714166522026
- 141.2800800204277
train_accuracy:
- 0.227
- 0.172
- 0.389
- 0.012
- 0.542
- 0.158
- 0.869
- 0.95
- 0.486
- 0.44
- 0.167
- 0.688
- 0.106
- 0.375
- 0.612
- 0.547
- 0.369
- 0.817
- 0.657
- 0.792
- 0.392
- 0.688
- 0.729
- 0.629
- 0.511
- 0.319
- 0.458
- 0.675
- 0.638
- 0.831
- 0.292
- 0.542
- 0.415
- 0.688
- 0.864
- 0.045
- 0.8
- 0.333
- 0.71
- 0.475
- 0.893
- 0.39
- 0.505
- 0.573
- 0.2
- 0.666
- 0.85
- 0.518
- 0.425
- 0.677
- 0.554
- 0.675
- 0.864
- 0.888
- 0.6
- 0.319
- 0.438
- 0.783
- 0.883
- 0.79
- 0.933
- 0.769
- 0.692
- 0.958
- 0.87
- 0.683
- 0.742
- 0.425
- 0.519
- 0.554
- 0.538
- 0.764
- 0.709
- 0.706
- 0.082
- 0.062
- 0.919
- 0.842
- 0.815
- 0.614
- 0.69
- 0.0
- 0.9
- 0.875
- 0.509
- 0.439
- 0.7
- 0.912
- 0.828
- 0.167
- 0.367
- 0.616
- 0.529
- 0.587
- 0.632
- 0.668
- 0.628
- 0.917
- 0.889
- 0.682
train_loss:
- 1.224
- 0.841
- 0.579
- 0.497
- 0.538
- 0.537
- 0.468
- 0.463
- 0.433
- 0.383
- 0.358
- 0.363
- 0.412
- 0.492
- 0.472
- 0.413
- 0.467
- 0.432
- 0.437
- 0.391
- 0.4
- 0.284
- 0.376
- 0.349
- 0.429
- 0.321
- 0.459
- 0.339
- 0.387
- 0.346
- 0.412
- 0.321
- 0.342
- 0.345
- 0.357
- 0.342
- 0.293
- 0.319
- 0.335
- 0.337
- 0.358
- 0.327
- 0.348
- 0.336
- 0.314
- 0.317
- 0.343
- 0.394
- 0.321
- 0.347
- 0.401
- 0.295
- 0.311
- 0.347
- 0.377
- 0.308
- 0.329
- 0.345
- 0.256
- 0.351
- 0.286
- 0.398
- 0.379
- 0.277
- 0.341
- 0.317
- 0.321
- 0.297
- 0.265
- 0.399
- 0.271
- 0.279
- 0.28
- 0.38
- 0.298
- 0.283
- 0.299
- 0.382
- 0.332
- 0.3
- 0.344
- 0.274
- 0.243
- 0.365
- 0.338
- 0.251
- 0.408
- 0.287
- 0.316
- 0.305
- 0.336
- 0.272
- 0.354
- 0.326
- 0.337
- 0.273
- 0.308
- 0.336
- 0.278
- 0.302
unequal: 1
verbose: 1
