avg_train_accuracy: 0.84
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.15398936170212765
- 0.26324468085106384
- 0.28840425531914893
- 0.40117021276595743
- 0.4381914893617021
- 0.3887765957446809
- 0.5247340425531914
- 0.4732446808510638
- 0.44648936170212766
- 0.5287234042553192
- 0.5590425531914893
- 0.5571808510638298
- 0.4875
- 0.5334042553191489
- 0.5657978723404256
- 0.575
- 0.6525531914893618
- 0.5563829787234043
- 0.5776595744680851
- 0.5314893617021277
- 0.5685106382978723
- 0.6120744680851063
- 0.6030319148936171
- 0.6168617021276596
- 0.6068085106382979
- 0.5917553191489362
- 0.6776595744680851
- 0.5701595744680851
- 0.6208510638297873
- 0.6365425531914893
- 0.6043617021276596
- 0.6317553191489361
- 0.6695212765957447
- 0.6620744680851064
- 0.6558510638297872
- 0.6418617021276596
- 0.579627659574468
- 0.6206914893617022
- 0.6019148936170213
- 0.6478723404255319
- 0.5587765957446809
- 0.6276063829787234
- 0.6667021276595745
- 0.6101063829787234
- 0.631436170212766
- 0.6258510638297873
- 0.6003191489361702
- 0.6751063829787234
- 0.6790425531914893
- 0.6518617021276596
- 0.7184574468085106
- 0.6718085106382978
- 0.6288829787234043
- 0.6723404255319149
- 0.6336170212765957
- 0.6648936170212766
- 0.6587765957446808
- 0.5577659574468085
- 0.6297872340425532
- 0.6469680851063829
- 0.6672340425531915
- 0.6747340425531915
- 0.6312234042553192
- 0.6593085106382979
- 0.5970744680851063
- 0.6262765957446809
- 0.6228191489361702
- 0.7014361702127659
- 0.6723936170212766
- 0.7129787234042553
- 0.683936170212766
- 0.7023936170212766
- 0.6790957446808511
- 0.6342553191489362
- 0.6163297872340425
- 0.6573404255319149
- 0.6466489361702128
- 0.6497340425531914
- 0.6224468085106383
- 0.6382446808510638
- 0.6420212765957447
- 0.6735638297872341
- 0.7132446808510639
- 0.6625
- 0.6469148936170213
- 0.6447340425531914
- 0.6497872340425532
- 0.7254255319148936
- 0.7057446808510638
- 0.6712765957446809
- 0.7165425531914894
- 0.7113829787234043
- 0.6970212765957446
- 0.6491489361702127
- 0.6427659574468085
- 0.6727127659574468
- 0.6786702127659574
- 0.7054255319148937
- 0.7002127659574469
- 0.6402127659574468
test_loss_list:
- 532.9941263198853
- 435.8897223472595
- 393.7979438304901
- 321.28090012073517
- 301.83650267124176
- 307.3457124233246
- 234.51069474220276
- 242.99945163726807
- 256.3400821685791
- 220.92586994171143
- 202.72870337963104
- 196.07298612594604
- 226.59141862392426
- 204.01007974147797
- 195.79765284061432
- 185.4072551727295
- 159.451021194458
- 203.4407547712326
- 187.82903438806534
- 198.70583879947662
- 207.4629544019699
- 183.81497412919998
- 171.22612273693085
- 166.51798856258392
- 168.03168958425522
- 172.61262953281403
- 149.388966858387
- 177.50298297405243
- 160.23620849847794
- 159.19884365797043
- 166.60309451818466
- 164.56115645170212
- 150.01613461971283
- 148.9733825325966
- 154.03870391845703
- 152.36264622211456
- 168.11531138420105
- 155.6619210243225
- 162.86650162935257
- 152.25875121355057
- 199.97526621818542
- 157.69871777296066
- 139.05928164720535
- 157.93089354038239
- 161.25289225578308
- 162.69285935163498
- 172.3931529521942
- 137.52725857496262
- 141.82168394327164
- 139.20352244377136
- 126.24859136343002
- 142.58568519353867
- 150.06936460733414
- 146.88243067264557
- 168.49868059158325
- 148.4036613702774
- 147.34417110681534
- 208.3857306241989
- 158.36101001501083
- 151.58797806501389
- 140.7827707529068
- 134.47092652320862
- 154.6782311797142
- 143.5633916258812
- 167.85134637355804
- 148.7521241903305
- 154.97275286912918
- 121.00565975904465
- 143.77232539653778
- 127.10241174697876
- 129.7203775048256
- 134.48811280727386
- 151.02455592155457
- 151.1728659272194
- 169.5325403213501
- 146.7562655210495
- 142.9370880126953
- 146.49239778518677
- 149.8039067387581
- 153.07669579982758
- 145.104871571064
- 129.23156040906906
- 121.10124671459198
- 135.84961491823196
- 149.35868901014328
- 147.3431232571602
- 150.448544383049
- 114.52706098556519
- 123.07191061973572
- 136.3900865316391
- 125.41799902915955
- 130.66264075040817
- 126.48979127407074
- 152.183600127697
- 148.63413268327713
- 139.0479217171669
- 129.08256739377975
- 120.94198966026306
- 126.69852459430695
- 156.78755050897598
train_accuracy:
- 0.205
- 0.586
- 0.117
- 0.57
- 0.243
- 0.396
- 0.332
- 0.036
- 0.128
- 0.487
- 0.491
- 0.796
- 0.74
- 0.522
- 0.886
- 0.858
- 0.64
- 0.35
- 0.614
- 0.66
- 0.653
- 0.393
- 0.711
- 0.5
- 0.283
- 0.681
- 0.729
- 0.525
- 0.439
- 0.875
- 0.733
- 0.487
- 0.531
- 0.954
- 0.618
- 0.31
- 0.854
- 0.757
- 0.7
- 0.728
- 0.592
- 0.603
- 0.571
- 0.2
- 0.531
- 0.722
- 0.723
- 0.13
- 0.646
- 0.945
- 0.959
- 0.865
- 0.98
- 0.979
- 0.113
- 0.529
- 0.611
- 0.646
- 0.85
- 0.718
- 0.7
- 0.3
- 0.847
- 0.746
- 0.314
- 0.486
- 0.5
- 0.93
- 0.719
- 0.7
- 0.814
- 0.275
- 0.906
- 0.137
- 0.833
- 0.836
- 0.656
- 0.769
- 0.636
- 0.842
- 0.636
- 0.964
- 0.95
- 0.582
- 0.731
- 0.85
- 0.866
- 0.63
- 0.875
- 0.846
- 0.86
- 0.621
- 0.638
- 0.527
- 0.58
- 0.66
- 0.8
- 0.914
- 0.915
- 0.84
train_loss:
- 1.289
- 0.779
- 0.697
- 0.644
- 0.495
- 0.583
- 0.466
- 0.449
- 0.513
- 0.481
- 0.493
- 0.472
- 0.369
- 0.399
- 0.469
- 0.347
- 0.42
- 0.408
- 0.397
- 0.414
- 0.365
- 0.374
- 0.405
- 0.333
- 0.402
- 0.351
- 0.432
- 0.401
- 0.356
- 0.364
- 0.355
- 0.356
- 0.328
- 0.37
- 0.371
- 0.33
- 0.342
- 0.328
- 0.348
- 0.322
- 0.287
- 0.394
- 0.323
- 0.341
- 0.333
- 0.347
- 0.38
- 0.394
- 0.387
- 0.341
- 0.335
- 0.337
- 0.277
- 0.319
- 0.354
- 0.331
- 0.32
- 0.343
- 0.3
- 0.371
- 0.372
- 0.345
- 0.319
- 0.357
- 0.319
- 0.327
- 0.332
- 0.355
- 0.325
- 0.367
- 0.301
- 0.302
- 0.305
- 0.318
- 0.346
- 0.329
- 0.335
- 0.32
- 0.366
- 0.383
- 0.33
- 0.283
- 0.339
- 0.284
- 0.324
- 0.349
- 0.335
- 0.365
- 0.304
- 0.372
- 0.418
- 0.354
- 0.347
- 0.305
- 0.314
- 0.347
- 0.315
- 0.353
- 0.328
- 0.314
unequal: 1
verbose: 1
