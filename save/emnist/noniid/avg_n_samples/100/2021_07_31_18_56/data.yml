avg_train_accuracy: 0.2
avg_train_loss: 0.004
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.16367021276595745
- 0.21670212765957447
- 0.32
- 0.32313829787234044
- 0.40382978723404256
- 0.3776595744680851
- 0.5160638297872341
- 0.49127659574468086
- 0.48914893617021277
- 0.5329787234042553
- 0.5398404255319149
- 0.5693617021276596
- 0.5816489361702127
- 0.5442021276595744
- 0.5473936170212766
- 0.5667553191489362
- 0.531063829787234
- 0.5478191489361702
- 0.5614361702127659
- 0.5456382978723404
- 0.5801063829787234
- 0.6125531914893617
- 0.5100531914893617
- 0.49909574468085105
- 0.6173936170212766
- 0.6411702127659574
- 0.5848936170212766
- 0.5887234042553191
- 0.6184574468085107
- 0.5979787234042553
- 0.6147340425531915
- 0.5881914893617022
- 0.643936170212766
- 0.666968085106383
- 0.6278191489361702
- 0.6752127659574468
- 0.6413829787234042
- 0.6351063829787233
- 0.6365425531914893
- 0.6470744680851064
- 0.6577659574468085
- 0.6156382978723405
- 0.6410638297872341
- 0.6579255319148937
- 0.6434042553191489
- 0.634468085106383
- 0.636436170212766
- 0.6545212765957447
- 0.6437234042553192
- 0.6551063829787234
- 0.6398404255319149
- 0.6227127659574468
- 0.590531914893617
- 0.6507446808510639
- 0.6470744680851064
- 0.6506914893617022
- 0.6797872340425531
- 0.6573404255319149
- 0.6743085106382979
- 0.6640425531914894
- 0.6596276595744681
- 0.6839893617021277
- 0.6618085106382978
- 0.6354255319148936
- 0.6202659574468085
- 0.6306914893617022
- 0.6530851063829787
- 0.6512765957446809
- 0.6225
- 0.6593617021276595
- 0.6881382978723404
- 0.6410106382978723
- 0.6302659574468085
- 0.6571276595744681
- 0.6340425531914894
- 0.6859042553191489
- 0.6447340425531914
- 0.6576063829787234
- 0.6437765957446808
- 0.6674468085106383
- 0.6738829787234043
- 0.6610106382978723
- 0.655
- 0.6488297872340425
- 0.6986702127659574
- 0.646595744680851
- 0.6894148936170212
- 0.6728723404255319
- 0.6797872340425531
- 0.6782978723404255
- 0.6455851063829787
- 0.6229787234042553
- 0.6831914893617022
- 0.6838829787234042
- 0.6691489361702128
- 0.6414893617021277
- 0.6713829787234042
- 0.6669148936170213
- 0.6835106382978723
- 0.6583510638297873
test_loss_list:
- 524.3804585933685
- 449.18071484565735
- 383.55469512939453
- 353.5334405899048
- 290.70207250118256
- 304.62423598766327
- 236.06446278095245
- 236.75582492351532
- 234.4607982635498
- 226.19512009620667
- 215.32961761951447
- 211.07465147972107
- 188.8983826637268
- 214.3590396642685
- 211.8094367980957
- 197.62948262691498
- 225.1494345664978
- 196.0498434305191
- 192.21487492322922
- 202.77177214622498
- 175.83261543512344
- 164.5828800201416
- 219.27099108695984
- 206.4939445257187
- 155.03128856420517
- 152.09673941135406
- 184.4669908285141
- 175.2861446738243
- 162.03763353824615
- 169.04569482803345
- 164.15234768390656
- 177.89143174886703
- 158.87857115268707
- 152.9968335032463
- 156.6042935848236
- 133.72338950634003
- 148.88150638341904
- 157.8912437558174
- 154.38945424556732
- 154.8081210255623
- 155.86234718561172
- 160.19827461242676
- 150.92182677984238
- 159.77105689048767
- 147.4874432682991
- 152.00914895534515
- 158.52971321344376
- 149.87487161159515
- 146.95557457208633
- 142.53273767232895
- 160.36170279979706
- 150.67684710025787
- 181.16727113723755
- 160.26967549324036
- 149.3144605755806
- 146.31741458177567
- 142.96926313638687
- 143.69561350345612
- 134.97159105539322
- 129.73475033044815
- 139.35687899589539
- 126.96987318992615
- 135.3589643239975
- 155.97370380163193
- 172.33067190647125
- 164.981411755085
- 153.10403156280518
- 144.44617027044296
- 158.82671362161636
- 146.96437937021255
- 122.81687480211258
- 150.65644544363022
- 167.90709763765335
- 140.98605108261108
- 151.86857974529266
- 130.91982340812683
- 165.4533606171608
- 139.31577217578888
- 150.931893825531
- 152.08319479227066
- 138.4500191807747
- 139.34739899635315
- 151.7968606352806
- 158.7091276049614
- 135.17187696695328
- 151.17668050527573
- 128.96383047103882
- 139.9970148205757
- 133.7292646765709
- 139.18559801578522
- 148.841606259346
- 156.86412900686264
- 130.1633066534996
- 127.29217994213104
- 133.49761843681335
- 146.00299715995789
- 137.5900036096573
- 142.6861553788185
- 144.239885866642
- 131.3976727128029
train_accuracy:
- 0.061
- 0.543
- 0.067
- 0.306
- 0.45
- 0.453
- 0.216
- 0.491
- 0.967
- 0.321
- 0.71
- 0.513
- 0.817
- 0.06
- 0.096
- 0.805
- 0.65
- 0.377
- 0.35
- 0.65
- 0.9
- 0.614
- 0.127
- 0.483
- 0.312
- 0.562
- 0.3
- 0.025
- 0.441
- 0.45
- 0.468
- 0.819
- 0.746
- 0.75
- 0.886
- 0.942
- 0.642
- 0.394
- 0.629
- 0.418
- 0.413
- 0.779
- 0.446
- 0.321
- 0.564
- 0.678
- 0.757
- 0.293
- 0.728
- 0.0
- 0.775
- 0.633
- 0.33
- 0.709
- 0.522
- 0.6
- 0.92
- 0.768
- 0.914
- 0.557
- 0.225
- 0.11
- 0.945
- 0.9
- 0.734
- 0.856
- 0.91
- 0.086
- 0.8
- 0.406
- 0.675
- 0.786
- 0.943
- 0.771
- 0.96
- 0.586
- 0.478
- 0.686
- 0.731
- 0.445
- 0.717
- 0.567
- 0.779
- 0.763
- 0.65
- 0.914
- 0.987
- 0.96
- 0.993
- 0.895
- 0.79
- 0.7
- 0.5
- 0.9
- 0.26
- 0.575
- 0.817
- 0.443
- 0.709
- 0.2
train_loss:
- 1.204
- 0.806
- 0.59
- 0.637
- 0.516
- 0.581
- 0.53
- 0.513
- 0.402
- 0.485
- 0.367
- 0.401
- 0.461
- 0.463
- 0.48
- 0.441
- 0.405
- 0.41
- 0.377
- 0.436
- 0.455
- 0.439
- 0.368
- 0.377
- 0.377
- 0.397
- 0.464
- 0.361
- 0.412
- 0.416
- 0.361
- 0.381
- 0.391
- 0.355
- 0.47
- 0.325
- 0.294
- 0.329
- 0.331
- 0.417
- 0.329
- 0.331
- 0.364
- 0.335
- 0.346
- 0.308
- 0.355
- 0.459
- 0.326
- 0.248
- 0.385
- 0.344
- 0.303
- 0.376
- 0.375
- 0.34
- 0.413
- 0.312
- 0.422
- 0.361
- 0.389
- 0.324
- 0.396
- 0.332
- 0.352
- 0.295
- 0.327
- 0.411
- 0.382
- 0.313
- 0.4
- 0.383
- 0.303
- 0.291
- 0.317
- 0.391
- 0.39
- 0.326
- 0.373
- 0.314
- 0.309
- 0.231
- 0.386
- 0.328
- 0.281
- 0.334
- 0.288
- 0.335
- 0.291
- 0.327
- 0.364
- 0.313
- 0.3
- 0.287
- 0.461
- 0.388
- 0.35
- 0.273
- 0.352
- 0.391
unequal: 1
verbose: 1
