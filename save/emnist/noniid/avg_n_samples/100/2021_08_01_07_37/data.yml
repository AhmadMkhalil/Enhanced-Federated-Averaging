avg_train_accuracy: 0.929
avg_train_loss: 0.004
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.11547872340425532
- 0.23297872340425532
- 0.3701063829787234
- 0.34611702127659577
- 0.44643617021276594
- 0.43617021276595747
- 0.459468085106383
- 0.5296808510638298
- 0.4804255319148936
- 0.5310106382978723
- 0.5653723404255319
- 0.5332978723404256
- 0.6326595744680851
- 0.59
- 0.5395744680851063
- 0.5794680851063829
- 0.6112765957446809
- 0.5987765957446809
- 0.5972340425531915
- 0.5869148936170213
- 0.5975
- 0.57
- 0.5993617021276596
- 0.6343085106382979
- 0.6086170212765958
- 0.591968085106383
- 0.6127127659574468
- 0.6101595744680851
- 0.6206914893617022
- 0.5825531914893617
- 0.6785638297872341
- 0.6348404255319149
- 0.6188829787234043
- 0.6342021276595745
- 0.5934574468085106
- 0.6667021276595745
- 0.7109042553191489
- 0.6343617021276595
- 0.708404255319149
- 0.6523936170212766
- 0.6333510638297872
- 0.6132446808510639
- 0.6301595744680851
- 0.5866489361702127
- 0.6017553191489362
- 0.5648404255319149
- 0.6648404255319149
- 0.6495212765957447
- 0.6487234042553192
- 0.6400531914893617
- 0.655904255319149
- 0.6525531914893618
- 0.6433510638297872
- 0.6433510638297872
- 0.6491489361702127
- 0.6141489361702127
- 0.6602127659574468
- 0.6422340425531915
- 0.6588297872340425
- 0.6587234042553192
- 0.6393085106382979
- 0.6298404255319149
- 0.6681914893617021
- 0.705
- 0.6626063829787234
- 0.7132446808510639
- 0.6640425531914894
- 0.7024468085106383
- 0.7410638297872341
- 0.6809042553191489
- 0.6518617021276596
- 0.6652659574468085
- 0.6515425531914893
- 0.6784042553191489
- 0.6340957446808511
- 0.71
- 0.6979787234042554
- 0.6867021276595745
- 0.7045744680851064
- 0.7201063829787234
- 0.7373404255319149
- 0.6623936170212766
- 0.688031914893617
- 0.6611702127659574
- 0.6474468085106383
- 0.7229787234042553
- 0.7094148936170213
- 0.6757978723404255
- 0.6293085106382978
- 0.6536170212765957
- 0.6602127659574468
- 0.6805851063829788
- 0.6748404255319149
- 0.7029255319148936
- 0.6822872340425532
- 0.6826595744680851
- 0.7068617021276595
- 0.6989893617021277
- 0.6648936170212766
- 0.7219148936170213
test_loss_list:
- 537.7696642875671
- 442.60767102241516
- 347.51202058792114
- 331.0483087301254
- 289.5135748386383
- 280.57770824432373
- 259.0139129161835
- 223.86587488651276
- 231.54170882701874
- 209.0564103126526
- 196.77806091308594
- 224.69565451145172
- 181.87828689813614
- 175.17641842365265
- 204.90481972694397
- 197.23067712783813
- 175.73727333545685
- 181.01100260019302
- 187.83028107881546
- 184.41714477539062
- 177.92998868227005
- 185.5789475440979
- 170.84921658039093
- 170.9466388821602
- 178.0076703429222
- 180.7959607243538
- 165.00467729568481
- 179.41505360603333
- 168.0333327651024
- 180.2163554430008
- 138.6125226020813
- 144.5383123755455
- 159.22340595722198
- 153.96180498600006
- 173.2965143918991
- 149.15188521146774
- 124.86201471090317
- 168.55962425470352
- 134.42252725362778
- 142.75418639183044
- 167.14024114608765
- 153.49308359622955
- 164.43597948551178
- 194.82546973228455
- 198.0983293056488
- 187.98013335466385
- 137.7586773633957
- 134.8313565850258
- 145.3825288414955
- 158.25608694553375
- 142.478630900383
- 158.1991975903511
- 161.466075360775
- 151.8020474910736
- 141.54156857728958
- 163.7628783583641
- 144.4845477938652
- 137.65705609321594
- 140.16527557373047
- 153.86957186460495
- 154.2435451745987
- 163.73084139823914
- 142.142471909523
- 123.60929316282272
- 139.25527012348175
- 121.89874744415283
- 145.52538520097733
- 136.411923289299
- 118.53750187158585
- 129.3293177485466
- 162.07112950086594
- 132.51082104444504
- 158.9695761203766
- 128.1977465748787
- 160.7475112080574
- 125.72361892461777
- 133.63769567012787
- 150.40575337409973
- 127.15889155864716
- 122.33367961645126
- 116.91584742069244
- 149.16553062200546
- 125.51641112565994
- 147.37191712856293
- 148.4916668534279
- 117.99170905351639
- 128.2188845872879
- 131.08507400751114
- 155.0851389169693
- 149.2338623404503
- 139.8880478143692
- 138.55015063285828
- 127.39465433359146
- 122.48457962274551
- 135.85330057144165
- 130.0341055393219
- 119.00651597976685
- 130.83723121881485
- 139.8120515346527
- 113.01045149564743
train_accuracy:
- 0.061
- 0.358
- 0.512
- 0.092
- 0.086
- 0.52
- 0.012
- 0.368
- 0.0
- 0.655
- 0.1
- 0.0
- 0.76
- 0.575
- 0.744
- 0.706
- 0.639
- 0.67
- 0.579
- 0.825
- 0.083
- 0.818
- 0.86
- 0.729
- 0.944
- 0.875
- 0.381
- 0.375
- 0.828
- 0.65
- 0.705
- 0.966
- 0.35
- 0.638
- 0.712
- 0.832
- 0.58
- 0.365
- 0.67
- 0.8
- 0.627
- 0.93
- 0.584
- 0.74
- 0.356
- 0.332
- 0.825
- 0.553
- 0.708
- 0.46
- 0.731
- 0.969
- 0.45
- 0.882
- 0.787
- 0.06
- 0.806
- 0.238
- 0.963
- 0.891
- 0.383
- 0.778
- 0.483
- 0.894
- 0.84
- 0.609
- 0.542
- 0.871
- 0.896
- 0.52
- 0.735
- 0.337
- 0.825
- 0.596
- 0.444
- 0.831
- 0.718
- 0.7
- 0.921
- 0.825
- 0.85
- 0.521
- 0.95
- 0.772
- 0.471
- 0.908
- 0.995
- 0.833
- 0.655
- 0.7
- 0.487
- 0.609
- 0.643
- 0.768
- 0.922
- 0.717
- 0.775
- 0.859
- 0.669
- 0.929
train_loss:
- 1.205
- 0.865
- 0.769
- 0.631
- 0.563
- 0.548
- 0.461
- 0.562
- 0.557
- 0.482
- 0.472
- 0.369
- 0.483
- 0.442
- 0.413
- 0.478
- 0.495
- 0.48
- 0.353
- 0.399
- 0.387
- 0.432
- 0.41
- 0.422
- 0.331
- 0.403
- 0.328
- 0.382
- 0.359
- 0.389
- 0.4
- 0.338
- 0.399
- 0.396
- 0.324
- 0.406
- 0.331
- 0.327
- 0.314
- 0.332
- 0.367
- 0.337
- 0.339
- 0.327
- 0.305
- 0.352
- 0.352
- 0.426
- 0.314
- 0.324
- 0.346
- 0.336
- 0.376
- 0.385
- 0.357
- 0.294
- 0.293
- 0.362
- 0.292
- 0.38
- 0.363
- 0.296
- 0.361
- 0.356
- 0.319
- 0.339
- 0.378
- 0.332
- 0.327
- 0.332
- 0.341
- 0.355
- 0.344
- 0.341
- 0.349
- 0.353
- 0.299
- 0.371
- 0.377
- 0.322
- 0.279
- 0.327
- 0.36
- 0.354
- 0.288
- 0.283
- 0.309
- 0.396
- 0.303
- 0.363
- 0.338
- 0.344
- 0.376
- 0.349
- 0.335
- 0.311
- 0.289
- 0.339
- 0.378
- 0.361
unequal: 1
verbose: 1
