avg_train_accuracy: 0.765
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.163031914893617
- 0.23111702127659575
- 0.3225531914893617
- 0.428936170212766
- 0.3680851063829787
- 0.4157446808510638
- 0.4997340425531915
- 0.541063829787234
- 0.4538829787234043
- 0.4196808510638298
- 0.48611702127659573
- 0.491436170212766
- 0.5482446808510638
- 0.5457446808510639
- 0.6307978723404255
- 0.5634574468085106
- 0.5859042553191489
- 0.554468085106383
- 0.5952127659574468
- 0.6072340425531915
- 0.6414893617021277
- 0.5912765957446808
- 0.5747872340425532
- 0.6328191489361702
- 0.5976063829787234
- 0.5781914893617022
- 0.6026063829787234
- 0.6317553191489361
- 0.6102659574468086
- 0.5915425531914894
- 0.6333510638297872
- 0.6408510638297872
- 0.5963297872340425
- 0.6316489361702128
- 0.5898936170212766
- 0.6179787234042553
- 0.6136170212765958
- 0.6973936170212766
- 0.6760106382978723
- 0.5895212765957447
- 0.6033510638297872
- 0.6704787234042553
- 0.6181382978723404
- 0.6160638297872341
- 0.595
- 0.6625
- 0.6076595744680852
- 0.6451595744680851
- 0.6614361702127659
- 0.663031914893617
- 0.6706382978723404
- 0.5988297872340426
- 0.5919148936170213
- 0.6506382978723404
- 0.6380851063829788
- 0.6309574468085106
- 0.6446808510638298
- 0.6517553191489361
- 0.6518617021276596
- 0.6518085106382979
- 0.6404255319148936
- 0.6679255319148936
- 0.6518085106382979
- 0.6023936170212766
- 0.6784574468085106
- 0.638031914893617
- 0.6462765957446809
- 0.6611702127659574
- 0.6652659574468085
- 0.6660106382978723
- 0.6575
- 0.6386702127659575
- 0.7076063829787234
- 0.6662765957446809
- 0.6966489361702127
- 0.6636170212765957
- 0.6629787234042553
- 0.6929787234042554
- 0.6463297872340426
- 0.5944148936170213
- 0.5720212765957446
- 0.6080319148936171
- 0.6266489361702128
- 0.6784574468085106
- 0.6547872340425532
- 0.6389893617021276
- 0.6537234042553192
- 0.6724468085106383
- 0.6578723404255319
- 0.5805851063829788
- 0.6383510638297872
- 0.6678191489361702
- 0.6553723404255319
- 0.7172872340425532
- 0.6328723404255319
- 0.6802659574468085
- 0.6413297872340425
- 0.6359574468085106
- 0.6698404255319149
- 0.6303191489361702
test_loss_list:
- 525.4910418987274
- 435.2779176235199
- 362.17309832572937
- 308.9746721982956
- 301.7933133840561
- 303.1661071777344
- 250.100484251976
- 226.88993775844574
- 264.42363929748535
- 279.0347423553467
- 231.1471688747406
- 211.9782373905182
- 199.8289223909378
- 192.18076300621033
- 155.47277468442917
- 222.87785375118256
- 180.6642867922783
- 208.3018193244934
- 186.80417519807816
- 194.05172210931778
- 160.01062321662903
- 179.25915670394897
- 175.93773692846298
- 161.49169850349426
- 178.9436423778534
- 184.77073806524277
- 173.98415857553482
- 154.40129208564758
- 167.43994414806366
- 172.12996596097946
- 158.1271425485611
- 156.0747926235199
- 159.64502441883087
- 158.20509296655655
- 181.80589205026627
- 165.11589193344116
- 164.59775483608246
- 135.3985087275505
- 139.92014753818512
- 171.03004443645477
- 165.3956988453865
- 143.90598064661026
- 155.41335159540176
- 172.1763277053833
- 181.0417709350586
- 148.31679344177246
- 169.31903743743896
- 158.3287461400032
- 142.73193567991257
- 139.2570277452469
- 145.72228801250458
- 185.3256595134735
- 184.06832873821259
- 140.49874806404114
- 144.34333717823029
- 153.72509956359863
- 159.7937467098236
- 142.65737962722778
- 155.81304901838303
- 148.21841657161713
- 152.67406505346298
- 135.15632390975952
- 142.99923264980316
- 176.00502520799637
- 135.23451256752014
- 146.4234635233879
- 145.89667385816574
- 151.7236932516098
- 141.85484087467194
- 140.52086585760117
- 146.98126602172852
- 155.65132355690002
- 124.20104002952576
- 142.81878745555878
- 129.94326877593994
- 135.93690198659897
- 140.3486698269844
- 128.67119425535202
- 161.2443065047264
- 181.4736948609352
- 179.88393408060074
- 159.12902772426605
- 163.52840852737427
- 145.97808653116226
- 151.70789068937302
- 147.80728030204773
- 153.12318861484528
- 132.31348091363907
- 146.17026072740555
- 183.50203758478165
- 144.71417784690857
- 131.56917262077332
- 143.13752591609955
- 114.24534976482391
- 155.1088582277298
- 137.28613567352295
- 144.1837182044983
- 159.08195841312408
- 162.27130258083344
- 149.31971782445908
train_accuracy:
- 0.097
- 0.244
- 0.238
- 0.394
- 0.145
- 0.577
- 0.7
- 0.9
- 0.364
- 0.808
- 0.77
- 0.375
- 0.865
- 0.407
- 0.75
- 0.1
- 0.635
- 0.95
- 0.95
- 0.625
- 0.805
- 0.582
- 0.025
- 0.404
- 0.673
- 0.842
- 0.814
- 0.583
- 0.614
- 0.733
- 0.221
- 0.875
- 0.642
- 0.663
- 0.821
- 0.425
- 0.533
- 0.763
- 0.712
- 0.344
- 0.75
- 0.521
- 0.912
- 0.524
- 0.867
- 1.0
- 0.242
- 0.775
- 0.288
- 0.574
- 0.871
- 0.282
- 0.933
- 0.45
- 0.812
- 0.867
- 0.967
- 0.7
- 0.031
- 0.704
- 0.394
- 0.225
- 0.492
- 0.888
- 0.912
- 0.5
- 0.603
- 0.89
- 0.6
- 0.663
- 0.95
- 0.873
- 0.642
- 0.874
- 0.59
- 0.713
- 0.8
- 0.864
- 0.61
- 0.331
- 0.922
- 0.432
- 0.75
- 0.575
- 0.65
- 0.825
- 0.944
- 0.954
- 0.933
- 0.49
- 0.792
- 0.842
- 0.617
- 0.819
- 0.55
- 0.438
- 0.633
- 0.332
- 0.781
- 0.765
train_loss:
- 1.081
- 0.819
- 0.669
- 0.638
- 0.586
- 0.435
- 0.341
- 0.46
- 0.504
- 0.376
- 0.482
- 0.401
- 0.442
- 0.396
- 0.345
- 0.38
- 0.394
- 0.34
- 0.307
- 0.399
- 0.346
- 0.424
- 0.282
- 0.345
- 0.384
- 0.384
- 0.31
- 0.384
- 0.405
- 0.391
- 0.33
- 0.316
- 0.304
- 0.3
- 0.284
- 0.349
- 0.288
- 0.343
- 0.385
- 0.366
- 0.316
- 0.283
- 0.36
- 0.342
- 0.372
- 0.319
- 0.245
- 0.347
- 0.266
- 0.424
- 0.314
- 0.334
- 0.288
- 0.389
- 0.296
- 0.362
- 0.357
- 0.294
- 0.356
- 0.372
- 0.361
- 0.294
- 0.365
- 0.318
- 0.404
- 0.292
- 0.333
- 0.302
- 0.343
- 0.348
- 0.326
- 0.294
- 0.367
- 0.318
- 0.342
- 0.299
- 0.264
- 0.332
- 0.311
- 0.302
- 0.325
- 0.278
- 0.308
- 0.327
- 0.27
- 0.277
- 0.276
- 0.333
- 0.328
- 0.312
- 0.374
- 0.363
- 0.256
- 0.271
- 0.281
- 0.306
- 0.356
- 0.357
- 0.225
- 0.336
unequal: 1
verbose: 1
