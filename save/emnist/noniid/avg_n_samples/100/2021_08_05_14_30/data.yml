avg_train_accuracy: 0.779
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.16882978723404254
- 0.26441489361702125
- 0.36351063829787233
- 0.3725
- 0.39611702127659576
- 0.42340425531914894
- 0.5085106382978724
- 0.5091489361702127
- 0.5494148936170212
- 0.5410106382978723
- 0.5055851063829787
- 0.5535106382978724
- 0.5917021276595744
- 0.5545744680851064
- 0.5594148936170212
- 0.6135106382978723
- 0.5270212765957447
- 0.5899468085106383
- 0.6156914893617021
- 0.6186170212765958
- 0.598404255319149
- 0.6268617021276596
- 0.5932978723404255
- 0.6248404255319149
- 0.6572340425531915
- 0.5970212765957447
- 0.5691489361702128
- 0.6588829787234043
- 0.5852127659574468
- 0.6322872340425532
- 0.6131914893617021
- 0.6545212765957447
- 0.6307446808510638
- 0.636436170212766
- 0.6103723404255319
- 0.609095744680851
- 0.6222340425531915
- 0.5520744680851064
- 0.6046808510638297
- 0.6256382978723404
- 0.6906914893617021
- 0.6882978723404255
- 0.6286170212765958
- 0.6200531914893617
- 0.6324468085106383
- 0.6400531914893617
- 0.6170744680851064
- 0.5864893617021276
- 0.6660106382978723
- 0.6289361702127659
- 0.6627127659574468
- 0.6666489361702128
- 0.6918085106382978
- 0.6189361702127659
- 0.5873936170212766
- 0.6519680851063829
- 0.6334574468085107
- 0.6715425531914894
- 0.6301063829787235
- 0.6215425531914893
- 0.6416489361702128
- 0.6322872340425532
- 0.6697340425531915
- 0.6704787234042553
- 0.68
- 0.6943085106382979
- 0.6923936170212766
- 0.6394148936170213
- 0.6419148936170213
- 0.6319148936170212
- 0.6557446808510639
- 0.6595212765957447
- 0.654468085106383
- 0.661595744680851
- 0.648936170212766
- 0.6420744680851064
- 0.6518085106382979
- 0.6560106382978723
- 0.6768617021276596
- 0.6834574468085106
- 0.6786170212765957
- 0.6495744680851064
- 0.6829255319148936
- 0.680531914893617
- 0.6431382978723404
- 0.646595744680851
- 0.6746276595744681
- 0.6603191489361702
- 0.6761170212765958
- 0.6704787234042553
- 0.6514893617021277
- 0.7175531914893617
- 0.7039893617021277
- 0.6404255319148936
- 0.7030851063829787
- 0.653031914893617
- 0.6542021276595744
- 0.6492021276595744
- 0.6304787234042554
- 0.6159042553191489
test_loss_list:
- 534.5163717269897
- 424.7447681427002
- 368.4669563770294
- 334.71581184864044
- 310.25148499011993
- 287.3417571783066
- 233.12771689891815
- 219.2111986875534
- 206.84609496593475
- 204.34327852725983
- 217.5305073261261
- 202.77148520946503
- 184.96107614040375
- 188.63446176052094
- 196.92605316638947
- 169.39965051412582
- 207.184907913208
- 190.15391409397125
- 175.38137590885162
- 168.85364371538162
- 182.91502040624619
- 165.6858559846878
- 178.87400150299072
- 156.78095710277557
- 153.1200652718544
- 186.0956221818924
- 190.25535666942596
- 154.65599685907364
- 184.11507153511047
- 154.86028295755386
- 168.63009452819824
- 144.0493511557579
- 155.77187359333038
- 161.3280884027481
- 156.1256901025772
- 173.05395716428757
- 169.96871900558472
- 197.61346447467804
- 156.38712710142136
- 150.77699160575867
- 131.40601176023483
- 135.7053394317627
- 159.08614432811737
- 177.64916896820068
- 156.42809998989105
- 151.7033959031105
- 169.40872156620026
- 183.1689339876175
- 143.47789865732193
- 161.72437399625778
- 145.0847355723381
- 132.88668209314346
- 128.35438066720963
- 155.354483127594
- 188.4449046254158
- 149.8310465812683
- 165.17439538240433
- 143.04495614767075
- 160.38812285661697
- 167.6860105395317
- 158.60574465990067
- 175.61339616775513
- 136.04805862903595
- 145.5450817346573
- 142.9303640127182
- 138.24522459506989
- 135.46596103906631
- 146.23848116397858
- 149.76585894823074
- 168.0011259317398
- 145.60349917411804
- 139.01815402507782
- 149.11316442489624
- 145.72864001989365
- 160.7384588122368
- 145.00874817371368
- 140.79614573717117
- 144.72648972272873
- 144.87895369529724
- 134.84460282325745
- 141.43294090032578
- 146.9208397269249
- 136.54882925748825
- 128.37660264968872
- 154.68084174394608
- 150.10422825813293
- 142.4074825644493
- 142.78664219379425
- 141.61733400821686
- 138.32965803146362
- 155.95765668153763
- 123.42367231845856
- 129.1982472538948
- 142.23684972524643
- 120.39166527986526
- 139.23385030031204
- 151.03607368469238
- 145.039426445961
- 161.17076992988586
- 153.11797803640366
train_accuracy:
- 0.022
- 0.183
- 0.58
- 0.714
- 0.245
- 0.963
- 0.133
- 0.436
- 0.758
- 0.978
- 0.262
- 0.017
- 0.661
- 0.017
- 0.641
- 0.25
- 0.733
- 0.292
- 0.97
- 0.753
- 0.525
- 0.58
- 0.08
- 0.729
- 0.275
- 0.72
- 0.263
- 0.958
- 0.763
- 0.523
- 0.742
- 0.957
- 0.493
- 0.779
- 0.325
- 0.581
- 0.489
- 0.867
- 0.7
- 0.394
- 0.912
- 0.625
- 0.804
- 0.462
- 1.0
- 0.875
- 0.596
- 0.5
- 0.386
- 0.592
- 0.906
- 0.592
- 0.89
- 0.515
- 0.67
- 0.869
- 0.596
- 0.975
- 0.511
- 0.7
- 0.455
- 0.317
- 0.835
- 0.694
- 0.921
- 0.508
- 0.55
- 0.225
- 0.653
- 0.822
- 0.328
- 0.7
- 0.859
- 0.38
- 0.462
- 0.788
- 0.425
- 0.938
- 0.787
- 0.857
- 0.881
- 0.673
- 0.94
- 0.908
- 0.636
- 0.668
- 0.94
- 0.897
- 0.73
- 0.625
- 0.643
- 0.842
- 0.825
- 0.506
- 0.619
- 0.587
- 0.281
- 0.511
- 0.116
- 0.779
train_loss:
- 1.204
- 0.783
- 0.682
- 0.725
- 0.548
- 0.557
- 0.487
- 0.527
- 0.379
- 0.4
- 0.392
- 0.446
- 0.415
- 0.444
- 0.417
- 0.463
- 0.38
- 0.476
- 0.392
- 0.316
- 0.422
- 0.367
- 0.366
- 0.319
- 0.367
- 0.328
- 0.356
- 0.33
- 0.399
- 0.368
- 0.389
- 0.361
- 0.337
- 0.404
- 0.383
- 0.343
- 0.425
- 0.324
- 0.393
- 0.411
- 0.314
- 0.37
- 0.386
- 0.298
- 0.342
- 0.375
- 0.335
- 0.259
- 0.396
- 0.288
- 0.36
- 0.364
- 0.318
- 0.339
- 0.331
- 0.4
- 0.281
- 0.363
- 0.354
- 0.378
- 0.341
- 0.283
- 0.414
- 0.302
- 0.372
- 0.269
- 0.367
- 0.34
- 0.393
- 0.319
- 0.351
- 0.363
- 0.305
- 0.271
- 0.31
- 0.405
- 0.356
- 0.262
- 0.318
- 0.32
- 0.388
- 0.355
- 0.332
- 0.271
- 0.293
- 0.311
- 0.248
- 0.386
- 0.344
- 0.248
- 0.275
- 0.361
- 0.327
- 0.349
- 0.343
- 0.346
- 0.315
- 0.426
- 0.37
- 0.327
unequal: 1
verbose: 1
