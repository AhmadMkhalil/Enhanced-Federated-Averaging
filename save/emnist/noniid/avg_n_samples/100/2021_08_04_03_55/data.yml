avg_train_accuracy: 0.912
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.21643617021276595
- 0.27074468085106385
- 0.3780851063829787
- 0.4085106382978723
- 0.3598936170212766
- 0.3836702127659574
- 0.4366489361702128
- 0.4845212765957447
- 0.5758510638297872
- 0.5473936170212766
- 0.5307978723404255
- 0.6088297872340426
- 0.5427127659574468
- 0.5518085106382978
- 0.476968085106383
- 0.5519148936170213
- 0.5943085106382979
- 0.614468085106383
- 0.5727659574468085
- 0.5026063829787234
- 0.5829787234042553
- 0.583936170212766
- 0.5965425531914894
- 0.6301063829787235
- 0.6082446808510639
- 0.6344148936170213
- 0.6236702127659575
- 0.6386702127659575
- 0.612872340425532
- 0.6601595744680852
- 0.6562765957446809
- 0.6080851063829787
- 0.6221808510638298
- 0.6019148936170213
- 0.6130851063829788
- 0.6093085106382978
- 0.6153191489361702
- 0.543031914893617
- 0.6096276595744681
- 0.6558510638297872
- 0.6410106382978723
- 0.6598404255319149
- 0.6697340425531915
- 0.6555851063829787
- 0.6811170212765958
- 0.6846808510638298
- 0.6436702127659575
- 0.6127127659574468
- 0.5856914893617021
- 0.6225
- 0.6322872340425532
- 0.6134042553191489
- 0.6289893617021277
- 0.6058510638297873
- 0.5973404255319149
- 0.6777127659574468
- 0.6581914893617021
- 0.6786702127659574
- 0.636436170212766
- 0.6421808510638298
- 0.6524468085106383
- 0.6850531914893617
- 0.6440425531914894
- 0.668031914893617
- 0.6348936170212766
- 0.6005851063829787
- 0.6943085106382979
- 0.7164893617021276
- 0.6427127659574469
- 0.6720212765957447
- 0.6786170212765957
- 0.6886170212765957
- 0.6733510638297873
- 0.6368617021276596
- 0.6161702127659574
- 0.6391489361702127
- 0.7004787234042553
- 0.6579255319148937
- 0.5832978723404255
- 0.6764361702127659
- 0.6905851063829788
- 0.6473936170212766
- 0.708031914893617
- 0.6907446808510638
- 0.6350531914893617
- 0.6701595744680852
- 0.6162234042553192
- 0.6719148936170213
- 0.6814361702127659
- 0.6990425531914893
- 0.6960106382978724
- 0.7031382978723404
- 0.6445744680851064
- 0.6875531914893617
- 0.6604787234042553
- 0.6747340425531915
- 0.6267553191489361
- 0.6367553191489361
- 0.6634042553191489
- 0.6678723404255319
test_loss_list:
- 520.4716937541962
- 419.6408100128174
- 350.97653245925903
- 311.6522207260132
- 362.7428501844406
- 313.1048365831375
- 273.7931020259857
- 241.81412816047668
- 210.9727998971939
- 239.99185001850128
- 218.53939521312714
- 196.27382826805115
- 206.99220311641693
- 202.11052882671356
- 251.32112836837769
- 195.092658162117
- 183.5282266139984
- 180.97878974676132
- 183.1541041135788
- 221.85099351406097
- 174.17222583293915
- 188.11296319961548
- 175.04051440954208
- 162.16447180509567
- 164.7506935596466
- 161.8225628733635
- 164.92905455827713
- 160.8705969452858
- 165.63515847921371
- 158.51719546318054
- 149.90375983715057
- 165.93018013238907
- 174.10267639160156
- 173.2337011694908
- 163.24339747428894
- 166.82071667909622
- 163.4476072192192
- 194.11487066745758
- 165.1871348619461
- 149.3614729642868
- 150.314466714859
- 149.72724372148514
- 153.29547852277756
- 147.68743669986725
- 134.6366840004921
- 141.67129743099213
- 150.99205446243286
- 160.00371807813644
- 167.42024850845337
- 157.9747707247734
- 152.57228362560272
- 152.5737332701683
- 154.15780586004257
- 160.9271795153618
- 172.02000522613525
- 146.05556333065033
- 140.1084486246109
- 132.29803383350372
- 145.658296585083
- 150.30599069595337
- 151.79779160022736
- 129.21993458271027
- 140.96401929855347
- 150.55024874210358
- 162.4263527393341
- 180.50499147176743
- 129.0982848405838
- 123.29046607017517
- 147.29151558876038
- 140.5709193944931
- 157.36673879623413
- 145.07057398557663
- 137.16041111946106
- 148.48795199394226
- 167.40317463874817
- 151.12299489974976
- 130.24721360206604
- 137.39434558153152
- 180.2623815536499
- 138.03537702560425
- 128.78954362869263
- 148.6310037970543
- 131.26060545444489
- 139.6467062830925
- 157.3045009970665
- 135.65977692604065
- 158.6066910624504
- 141.3306179046631
- 130.95231527090073
- 126.90218198299408
- 129.64424574375153
- 133.6515718102455
- 154.09905797243118
- 130.0210349559784
- 135.1165855526924
- 148.63424050807953
- 158.14721781015396
- 150.49757552146912
- 143.8889021873474
- 155.18722367286682
train_accuracy:
- 0.0
- 0.219
- 0.777
- 0.243
- 0.907
- 0.659
- 0.625
- 0.711
- 0.812
- 0.633
- 0.892
- 0.553
- 0.407
- 0.607
- 0.257
- 0.514
- 0.944
- 0.527
- 0.262
- 0.217
- 0.637
- 0.0
- 0.67
- 0.508
- 0.677
- 0.43
- 0.786
- 0.576
- 0.782
- 0.312
- 0.819
- 0.67
- 0.9
- 0.353
- 0.697
- 0.93
- 0.372
- 0.2
- 0.64
- 0.6
- 0.625
- 0.783
- 0.76
- 0.607
- 0.792
- 0.862
- 0.896
- 0.4
- 0.633
- 0.555
- 0.55
- 0.925
- 0.681
- 0.962
- 0.383
- 0.663
- 0.297
- 0.527
- 0.917
- 0.931
- 0.375
- 0.907
- 0.811
- 0.647
- 0.282
- 0.769
- 0.775
- 0.481
- 0.527
- 0.821
- 0.854
- 0.8
- 0.573
- 0.25
- 0.45
- 0.425
- 0.642
- 0.864
- 0.547
- 0.919
- 0.167
- 0.965
- 0.7
- 0.569
- 0.45
- 0.72
- 0.875
- 0.1
- 0.0
- 0.515
- 0.682
- 0.934
- 0.624
- 0.904
- 0.65
- 0.736
- 0.144
- 0.818
- 0.859
- 0.912
train_loss:
- 1.168
- 0.792
- 0.664
- 0.569
- 0.456
- 0.551
- 0.473
- 0.485
- 0.491
- 0.39
- 0.437
- 0.489
- 0.459
- 0.438
- 0.579
- 0.492
- 0.42
- 0.426
- 0.372
- 0.512
- 0.439
- 0.447
- 0.432
- 0.344
- 0.431
- 0.36
- 0.408
- 0.371
- 0.336
- 0.347
- 0.394
- 0.344
- 0.35
- 0.361
- 0.39
- 0.336
- 0.39
- 0.438
- 0.408
- 0.443
- 0.385
- 0.336
- 0.369
- 0.34
- 0.382
- 0.357
- 0.303
- 0.379
- 0.346
- 0.331
- 0.347
- 0.378
- 0.344
- 0.35
- 0.311
- 0.365
- 0.361
- 0.342
- 0.346
- 0.379
- 0.33
- 0.337
- 0.411
- 0.396
- 0.366
- 0.405
- 0.354
- 0.312
- 0.334
- 0.325
- 0.398
- 0.417
- 0.393
- 0.374
- 0.357
- 0.411
- 0.349
- 0.389
- 0.38
- 0.361
- 0.381
- 0.292
- 0.388
- 0.356
- 0.299
- 0.329
- 0.34
- 0.306
- 0.322
- 0.332
- 0.373
- 0.341
- 0.377
- 0.349
- 0.355
- 0.334
- 0.338
- 0.39
- 0.33
- 0.33
unequal: 1
verbose: 1
