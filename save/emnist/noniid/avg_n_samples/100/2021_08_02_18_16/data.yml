avg_train_accuracy: 0.85
avg_train_loss: 0.004
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1271808510638298
- 0.23835106382978724
- 0.3050531914893617
- 0.35297872340425535
- 0.4476063829787234
- 0.44632978723404254
- 0.5194148936170213
- 0.49361702127659574
- 0.5202127659574468
- 0.4728723404255319
- 0.47936170212765955
- 0.5197872340425532
- 0.5679787234042554
- 0.5072872340425532
- 0.5053723404255319
- 0.5592021276595744
- 0.5415425531914894
- 0.49856382978723407
- 0.5764893617021276
- 0.6070744680851063
- 0.5895744680851064
- 0.5349468085106382
- 0.5699468085106383
- 0.6283510638297872
- 0.6397340425531914
- 0.5802659574468085
- 0.6426063829787234
- 0.6360106382978723
- 0.6066489361702128
- 0.5696808510638298
- 0.5930319148936171
- 0.6226063829787234
- 0.6210638297872341
- 0.6561702127659574
- 0.625531914893617
- 0.6476595744680851
- 0.5979255319148936
- 0.5993085106382978
- 0.5714893617021276
- 0.639468085106383
- 0.6438829787234043
- 0.6256914893617022
- 0.6215425531914893
- 0.5540425531914893
- 0.6411170212765958
- 0.6305851063829787
- 0.5740425531914893
- 0.6228191489361702
- 0.6463297872340426
- 0.6354787234042554
- 0.6548404255319149
- 0.6280851063829788
- 0.6679255319148936
- 0.6636702127659575
- 0.6157446808510638
- 0.5836702127659574
- 0.626436170212766
- 0.6483510638297872
- 0.6876063829787235
- 0.6540425531914894
- 0.6312765957446809
- 0.6251063829787235
- 0.6059574468085106
- 0.5979787234042553
- 0.635
- 0.6595744680851063
- 0.6440425531914894
- 0.5915425531914894
- 0.6762234042553191
- 0.6555319148936171
- 0.671968085106383
- 0.6361702127659574
- 0.6387234042553191
- 0.6453723404255319
- 0.6803191489361702
- 0.630531914893617
- 0.6749468085106383
- 0.6848404255319149
- 0.6700531914893617
- 0.6674468085106383
- 0.6731382978723405
- 0.6968085106382979
- 0.6913829787234043
- 0.6624468085106383
- 0.6332446808510638
- 0.6381382978723404
- 0.71
- 0.6345744680851064
- 0.6741489361702128
- 0.6770212765957446
- 0.673936170212766
- 0.6439893617021276
- 0.6454787234042553
- 0.6607446808510639
- 0.6458510638297872
- 0.6413829787234042
- 0.6213297872340425
- 0.6313297872340425
- 0.673031914893617
- 0.6304255319148936
test_loss_list:
- 532.4372987747192
- 454.5603940486908
- 398.1496295928955
- 351.3604543209076
- 304.10548889636993
- 280.237585067749
- 272.93180108070374
- 269.4916788339615
- 246.19441175460815
- 255.73447954654694
- 239.01622366905212
- 209.82725143432617
- 204.0119788646698
- 226.1738795042038
- 222.68801867961884
- 187.94637846946716
- 206.28031742572784
- 235.43894374370575
- 179.67098093032837
- 178.42782843112946
- 187.28987020254135
- 202.88470816612244
- 198.4235657453537
- 177.4638438820839
- 154.0661696791649
- 171.89923006296158
- 151.82083070278168
- 167.15922218561172
- 157.85008722543716
- 175.78758919239044
- 171.60606503486633
- 160.90763622522354
- 171.9261456131935
- 159.48824280500412
- 169.8913133740425
- 163.676520049572
- 184.30961567163467
- 177.70011121034622
- 181.70927673578262
- 155.48661947250366
- 149.65339267253876
- 154.3867838382721
- 159.0363790988922
- 189.26586651802063
- 153.78312212228775
- 170.90583056211472
- 198.7584655880928
- 152.29589664936066
- 161.31209832429886
- 153.89053040742874
- 144.11745178699493
- 164.78592258691788
- 142.88395237922668
- 133.35632449388504
- 157.0569548010826
- 183.9202362895012
- 156.79443073272705
- 149.37076425552368
- 139.64218550920486
- 149.50673580169678
- 157.09468895196915
- 154.97871887683868
- 166.45900851488113
- 165.6327327489853
- 149.85635274648666
- 142.61432284116745
- 148.45794093608856
- 183.5866333246231
- 141.6110560297966
- 150.26074314117432
- 136.0289270877838
- 150.21606826782227
- 152.95942777395248
- 142.8277508020401
- 153.173100233078
- 160.4766410589218
- 132.93679147958755
- 145.55335468053818
- 150.07853615283966
- 132.0838360786438
- 135.14777702093124
- 135.91812938451767
- 128.06821709871292
- 152.80886709690094
- 154.00877451896667
- 140.9860122203827
- 125.13794732093811
- 146.3267039656639
- 135.0710676908493
- 141.2387109398842
- 128.1795602440834
- 136.446682035923
- 152.1195571422577
- 135.23066526651382
- 149.8687435388565
- 146.26092982292175
- 152.53574854135513
- 160.16505599021912
- 141.44983249902725
- 158.88033390045166
train_accuracy:
- 0.068
- 0.042
- 0.21
- 0.417
- 0.375
- 0.2
- 0.737
- 0.233
- 0.883
- 0.52
- 0.053
- 0.243
- 0.54
- 0.297
- 0.456
- 0.823
- 0.389
- 0.433
- 0.5
- 0.538
- 0.9
- 0.39
- 0.478
- 0.742
- 0.683
- 0.279
- 0.79
- 0.935
- 0.569
- 0.831
- 0.4
- 0.25
- 0.025
- 0.496
- 0.405
- 0.727
- 0.356
- 0.625
- 0.062
- 0.55
- 0.492
- 0.758
- 0.283
- 0.597
- 0.506
- 0.292
- 0.193
- 0.825
- 0.708
- 0.533
- 0.672
- 0.883
- 0.865
- 0.479
- 0.643
- 0.856
- 0.577
- 0.39
- 0.862
- 0.85
- 0.987
- 0.678
- 0.628
- 0.385
- 0.6
- 0.772
- 0.489
- 0.326
- 0.483
- 0.85
- 0.659
- 0.593
- 0.9
- 0.706
- 0.553
- 0.7
- 0.494
- 0.975
- 0.496
- 0.884
- 0.938
- 0.94
- 0.633
- 0.656
- 0.769
- 0.79
- 0.514
- 0.944
- 0.538
- 0.583
- 0.967
- 0.862
- 0.704
- 0.9
- 0.803
- 0.6
- 0.508
- 0.289
- 0.875
- 0.85
train_loss:
- 1.164
- 0.782
- 0.687
- 0.7
- 0.649
- 0.501
- 0.405
- 0.48
- 0.462
- 0.43
- 0.566
- 0.457
- 0.437
- 0.474
- 0.361
- 0.428
- 0.355
- 0.423
- 0.385
- 0.359
- 0.372
- 0.445
- 0.342
- 0.395
- 0.372
- 0.33
- 0.39
- 0.354
- 0.415
- 0.397
- 0.391
- 0.343
- 0.416
- 0.31
- 0.281
- 0.308
- 0.401
- 0.362
- 0.44
- 0.363
- 0.417
- 0.339
- 0.317
- 0.388
- 0.361
- 0.346
- 0.306
- 0.344
- 0.352
- 0.343
- 0.4
- 0.348
- 0.334
- 0.339
- 0.363
- 0.313
- 0.403
- 0.319
- 0.291
- 0.334
- 0.341
- 0.365
- 0.376
- 0.331
- 0.339
- 0.338
- 0.268
- 0.345
- 0.326
- 0.362
- 0.331
- 0.364
- 0.301
- 0.341
- 0.29
- 0.311
- 0.371
- 0.258
- 0.314
- 0.358
- 0.379
- 0.32
- 0.362
- 0.283
- 0.34
- 0.307
- 0.384
- 0.367
- 0.364
- 0.315
- 0.315
- 0.325
- 0.349
- 0.362
- 0.268
- 0.376
- 0.276
- 0.278
- 0.281
- 0.354
unequal: 1
verbose: 1
