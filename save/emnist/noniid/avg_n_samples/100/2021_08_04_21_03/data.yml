avg_train_accuracy: 0.617
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1626063829787234
- 0.16063829787234044
- 0.25611702127659575
- 0.3401063829787234
- 0.3255851063829787
- 0.39952127659574466
- 0.4672340425531915
- 0.4174468085106383
- 0.47191489361702127
- 0.48675531914893616
- 0.5041489361702127
- 0.5407446808510639
- 0.555531914893617
- 0.4921808510638298
- 0.5721808510638298
- 0.619468085106383
- 0.5502127659574468
- 0.5505851063829788
- 0.614095744680851
- 0.574627659574468
- 0.5969148936170213
- 0.6321808510638298
- 0.5686170212765957
- 0.6197872340425532
- 0.555
- 0.5722872340425532
- 0.5685106382978723
- 0.613031914893617
- 0.6353723404255319
- 0.6002659574468086
- 0.6441489361702127
- 0.6473936170212766
- 0.6232978723404256
- 0.6572340425531915
- 0.5981382978723404
- 0.5999468085106383
- 0.6525531914893618
- 0.6368085106382979
- 0.5998936170212766
- 0.6827127659574468
- 0.6457446808510638
- 0.6444148936170213
- 0.6220744680851064
- 0.6295744680851064
- 0.663936170212766
- 0.6667553191489362
- 0.6636170212765957
- 0.636968085106383
- 0.6406382978723404
- 0.6815425531914894
- 0.7164893617021276
- 0.643031914893617
- 0.6759042553191489
- 0.7213297872340425
- 0.6936170212765957
- 0.6869148936170213
- 0.6667021276595745
- 0.6676063829787234
- 0.6661170212765958
- 0.6694148936170212
- 0.6656914893617021
- 0.6775531914893617
- 0.6611170212765958
- 0.7088297872340426
- 0.6871808510638298
- 0.7015425531914894
- 0.6729255319148936
- 0.6721808510638297
- 0.6694148936170212
- 0.6786170212765957
- 0.6167021276595744
- 0.6748404255319149
- 0.6327659574468085
- 0.6335106382978724
- 0.6414893617021277
- 0.6682446808510638
- 0.6789893617021276
- 0.6695212765957447
- 0.6903191489361702
- 0.6832978723404255
- 0.6807446808510639
- 0.6702127659574468
- 0.6830851063829787
- 0.6801063829787234
- 0.6345744680851064
- 0.6502127659574468
- 0.6622340425531915
- 0.6625531914893616
- 0.6677659574468086
- 0.7027659574468085
- 0.6885638297872341
- 0.7007446808510638
- 0.695
- 0.6968617021276595
- 0.6948404255319149
- 0.6954787234042553
- 0.6925531914893617
- 0.6659574468085107
- 0.6721276595744681
- 0.6479255319148937
test_loss_list:
- 536.686704158783
- 513.0570304393768
- 402.9069480895996
- 349.0025682449341
- 347.0638573169708
- 298.7177757024765
- 253.15801787376404
- 269.98132026195526
- 245.36856615543365
- 245.78085470199585
- 223.89501285552979
- 197.60574877262115
- 203.33004355430603
- 246.80266213417053
- 195.1844276189804
- 173.95638447999954
- 193.2824103832245
- 202.20771181583405
- 181.04184919595718
- 170.86937081813812
- 184.43253004550934
- 153.0721681714058
- 181.09848773479462
- 164.67093467712402
- 181.4469535946846
- 175.69848507642746
- 189.3409168124199
- 184.04006606340408
- 166.2191562652588
- 195.79561269283295
- 156.6534914970398
- 171.38750886917114
- 159.08939814567566
- 152.61137849092484
- 171.0320794582367
- 160.60960048437119
- 143.51466929912567
- 146.971040725708
- 163.2233013510704
- 147.96074777841568
- 146.281165599823
- 144.57643061876297
- 163.22373354434967
- 147.9866247177124
- 139.78982269763947
- 135.95402932167053
- 133.885762155056
- 148.34311693906784
- 149.535649061203
- 137.25243723392487
- 122.20167076587677
- 154.05045557022095
- 130.11268776655197
- 117.564846098423
- 128.6316020488739
- 131.78643018007278
- 148.98496413230896
- 138.96122688055038
- 156.07763189077377
- 134.12716925144196
- 154.11906337738037
- 138.06720823049545
- 141.5476496219635
- 126.8785548210144
- 135.0264014005661
- 129.09137654304504
- 133.95678210258484
- 147.94972562789917
- 140.69248193502426
- 148.30304366350174
- 165.94103956222534
- 137.5930870771408
- 141.19411045312881
- 154.4008713364601
- 143.81718462705612
- 132.36128896474838
- 130.5389010310173
- 137.72808051109314
- 128.13995665311813
- 145.50857716798782
- 126.8916164636612
- 140.39903908967972
- 128.22075980901718
- 140.89179062843323
- 163.90869957208633
- 151.4990963935852
- 143.96272218227386
- 143.68675255775452
- 132.15940594673157
- 134.13168907165527
- 128.40647345781326
- 128.45974719524384
- 139.86249196529388
- 125.93508678674698
- 125.28648006916046
- 126.77562391757965
- 130.39480036497116
- 134.71737617254257
- 131.71783369779587
- 146.3669165968895
train_accuracy:
- 0.003
- 0.0
- 0.43
- 0.8
- 0.233
- 0.883
- 0.684
- 0.036
- 0.282
- 0.266
- 0.492
- 0.083
- 0.318
- 0.833
- 0.513
- 0.779
- 0.635
- 0.78
- 0.555
- 0.891
- 0.873
- 0.775
- 0.74
- 0.555
- 0.672
- 0.544
- 0.95
- 0.95
- 0.52
- 0.683
- 0.31
- 0.475
- 0.83
- 0.959
- 0.56
- 0.33
- 0.942
- 0.838
- 0.718
- 0.73
- 0.845
- 0.154
- 0.879
- 0.95
- 0.67
- 0.9
- 0.89
- 0.929
- 0.494
- 0.3
- 0.775
- 0.927
- 0.425
- 0.664
- 0.607
- 0.542
- 0.532
- 0.379
- 0.647
- 0.786
- 0.856
- 0.868
- 0.645
- 0.67
- 0.416
- 0.908
- 0.885
- 0.925
- 0.473
- 0.31
- 0.429
- 0.871
- 0.834
- 0.85
- 0.594
- 0.561
- 0.308
- 0.925
- 0.694
- 0.763
- 0.5
- 0.471
- 0.864
- 0.667
- 0.55
- 0.0
- 0.644
- 0.373
- 0.894
- 0.925
- 0.643
- 0.935
- 0.938
- 0.95
- 0.85
- 0.34
- 0.695
- 0.69
- 0.756
- 0.617
train_loss:
- 1.198
- 0.748
- 0.712
- 0.643
- 0.514
- 0.536
- 0.453
- 0.467
- 0.497
- 0.523
- 0.387
- 0.418
- 0.385
- 0.292
- 0.376
- 0.402
- 0.38
- 0.392
- 0.358
- 0.398
- 0.386
- 0.38
- 0.389
- 0.311
- 0.41
- 0.347
- 0.375
- 0.261
- 0.375
- 0.372
- 0.396
- 0.293
- 0.381
- 0.297
- 0.371
- 0.328
- 0.366
- 0.358
- 0.308
- 0.345
- 0.371
- 0.332
- 0.386
- 0.326
- 0.32
- 0.32
- 0.362
- 0.392
- 0.419
- 0.291
- 0.373
- 0.37
- 0.363
- 0.339
- 0.355
- 0.35
- 0.333
- 0.375
- 0.322
- 0.312
- 0.34
- 0.315
- 0.319
- 0.34
- 0.316
- 0.334
- 0.391
- 0.288
- 0.377
- 0.326
- 0.353
- 0.337
- 0.379
- 0.333
- 0.301
- 0.365
- 0.379
- 0.287
- 0.309
- 0.254
- 0.289
- 0.316
- 0.302
- 0.351
- 0.337
- 0.297
- 0.336
- 0.395
- 0.359
- 0.266
- 0.292
- 0.286
- 0.258
- 0.34
- 0.359
- 0.281
- 0.344
- 0.34
- 0.355
- 0.343
unequal: 1
verbose: 1
