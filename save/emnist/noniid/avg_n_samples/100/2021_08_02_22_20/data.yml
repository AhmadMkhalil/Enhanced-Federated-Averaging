avg_train_accuracy: 0.942
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.15914893617021278
- 0.29819148936170214
- 0.27595744680851064
- 0.3206914893617021
- 0.4018617021276596
- 0.3579787234042553
- 0.4765957446808511
- 0.45398936170212767
- 0.5420744680851064
- 0.5183510638297872
- 0.46632978723404256
- 0.5065425531914893
- 0.6064893617021276
- 0.5713829787234043
- 0.6033510638297872
- 0.5033510638297872
- 0.5739893617021277
- 0.5921276595744681
- 0.6292021276595745
- 0.6540425531914894
- 0.5782978723404255
- 0.5977127659574468
- 0.5738297872340425
- 0.5461702127659575
- 0.5532978723404255
- 0.5620212765957446
- 0.601436170212766
- 0.6223936170212766
- 0.5586170212765957
- 0.6418617021276596
- 0.6032446808510639
- 0.5975531914893617
- 0.6004787234042553
- 0.5467021276595745
- 0.5993085106382978
- 0.606436170212766
- 0.6452659574468085
- 0.6517553191489361
- 0.6484574468085106
- 0.6264893617021277
- 0.6031382978723404
- 0.6740957446808511
- 0.6279255319148936
- 0.5902127659574468
- 0.6159574468085106
- 0.625531914893617
- 0.6528191489361702
- 0.6210638297872341
- 0.6131914893617021
- 0.6317553191489361
- 0.6531382978723405
- 0.6375531914893617
- 0.6528191489361702
- 0.6542553191489362
- 0.6677127659574468
- 0.6647340425531915
- 0.6630851063829787
- 0.6681382978723405
- 0.6826595744680851
- 0.6552659574468085
- 0.6475
- 0.6447340425531914
- 0.6695212765957447
- 0.680372340425532
- 0.6624468085106383
- 0.6782446808510638
- 0.6609574468085107
- 0.6876595744680851
- 0.6681914893617021
- 0.6627127659574468
- 0.6513829787234042
- 0.6444148936170213
- 0.6057978723404255
- 0.6327127659574469
- 0.6237234042553191
- 0.6220212765957447
- 0.6863297872340426
- 0.6536170212765957
- 0.6478191489361702
- 0.6258510638297873
- 0.6442553191489362
- 0.665904255319149
- 0.6450531914893617
- 0.6418617021276596
- 0.6718085106382978
- 0.6710106382978723
- 0.6334574468085107
- 0.7032978723404255
- 0.6563829787234042
- 0.6225
- 0.5311702127659574
- 0.6798404255319149
- 0.6663829787234042
- 0.6197340425531915
- 0.661063829787234
- 0.6201595744680851
- 0.6316489361702128
- 0.6425531914893617
- 0.6293617021276596
- 0.6211170212765957
test_loss_list:
- 535.1812171936035
- 444.1209852695465
- 400.00322341918945
- 354.859751701355
- 302.9198088645935
- 319.4916261434555
- 264.07233679294586
- 260.3509409427643
- 227.00259792804718
- 256.5207464694977
- 286.6915228366852
- 242.72569179534912
- 199.07715713977814
- 199.32209134101868
- 198.61680245399475
- 205.94362151622772
- 188.91388130187988
- 181.889290869236
- 163.29509317874908
- 166.09040546417236
- 172.05064004659653
- 170.10362482070923
- 184.67924797534943
- 201.16659200191498
- 183.88124001026154
- 181.3402522802353
- 174.85193049907684
- 180.32222151756287
- 198.27014684677124
- 147.210484623909
- 174.35819536447525
- 170.865736246109
- 175.32259649038315
- 201.8414089679718
- 171.41266125440598
- 167.20310616493225
- 156.91479116678238
- 152.51304441690445
- 146.43560528755188
- 162.68695002794266
- 179.85813224315643
- 149.1133064031601
- 163.24865573644638
- 176.4664061665535
- 176.555321931839
- 148.6493283510208
- 144.1545506119728
- 155.74761140346527
- 154.18690294027328
- 150.70709091424942
- 165.86751198768616
- 164.07795304059982
- 155.37945246696472
- 146.10583984851837
- 144.43084061145782
- 135.22218656539917
- 142.50745391845703
- 142.94039380550385
- 135.30322992801666
- 160.86246037483215
- 139.3037247657776
- 156.81159925460815
- 144.17609691619873
- 134.61784303188324
- 142.93158984184265
- 133.05654120445251
- 132.97614169120789
- 130.88414537906647
- 143.15210288763046
- 143.32869082689285
- 143.4535357952118
- 140.4399065375328
- 158.34512442350388
- 152.04603517055511
- 153.83674758672714
- 150.4554267525673
- 140.9920237660408
- 150.9349531531334
- 146.12499129772186
- 164.54654723405838
- 161.59779012203217
- 157.6554805636406
- 163.4614872932434
- 151.45788043737411
- 127.10705834627151
- 141.38614517450333
- 149.2024707198143
- 126.15394526720047
- 137.5237199664116
- 149.6382621526718
- 222.15129375457764
- 135.45172810554504
- 147.69537335634232
- 158.06378203630447
- 152.51166707277298
- 145.11399364471436
- 150.3833048939705
- 155.20908254384995
- 163.4233226776123
- 153.78021150827408
train_accuracy:
- 0.406
- 0.131
- 0.095
- 0.234
- 0.506
- 0.0
- 0.422
- 0.283
- 0.72
- 0.609
- 0.231
- 0.15
- 0.875
- 0.438
- 0.443
- 0.167
- 0.964
- 0.435
- 0.862
- 0.55
- 0.333
- 0.682
- 0.288
- 0.458
- 0.538
- 0.325
- 0.543
- 0.732
- 0.715
- 0.394
- 0.623
- 0.679
- 0.706
- 0.435
- 0.312
- 0.466
- 0.5
- 0.894
- 0.631
- 0.805
- 0.3
- 0.7
- 0.763
- 0.963
- 0.192
- 0.958
- 0.761
- 0.66
- 0.819
- 0.55
- 0.888
- 0.867
- 0.653
- 0.541
- 0.417
- 0.492
- 0.855
- 0.388
- 0.363
- 0.9
- 0.363
- 0.29
- 0.692
- 0.989
- 0.623
- 0.905
- 0.072
- 0.363
- 0.369
- 0.113
- 0.662
- 0.268
- 0.394
- 0.572
- 0.671
- 0.736
- 0.245
- 0.893
- 0.9
- 0.642
- 0.47
- 0.722
- 0.967
- 0.631
- 0.862
- 0.87
- 0.444
- 0.506
- 0.778
- 0.562
- 0.183
- 0.842
- 0.323
- 0.336
- 0.95
- 0.897
- 0.95
- 0.955
- 0.162
- 0.942
train_loss:
- 1.212
- 0.807
- 0.757
- 0.624
- 0.487
- 0.482
- 0.496
- 0.44
- 0.446
- 0.412
- 0.419
- 0.437
- 0.305
- 0.512
- 0.401
- 0.368
- 0.312
- 0.393
- 0.352
- 0.405
- 0.357
- 0.424
- 0.286
- 0.477
- 0.312
- 0.462
- 0.369
- 0.372
- 0.389
- 0.35
- 0.369
- 0.369
- 0.335
- 0.368
- 0.35
- 0.385
- 0.34
- 0.342
- 0.371
- 0.292
- 0.262
- 0.327
- 0.453
- 0.304
- 0.262
- 0.382
- 0.399
- 0.27
- 0.356
- 0.333
- 0.359
- 0.329
- 0.349
- 0.315
- 0.321
- 0.341
- 0.267
- 0.323
- 0.382
- 0.325
- 0.289
- 0.272
- 0.304
- 0.376
- 0.383
- 0.273
- 0.299
- 0.29
- 0.344
- 0.31
- 0.316
- 0.348
- 0.391
- 0.406
- 0.274
- 0.364
- 0.364
- 0.296
- 0.348
- 0.346
- 0.294
- 0.304
- 0.36
- 0.309
- 0.364
- 0.275
- 0.262
- 0.336
- 0.326
- 0.294
- 0.26
- 0.377
- 0.39
- 0.334
- 0.358
- 0.335
- 0.275
- 0.288
- 0.287
- 0.34
unequal: 1
verbose: 1
