avg_train_accuracy: 0.343
avg_train_loss: 0.004
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.15617021276595744
- 0.32143617021276594
- 0.3220212765957447
- 0.38659574468085106
- 0.4351063829787234
- 0.4198404255319149
- 0.4731382978723404
- 0.5125
- 0.5031914893617021
- 0.5118085106382979
- 0.5577127659574468
- 0.5671808510638298
- 0.5557446808510639
- 0.5586702127659574
- 0.5712234042553191
- 0.5455319148936171
- 0.5932446808510639
- 0.5902659574468085
- 0.6372872340425532
- 0.5408510638297872
- 0.6108510638297873
- 0.6466489361702128
- 0.585
- 0.6353723404255319
- 0.6569148936170213
- 0.6356914893617022
- 0.6218617021276596
- 0.5953191489361702
- 0.566968085106383
- 0.6412765957446809
- 0.5923936170212766
- 0.6688829787234043
- 0.6614361702127659
- 0.6076063829787234
- 0.6239361702127659
- 0.6202127659574468
- 0.6198404255319149
- 0.5995212765957447
- 0.6317553191489361
- 0.6578723404255319
- 0.644468085106383
- 0.6245212765957446
- 0.6756382978723404
- 0.6821276595744681
- 0.6273404255319149
- 0.6406382978723404
- 0.601436170212766
- 0.6195212765957446
- 0.6315957446808511
- 0.6496808510638298
- 0.6571808510638298
- 0.6372340425531915
- 0.6643085106382979
- 0.6126063829787234
- 0.6779787234042554
- 0.6286170212765958
- 0.6628191489361702
- 0.6651595744680852
- 0.6578723404255319
- 0.6205851063829787
- 0.6381382978723404
- 0.6420212765957447
- 0.5848936170212766
- 0.6101595744680851
- 0.6517553191489361
- 0.636968085106383
- 0.6793617021276596
- 0.7065957446808511
- 0.6689893617021276
- 0.6661702127659574
- 0.6391489361702127
- 0.6581914893617021
- 0.6437765957446808
- 0.6741489361702128
- 0.6623404255319149
- 0.6625
- 0.6887765957446809
- 0.6926595744680851
- 0.6937234042553192
- 0.6661170212765958
- 0.6925531914893617
- 0.6588829787234043
- 0.6366489361702128
- 0.6912765957446808
- 0.7092021276595745
- 0.6626063829787234
- 0.6631382978723405
- 0.6699468085106383
- 0.6505319148936171
- 0.6978191489361703
- 0.6872872340425532
- 0.6775531914893617
- 0.6832978723404255
- 0.6779255319148936
- 0.658563829787234
- 0.6412234042553191
- 0.7256914893617021
- 0.6857446808510639
- 0.653936170212766
- 0.6544148936170213
test_loss_list:
- 533.9603900909424
- 430.5092191696167
- 373.8607106208801
- 329.2863291501999
- 288.8921730518341
- 287.8008155822754
- 266.89400339126587
- 236.51055574417114
- 233.96979773044586
- 214.62330722808838
- 207.99540996551514
- 184.54870337247849
- 209.0938093662262
- 202.09563851356506
- 181.88716727495193
- 205.94286501407623
- 180.36202770471573
- 171.35448640584946
- 159.41219121217728
- 196.87878906726837
- 173.45103096961975
- 161.23923182487488
- 180.03854233026505
- 159.620166182518
- 147.91605031490326
- 164.44400036334991
- 160.53473043441772
- 160.22204798460007
- 188.42356145381927
- 154.90508604049683
- 172.01856237649918
- 143.17381739616394
- 161.28968971967697
- 155.42426306009293
- 150.04109781980515
- 168.79061722755432
- 161.10152596235275
- 169.17031288146973
- 167.92492020130157
- 147.06370681524277
- 158.74216848611832
- 164.0811743736267
- 138.41838878393173
- 135.81677269935608
- 178.73242753744125
- 157.7945030927658
- 186.8185511827469
- 168.03414982557297
- 155.81694620847702
- 153.2812824845314
- 157.30496490001678
- 150.90962433815002
- 138.9369358420372
- 163.03874909877777
- 135.09234219789505
- 167.38695150613785
- 143.84450322389603
- 137.99819213151932
- 143.13519775867462
- 158.71153259277344
- 152.75773447752
- 142.46964114904404
- 167.85777860879898
- 169.5090011358261
- 162.2032304406166
- 151.6834997534752
- 141.9476620554924
- 125.5978792309761
- 145.06330931186676
- 137.07100826501846
- 159.24414557218552
- 143.56256330013275
- 154.5555019378662
- 139.43685674667358
- 151.15246826410294
- 150.88316410779953
- 128.58412379026413
- 126.75610327720642
- 129.2183051109314
- 134.61721777915955
- 135.06482791900635
- 146.4695058465004
- 148.2033230662346
- 131.32680869102478
- 127.66754180192947
- 138.14016580581665
- 147.8170706629753
- 165.97431063652039
- 134.62074333429337
- 124.07456475496292
- 125.47287148237228
- 139.49586308002472
- 139.27925568819046
- 133.95141530036926
- 146.6653664112091
- 144.76685345172882
- 119.80340683460236
- 129.08768570423126
- 138.2685445547104
- 132.64234054088593
train_accuracy:
- 0.65
- 0.384
- 0.442
- 0.471
- 0.297
- 0.453
- 0.756
- 0.519
- 0.669
- 0.911
- 0.306
- 0.742
- 0.78
- 0.406
- 0.808
- 0.675
- 0.833
- 0.644
- 0.65
- 0.794
- 0.513
- 0.792
- 0.01
- 0.9
- 0.771
- 0.728
- 0.459
- 0.885
- 0.636
- 0.756
- 0.819
- 0.95
- 0.853
- 0.35
- 0.641
- 0.879
- 0.692
- 0.565
- 0.525
- 0.523
- 0.556
- 0.967
- 0.856
- 0.859
- 0.933
- 0.625
- 0.841
- 0.395
- 0.779
- 0.587
- 0.739
- 0.15
- 0.529
- 0.866
- 0.35
- 0.436
- 0.871
- 0.55
- 0.428
- 0.625
- 0.122
- 0.592
- 0.56
- 0.305
- 0.194
- 0.9
- 0.231
- 0.878
- 0.86
- 0.565
- 0.35
- 0.832
- 0.2
- 0.972
- 0.425
- 0.928
- 0.939
- 0.823
- 0.725
- 0.947
- 0.838
- 0.672
- 0.746
- 0.569
- 0.838
- 0.831
- 0.586
- 0.906
- 0.33
- 0.528
- 0.693
- 0.858
- 0.731
- 0.678
- 0.8
- 0.629
- 0.969
- 0.838
- 0.569
- 0.343
train_loss:
- 1.193
- 0.898
- 0.547
- 0.636
- 0.573
- 0.393
- 0.532
- 0.492
- 0.512
- 0.453
- 0.474
- 0.429
- 0.392
- 0.428
- 0.433
- 0.491
- 0.425
- 0.394
- 0.402
- 0.357
- 0.444
- 0.41
- 0.422
- 0.435
- 0.373
- 0.465
- 0.392
- 0.373
- 0.361
- 0.344
- 0.38
- 0.354
- 0.452
- 0.398
- 0.386
- 0.353
- 0.343
- 0.343
- 0.362
- 0.365
- 0.333
- 0.331
- 0.352
- 0.364
- 0.371
- 0.369
- 0.344
- 0.354
- 0.416
- 0.36
- 0.414
- 0.338
- 0.326
- 0.408
- 0.351
- 0.37
- 0.335
- 0.29
- 0.339
- 0.347
- 0.337
- 0.393
- 0.392
- 0.373
- 0.365
- 0.33
- 0.348
- 0.32
- 0.389
- 0.324
- 0.309
- 0.36
- 0.398
- 0.353
- 0.326
- 0.337
- 0.312
- 0.338
- 0.462
- 0.364
- 0.338
- 0.403
- 0.312
- 0.262
- 0.324
- 0.293
- 0.349
- 0.419
- 0.375
- 0.335
- 0.315
- 0.373
- 0.294
- 0.332
- 0.313
- 0.348
- 0.292
- 0.242
- 0.266
- 0.356
unequal: 1
verbose: 1
