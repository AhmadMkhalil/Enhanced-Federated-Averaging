avg_train_accuracy: 0.531
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.15122340425531916
- 0.24154255319148937
- 0.27617021276595743
- 0.3097340425531915
- 0.42558510638297875
- 0.3854787234042553
- 0.4469148936170213
- 0.5118617021276596
- 0.5199468085106383
- 0.5930319148936171
- 0.5489893617021276
- 0.5419148936170213
- 0.502127659574468
- 0.5051063829787235
- 0.5715957446808511
- 0.5842553191489361
- 0.5497340425531915
- 0.5839893617021277
- 0.5020212765957447
- 0.5632978723404255
- 0.5514893617021277
- 0.63
- 0.5963829787234043
- 0.6172872340425531
- 0.5973404255319149
- 0.6092553191489362
- 0.6113297872340425
- 0.6331914893617021
- 0.6442021276595745
- 0.6262234042553192
- 0.6253191489361702
- 0.575
- 0.6455851063829787
- 0.6232978723404256
- 0.6428723404255319
- 0.6296808510638298
- 0.6138297872340426
- 0.618031914893617
- 0.6018085106382979
- 0.6093085106382978
- 0.6325
- 0.6110638297872341
- 0.6667021276595745
- 0.6353191489361703
- 0.6567021276595745
- 0.6531382978723405
- 0.6600531914893617
- 0.6601595744680852
- 0.6859042553191489
- 0.6516489361702128
- 0.5508510638297872
- 0.6524468085106383
- 0.651436170212766
- 0.7287765957446809
- 0.6501063829787234
- 0.6611170212765958
- 0.5844148936170213
- 0.6937765957446809
- 0.6407978723404255
- 0.6732446808510638
- 0.7013829787234043
- 0.6443085106382979
- 0.6305851063829787
- 0.6217553191489362
- 0.6438829787234043
- 0.640531914893617
- 0.6737765957446809
- 0.5828191489361703
- 0.6394148936170213
- 0.5963829787234043
- 0.6789893617021276
- 0.6626063829787234
- 0.6053191489361702
- 0.6405851063829787
- 0.6498936170212766
- 0.6278191489361702
- 0.6680851063829787
- 0.6952127659574469
- 0.6956914893617021
- 0.6809042553191489
- 0.6531914893617021
- 0.6551063829787234
- 0.625
- 0.6598404255319149
- 0.6473936170212766
- 0.6698404255319149
- 0.7011702127659575
- 0.637340425531915
- 0.6554255319148936
- 0.6636702127659575
- 0.6790425531914893
- 0.6432446808510638
- 0.6687765957446808
- 0.6734042553191489
- 0.6997872340425532
- 0.7062234042553192
- 0.7135106382978723
- 0.6678191489361702
- 0.6871276595744681
- 0.6675531914893617
test_loss_list:
- 536.4073040485382
- 447.4884777069092
- 402.9181447029114
- 380.2705616950989
- 300.0591324567795
- 290.5785300731659
- 266.52789759635925
- 226.76792979240417
- 236.3010321855545
- 206.5825595855713
- 202.746497631073
- 214.1313523054123
- 230.2334704399109
- 213.92763924598694
- 185.09401774406433
- 181.11202079057693
- 199.15974748134613
- 183.4443615078926
- 224.4109445810318
- 188.9892921447754
- 210.6636176109314
- 163.27411222457886
- 171.7854404449463
- 172.50893849134445
- 192.727010846138
- 179.8123641014099
- 170.31589394807816
- 160.89516466856003
- 151.37881857156754
- 159.917345225811
- 156.27834057807922
- 167.89250695705414
- 156.1541079878807
- 167.2583725452423
- 167.0636270046234
- 162.57436549663544
- 169.3206267952919
- 176.58631950616837
- 167.78748106956482
- 173.9978812932968
- 158.31735694408417
- 162.53524613380432
- 150.79687416553497
- 157.910402238369
- 147.15202409029007
- 154.3952544927597
- 148.3068152666092
- 151.0597180724144
- 136.98811328411102
- 156.2898822426796
- 187.71893018484116
- 142.95329630374908
- 148.04781299829483
- 123.2727832198143
- 144.30077004432678
- 145.62791693210602
- 170.72266727685928
- 143.8629211783409
- 153.6458750963211
- 154.8080593943596
- 128.2605721950531
- 156.10226392745972
- 158.04070818424225
- 155.20938503742218
- 148.09246695041656
- 171.82973223924637
- 144.30247378349304
- 178.5474471449852
- 150.03722459077835
- 173.218754529953
- 144.78776824474335
- 144.24132871627808
- 164.1754453778267
- 146.75490146875381
- 145.61623775959015
- 152.2215404510498
- 142.71923863887787
- 141.28589457273483
- 140.3186338543892
- 132.8487303853035
- 133.46342581510544
- 144.3059573173523
- 150.15536081790924
- 148.98772430419922
- 147.33767837285995
- 137.12164044380188
- 132.8008130788803
- 171.44242638349533
- 148.62889802455902
- 149.67920631170273
- 142.8006967306137
- 147.99884259700775
- 152.9073742032051
- 146.34553676843643
- 133.01346516609192
- 124.82875072956085
- 129.09772622585297
- 141.4012850522995
- 131.15896379947662
- 129.23438584804535
train_accuracy:
- 0.0
- 0.004
- 0.045
- 0.207
- 0.539
- 0.133
- 0.482
- 0.342
- 0.545
- 0.472
- 0.507
- 0.342
- 0.933
- 0.176
- 0.931
- 0.5
- 0.184
- 0.641
- 0.05
- 0.841
- 0.185
- 0.844
- 0.4
- 0.954
- 0.66
- 0.583
- 0.692
- 0.529
- 0.6
- 0.35
- 0.914
- 0.29
- 0.689
- 0.556
- 0.55
- 0.836
- 0.955
- 0.846
- 0.706
- 0.8
- 0.718
- 0.718
- 0.8
- 0.325
- 0.81
- 0.45
- 0.509
- 0.543
- 0.961
- 0.875
- 0.05
- 0.834
- 0.933
- 0.656
- 0.731
- 0.691
- 0.691
- 0.626
- 0.359
- 0.586
- 0.66
- 0.754
- 0.742
- 0.595
- 0.936
- 0.835
- 0.267
- 0.179
- 0.367
- 0.075
- 0.721
- 0.859
- 0.864
- 0.775
- 0.646
- 0.883
- 0.9
- 0.495
- 0.808
- 0.719
- 0.88
- 0.678
- 0.067
- 0.767
- 0.238
- 0.866
- 0.736
- 0.809
- 0.796
- 0.358
- 0.533
- 0.559
- 0.784
- 0.812
- 0.631
- 0.655
- 0.936
- 0.967
- 0.517
- 0.531
train_loss:
- 1.163
- 0.879
- 0.677
- 0.679
- 0.64
- 0.647
- 0.53
- 0.47
- 0.497
- 0.508
- 0.493
- 0.5
- 0.408
- 0.399
- 0.46
- 0.442
- 0.421
- 0.488
- 0.388
- 0.47
- 0.421
- 0.343
- 0.413
- 0.398
- 0.363
- 0.407
- 0.314
- 0.419
- 0.449
- 0.404
- 0.337
- 0.458
- 0.357
- 0.39
- 0.337
- 0.425
- 0.383
- 0.362
- 0.385
- 0.396
- 0.359
- 0.43
- 0.262
- 0.377
- 0.398
- 0.395
- 0.404
- 0.276
- 0.332
- 0.321
- 0.37
- 0.409
- 0.351
- 0.471
- 0.342
- 0.343
- 0.338
- 0.36
- 0.301
- 0.309
- 0.335
- 0.381
- 0.409
- 0.329
- 0.34
- 0.354
- 0.292
- 0.338
- 0.34
- 0.277
- 0.351
- 0.337
- 0.264
- 0.385
- 0.415
- 0.319
- 0.366
- 0.378
- 0.367
- 0.376
- 0.358
- 0.366
- 0.366
- 0.368
- 0.304
- 0.364
- 0.283
- 0.315
- 0.279
- 0.324
- 0.347
- 0.401
- 0.347
- 0.292
- 0.308
- 0.388
- 0.29
- 0.394
- 0.309
- 0.348
unequal: 1
verbose: 1
