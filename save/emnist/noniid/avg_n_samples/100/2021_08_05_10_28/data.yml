avg_train_accuracy: 0.639
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.15611702127659574
- 0.24409574468085107
- 0.3071808510638298
- 0.37372340425531914
- 0.4629255319148936
- 0.44074468085106383
- 0.4466489361702128
- 0.5423404255319149
- 0.523563829787234
- 0.4718085106382979
- 0.5242553191489362
- 0.4647872340425532
- 0.5460106382978723
- 0.543563829787234
- 0.538936170212766
- 0.5510106382978723
- 0.6076595744680852
- 0.5642021276595744
- 0.5020212765957447
- 0.5863297872340425
- 0.6170744680851064
- 0.5593085106382979
- 0.5604787234042553
- 0.601968085106383
- 0.5288829787234043
- 0.5348404255319149
- 0.604095744680851
- 0.6180851063829788
- 0.5928191489361702
- 0.6042553191489362
- 0.6575
- 0.6092021276595745
- 0.6497872340425532
- 0.645531914893617
- 0.6326063829787234
- 0.6886702127659574
- 0.6343085106382979
- 0.601063829787234
- 0.6086702127659575
- 0.686968085106383
- 0.6475
- 0.6621276595744681
- 0.6575
- 0.6472340425531915
- 0.6422872340425532
- 0.6746808510638298
- 0.6118085106382979
- 0.6625531914893616
- 0.658031914893617
- 0.5573404255319149
- 0.6188297872340426
- 0.6609574468085107
- 0.5639893617021277
- 0.6595212765957447
- 0.6447872340425532
- 0.6260638297872341
- 0.6773936170212767
- 0.5907446808510638
- 0.684468085106383
- 0.6476595744680851
- 0.6901595744680851
- 0.6693085106382979
- 0.6481382978723405
- 0.6596276595744681
- 0.6865425531914894
- 0.6656382978723404
- 0.6513297872340426
- 0.6793085106382979
- 0.6445744680851064
- 0.6401063829787234
- 0.6218617021276596
- 0.6954787234042553
- 0.6550531914893617
- 0.6620744680851064
- 0.630531914893617
- 0.6744148936170212
- 0.603563829787234
- 0.6427127659574469
- 0.6710106382978723
- 0.6692021276595744
- 0.6982978723404255
- 0.7013297872340426
- 0.6949468085106383
- 0.6653191489361702
- 0.5946808510638298
- 0.668031914893617
- 0.6773404255319149
- 0.7151063829787234
- 0.6477127659574468
- 0.6318085106382979
- 0.6734042553191489
- 0.6037765957446809
- 0.6153723404255319
- 0.6767021276595745
- 0.6539893617021276
- 0.6551063829787234
- 0.6645744680851063
- 0.6396276595744681
- 0.6388829787234043
- 0.6798936170212766
test_loss_list:
- 527.8206412792206
- 454.1656355857849
- 390.3091731071472
- 339.277704000473
- 294.5497648715973
- 289.7549729347229
- 284.83676421642303
- 216.72152733802795
- 221.65669000148773
- 247.65917563438416
- 221.69451820850372
- 239.57909214496613
- 208.4062522649765
- 210.5071403980255
- 204.0845866203308
- 211.53344583511353
- 177.2414447069168
- 200.57894277572632
- 233.9941189289093
- 184.48774194717407
- 185.72706645727158
- 195.41484379768372
- 187.67757964134216
- 172.23771357536316
- 217.0967687368393
- 212.93923556804657
- 163.17921900749207
- 170.6532786488533
- 189.15956676006317
- 164.78729581832886
- 146.43617260456085
- 162.57923811674118
- 156.20564007759094
- 157.95571571588516
- 151.97648185491562
- 139.741550385952
- 166.37388342618942
- 159.32559061050415
- 175.95202672481537
- 136.72872149944305
- 144.84019315242767
- 156.20710027217865
- 144.5265053510666
- 153.57832467556
- 156.03055453300476
- 134.6637834906578
- 165.07585763931274
- 146.00128626823425
- 136.0415956377983
- 190.99290430545807
- 160.6132031083107
- 133.07697582244873
- 188.70617371797562
- 145.3079543709755
- 150.64434683322906
- 161.36141049861908
- 146.24333173036575
- 166.1698940396309
- 132.0441738963127
- 150.37710624933243
- 131.9754617214203
- 150.8419845700264
- 153.24778389930725
- 146.37076032161713
- 138.60913348197937
- 145.08560013771057
- 148.06485199928284
- 146.22203105688095
- 146.987203001976
- 143.14635747671127
- 172.94072610139847
- 128.669639647007
- 151.77315604686737
- 140.1481570005417
- 161.68113458156586
- 137.35918021202087
- 168.1114000082016
- 159.67765283584595
- 142.54769122600555
- 131.21423280239105
- 124.76182180643082
- 125.64478421211243
- 127.25407618284225
- 144.29175972938538
- 168.44124937057495
- 159.14591586589813
- 138.325250685215
- 122.50044512748718
- 143.48277360200882
- 165.04219490289688
- 131.7899707555771
- 163.7694965004921
- 155.25151205062866
- 135.97219574451447
- 140.53969824314117
- 148.17642349004745
- 141.6624874472618
- 149.19417482614517
- 141.48600655794144
- 132.7034239768982
train_accuracy:
- 0.508
- 0.322
- 0.0
- 0.737
- 0.489
- 0.45
- 0.723
- 0.525
- 0.197
- 0.0
- 0.093
- 0.654
- 0.736
- 0.611
- 0.894
- 0.477
- 0.35
- 0.826
- 0.616
- 0.0
- 0.725
- 0.53
- 0.609
- 0.955
- 0.309
- 0.256
- 0.777
- 0.57
- 0.815
- 0.153
- 0.0
- 0.869
- 0.781
- 0.573
- 0.56
- 0.875
- 0.764
- 0.944
- 0.665
- 0.768
- 0.838
- 0.95
- 0.363
- 0.792
- 0.8
- 0.905
- 0.945
- 0.8
- 0.941
- 0.8
- 0.945
- 0.907
- 0.482
- 0.6
- 0.565
- 0.514
- 0.63
- 0.634
- 0.927
- 0.29
- 0.591
- 0.963
- 0.538
- 0.75
- 0.944
- 0.57
- 0.215
- 0.382
- 0.467
- 0.972
- 0.95
- 0.686
- 0.75
- 0.732
- 0.7
- 0.758
- 0.544
- 0.828
- 0.647
- 0.787
- 0.358
- 0.741
- 0.817
- 0.491
- 0.594
- 0.585
- 0.782
- 0.819
- 0.381
- 0.356
- 0.844
- 0.817
- 0.514
- 0.814
- 0.44
- 0.425
- 0.217
- 0.821
- 0.363
- 0.639
train_loss:
- 1.028
- 0.853
- 0.65
- 0.55
- 0.684
- 0.524
- 0.525
- 0.546
- 0.436
- 0.459
- 0.491
- 0.429
- 0.457
- 0.421
- 0.403
- 0.458
- 0.395
- 0.435
- 0.437
- 0.405
- 0.389
- 0.378
- 0.399
- 0.42
- 0.358
- 0.371
- 0.342
- 0.434
- 0.406
- 0.398
- 0.352
- 0.425
- 0.297
- 0.412
- 0.353
- 0.354
- 0.393
- 0.339
- 0.391
- 0.385
- 0.406
- 0.292
- 0.405
- 0.384
- 0.352
- 0.384
- 0.37
- 0.39
- 0.394
- 0.316
- 0.33
- 0.352
- 0.335
- 0.447
- 0.338
- 0.369
- 0.354
- 0.345
- 0.399
- 0.371
- 0.348
- 0.278
- 0.301
- 0.293
- 0.364
- 0.283
- 0.369
- 0.414
- 0.368
- 0.319
- 0.325
- 0.319
- 0.286
- 0.389
- 0.299
- 0.363
- 0.34
- 0.32
- 0.435
- 0.356
- 0.363
- 0.374
- 0.33
- 0.316
- 0.287
- 0.332
- 0.272
- 0.321
- 0.328
- 0.364
- 0.32
- 0.307
- 0.291
- 0.367
- 0.285
- 0.317
- 0.318
- 0.381
- 0.315
- 0.336
unequal: 1
verbose: 1
