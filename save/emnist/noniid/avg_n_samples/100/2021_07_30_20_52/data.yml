avg_train_accuracy: 0.246
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.13632978723404254
- 0.2976063829787234
- 0.2995744680851064
- 0.3108510638297872
- 0.3753723404255319
- 0.4699468085106383
- 0.4580851063829787
- 0.505904255319149
- 0.47297872340425534
- 0.49707446808510636
- 0.5565425531914894
- 0.513031914893617
- 0.5204787234042553
- 0.6080851063829787
- 0.5453191489361702
- 0.48840425531914894
- 0.5453191489361702
- 0.5565425531914894
- 0.5466489361702128
- 0.5870744680851064
- 0.565531914893617
- 0.5792021276595745
- 0.556063829787234
- 0.5745744680851064
- 0.5815425531914894
- 0.5629255319148936
- 0.589627659574468
- 0.5632978723404255
- 0.5514361702127659
- 0.6389361702127659
- 0.6487765957446808
- 0.6024468085106383
- 0.6220212765957447
- 0.6190425531914894
- 0.632127659574468
- 0.678936170212766
- 0.6053191489361702
- 0.640531914893617
- 0.6291489361702127
- 0.6207978723404255
- 0.5738297872340425
- 0.6360106382978723
- 0.5985106382978723
- 0.6180851063829788
- 0.5881914893617022
- 0.6514893617021277
- 0.6576595744680851
- 0.651595744680851
- 0.6234042553191489
- 0.6251063829787235
- 0.6963297872340426
- 0.6453191489361703
- 0.6253723404255319
- 0.6417021276595745
- 0.6671276595744681
- 0.6627127659574468
- 0.6525531914893618
- 0.6388829787234043
- 0.6452127659574468
- 0.5892553191489361
- 0.6392553191489362
- 0.601436170212766
- 0.6074468085106383
- 0.6537765957446808
- 0.6409574468085106
- 0.6572872340425532
- 0.616968085106383
- 0.6664893617021277
- 0.660904255319149
- 0.7027127659574468
- 0.5947872340425532
- 0.6692021276595744
- 0.660372340425532
- 0.6550531914893617
- 0.6026595744680852
- 0.7048404255319148
- 0.6497872340425532
- 0.6369148936170212
- 0.5986170212765958
- 0.6387765957446808
- 0.6331914893617021
- 0.5763297872340426
- 0.6840425531914893
- 0.6732446808510638
- 0.6763829787234042
- 0.6717553191489362
- 0.618031914893617
- 0.6385106382978724
- 0.6953723404255319
- 0.6412234042553191
- 0.6539893617021276
- 0.6545212765957447
- 0.695531914893617
- 0.6406914893617022
- 0.6623936170212766
- 0.6628723404255319
- 0.6462765957446809
- 0.6417553191489361
- 0.6315425531914893
- 0.6536170212765957
test_loss_list:
- 528.6950123310089
- 434.3161163330078
- 396.4794850349426
- 372.79292464256287
- 324.50532257556915
- 275.8298155069351
- 248.41547667980194
- 231.39378094673157
- 232.24309647083282
- 227.31388580799103
- 218.10788917541504
- 223.97192072868347
- 212.64650106430054
- 171.89993274211884
- 197.812713265419
- 229.8144212961197
- 205.45767605304718
- 202.002556681633
- 203.4363261461258
- 192.78125298023224
- 190.5405775308609
- 177.1794375181198
- 187.19075095653534
- 191.55687004327774
- 185.0348784327507
- 187.46513682603836
- 173.77214592695236
- 170.76149255037308
- 190.8969680070877
- 147.477614402771
- 158.24506431818008
- 181.77311354875565
- 160.35099309682846
- 160.32425260543823
- 160.15782672166824
- 145.64552092552185
- 171.6440098285675
- 160.84042286872864
- 158.1253650188446
- 167.36378145217896
- 183.61625981330872
- 159.52781796455383
- 157.86729538440704
- 154.24562615156174
- 190.45812207460403
- 154.01633685827255
- 141.9340500831604
- 147.8076851963997
- 162.3592689037323
- 172.02551954984665
- 134.72407710552216
- 148.05175626277924
- 149.5855325460434
- 155.1363314986229
- 144.7684994339943
- 157.17235600948334
- 148.2242933511734
- 150.813542842865
- 150.94212138652802
- 180.53135865926743
- 147.43998849391937
- 172.00515514612198
- 155.4583444595337
- 135.63325208425522
- 152.70560270547867
- 138.14679676294327
- 174.8872833251953
- 140.3090153336525
- 138.64796793460846
- 139.78308522701263
- 164.43294501304626
- 137.859510242939
- 145.45206546783447
- 147.94153887033463
- 159.8718032836914
- 120.55472481250763
- 148.52943909168243
- 157.8361275792122
- 171.37452006340027
- 145.6818283200264
- 170.80825352668762
- 194.40543949604034
- 128.19690865278244
- 141.60889089107513
- 143.99453336000443
- 132.89697670936584
- 154.79427713155746
- 146.24003678560257
- 131.03852289915085
- 150.39410096406937
- 142.66102403402328
- 150.386095225811
- 124.5556789636612
- 149.6566396355629
- 143.1434969305992
- 134.53539371490479
- 140.05014419555664
- 149.0407399535179
- 153.59810811281204
- 152.12365889549255
train_accuracy:
- 0.0
- 0.52
- 0.0
- 0.247
- 0.467
- 0.308
- 0.14
- 0.85
- 0.48
- 0.435
- 0.883
- 0.311
- 0.478
- 0.45
- 0.225
- 0.9
- 0.108
- 0.8
- 0.99
- 0.675
- 0.25
- 0.015
- 0.5
- 0.485
- 0.843
- 0.915
- 0.893
- 0.045
- 0.321
- 0.517
- 0.872
- 0.833
- 0.342
- 0.7
- 0.444
- 0.814
- 0.908
- 0.73
- 0.23
- 0.537
- 0.55
- 0.926
- 0.262
- 0.619
- 0.377
- 0.425
- 0.84
- 0.936
- 0.725
- 0.719
- 0.841
- 0.745
- 0.175
- 0.8
- 0.0
- 0.956
- 0.935
- 1.0
- 0.417
- 0.4
- 0.361
- 0.453
- 0.781
- 0.517
- 0.883
- 0.13
- 0.989
- 0.608
- 0.368
- 0.789
- 0.95
- 0.062
- 0.764
- 0.408
- 0.92
- 0.325
- 0.658
- 0.938
- 0.95
- 0.85
- 0.72
- 0.55
- 0.43
- 0.264
- 0.683
- 0.833
- 0.817
- 0.494
- 0.405
- 0.625
- 0.475
- 0.873
- 0.106
- 0.641
- 0.638
- 0.84
- 0.627
- 0.955
- 0.883
- 0.246
train_loss:
- 1.172
- 0.743
- 0.617
- 0.612
- 0.536
- 0.547
- 0.43
- 0.47
- 0.546
- 0.423
- 0.401
- 0.387
- 0.464
- 0.439
- 0.352
- 0.338
- 0.373
- 0.352
- 0.381
- 0.424
- 0.366
- 0.46
- 0.401
- 0.339
- 0.353
- 0.319
- 0.371
- 0.403
- 0.3
- 0.418
- 0.393
- 0.438
- 0.371
- 0.306
- 0.402
- 0.325
- 0.362
- 0.365
- 0.35
- 0.359
- 0.316
- 0.343
- 0.317
- 0.362
- 0.363
- 0.341
- 0.299
- 0.378
- 0.278
- 0.311
- 0.311
- 0.408
- 0.352
- 0.333
- 0.35
- 0.352
- 0.421
- 0.321
- 0.392
- 0.339
- 0.384
- 0.31
- 0.281
- 0.322
- 0.302
- 0.311
- 0.333
- 0.282
- 0.334
- 0.244
- 0.317
- 0.332
- 0.348
- 0.317
- 0.298
- 0.292
- 0.369
- 0.316
- 0.273
- 0.265
- 0.341
- 0.28
- 0.304
- 0.363
- 0.385
- 0.318
- 0.273
- 0.31
- 0.35
- 0.266
- 0.279
- 0.276
- 0.275
- 0.258
- 0.284
- 0.3
- 0.327
- 0.3
- 0.313
- 0.317
unequal: 1
verbose: 1
