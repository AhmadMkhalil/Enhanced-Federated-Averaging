avg_train_accuracy: 0.925
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1624468085106383
- 0.23148936170212767
- 0.35010638297872343
- 0.3347872340425532
- 0.3663297872340426
- 0.3575
- 0.4915425531914894
- 0.5026595744680851
- 0.520904255319149
- 0.540372340425532
- 0.5519148936170213
- 0.5811702127659575
- 0.5393085106382979
- 0.5355851063829787
- 0.593404255319149
- 0.5754255319148937
- 0.5776063829787234
- 0.5750531914893617
- 0.6052659574468086
- 0.6063829787234043
- 0.5811170212765957
- 0.5930851063829787
- 0.6093617021276596
- 0.6071808510638298
- 0.5997340425531915
- 0.5996808510638297
- 0.5886702127659574
- 0.6101063829787234
- 0.6202659574468085
- 0.6262765957446809
- 0.6394148936170213
- 0.6193617021276596
- 0.645
- 0.594095744680851
- 0.6834574468085106
- 0.6337234042553191
- 0.6133510638297872
- 0.6542021276595744
- 0.651436170212766
- 0.678936170212766
- 0.6234042553191489
- 0.6345744680851064
- 0.5470744680851064
- 0.6390957446808511
- 0.6054255319148936
- 0.6427659574468085
- 0.6507978723404255
- 0.6955851063829788
- 0.6350531914893617
- 0.6774468085106383
- 0.6542021276595744
- 0.6753191489361702
- 0.589627659574468
- 0.6422340425531915
- 0.6448404255319149
- 0.6012765957446808
- 0.5973936170212766
- 0.6454255319148936
- 0.6495744680851064
- 0.6627659574468086
- 0.7079255319148936
- 0.6823936170212765
- 0.6681914893617021
- 0.6601595744680852
- 0.6129787234042553
- 0.6455851063829787
- 0.6502659574468085
- 0.6497340425531914
- 0.6621276595744681
- 0.6921276595744681
- 0.625904255319149
- 0.7135106382978723
- 0.6648936170212766
- 0.69
- 0.6273936170212766
- 0.645904255319149
- 0.6727127659574468
- 0.6446276595744681
- 0.6647340425531915
- 0.699627659574468
- 0.6876063829787235
- 0.6688297872340425
- 0.6965425531914894
- 0.6656914893617021
- 0.6568617021276596
- 0.6715425531914894
- 0.6426063829787234
- 0.6504255319148936
- 0.6747872340425531
- 0.7152127659574468
- 0.7012234042553191
- 0.6799468085106383
- 0.6789893617021276
- 0.6766489361702127
- 0.7063829787234043
- 0.6613297872340426
- 0.6811170212765958
- 0.6764361702127659
- 0.6759574468085107
- 0.693031914893617
test_loss_list:
- 524.0398688316345
- 459.57711386680603
- 406.3359441757202
- 357.0160472393036
- 343.617902636528
- 326.2580728530884
- 243.261167883873
- 242.53596663475037
- 229.3209902048111
- 214.0733585357666
- 205.7861557006836
- 194.98238706588745
- 194.79935240745544
- 204.95981752872467
- 185.42451131343842
- 179.69701325893402
- 191.63239115476608
- 180.82853561639786
- 168.5321220755577
- 173.59956938028336
- 184.58895707130432
- 189.864402115345
- 164.02585232257843
- 177.4632347226143
- 174.3759760260582
- 167.50014740228653
- 183.3576255440712
- 181.645558655262
- 173.10353887081146
- 160.12422496080399
- 155.56148248910904
- 170.18712985515594
- 153.64025509357452
- 167.5271499156952
- 138.16418761014938
- 154.56536442041397
- 157.2441359758377
- 160.05798614025116
- 140.18841743469238
- 128.61674416065216
- 154.88780957460403
- 149.1520020365715
- 200.37191873788834
- 148.97264182567596
- 165.88384360074997
- 150.2802442908287
- 136.68418282270432
- 136.47127842903137
- 168.85811591148376
- 136.65842020511627
- 155.2831762433052
- 150.6565784215927
- 192.2681815624237
- 148.9145097732544
- 162.75417840480804
- 184.7900362610817
- 165.4541176557541
- 146.67782962322235
- 144.86807084083557
- 145.76618778705597
- 124.02020996809006
- 132.7442820072174
- 143.36457931995392
- 143.78460919857025
- 167.38710409402847
- 152.3939931988716
- 156.0997959971428
- 165.06179052591324
- 160.38988494873047
- 140.34016871452332
- 153.87414282560349
- 121.8033384680748
- 141.89143651723862
- 134.18291419744492
- 165.16319793462753
- 146.83821594715118
- 135.95812219381332
- 146.40511602163315
- 142.30229341983795
- 122.07381570339203
- 132.00907784700394
- 140.09688168764114
- 129.9847553372383
- 155.8000729084015
- 151.16602039337158
- 142.53534603118896
- 147.81546634435654
- 148.59079164266586
- 138.68083655834198
- 122.81724542379379
- 131.77152699232101
- 138.41128826141357
- 145.71689188480377
- 143.9616198539734
- 122.14543777704239
- 158.55655282735825
- 136.10779285430908
- 140.29608112573624
- 137.8279665708542
- 127.13814210891724
train_accuracy:
- 0.79
- 0.0
- 0.0
- 0.328
- 0.0
- 0.129
- 0.308
- 0.16
- 0.544
- 0.2
- 0.593
- 0.48
- 0.505
- 0.89
- 0.425
- 0.236
- 0.786
- 0.044
- 0.429
- 0.225
- 0.75
- 0.555
- 0.535
- 0.593
- 0.547
- 0.9
- 0.833
- 0.15
- 0.767
- 0.54
- 0.854
- 0.61
- 0.9
- 0.967
- 0.867
- 0.612
- 0.408
- 0.918
- 0.967
- 0.863
- 0.621
- 0.309
- 0.611
- 0.817
- 0.4
- 0.86
- 0.855
- 0.432
- 0.229
- 0.958
- 0.881
- 0.777
- 0.833
- 0.647
- 0.14
- 0.039
- 0.813
- 0.94
- 0.764
- 0.542
- 0.728
- 0.007
- 0.736
- 0.714
- 0.383
- 0.955
- 0.327
- 0.933
- 0.828
- 0.815
- 0.9
- 0.872
- 0.883
- 0.911
- 0.15
- 0.816
- 0.535
- 0.11
- 0.687
- 0.938
- 0.906
- 0.912
- 0.617
- 0.9
- 0.357
- 0.92
- 0.503
- 0.814
- 0.76
- 0.825
- 0.865
- 0.681
- 0.819
- 0.942
- 0.75
- 0.87
- 0.495
- 0.643
- 0.854
- 0.925
train_loss:
- 1.067
- 0.78
- 0.635
- 0.772
- 0.481
- 0.71
- 0.547
- 0.473
- 0.526
- 0.417
- 0.445
- 0.424
- 0.462
- 0.468
- 0.506
- 0.333
- 0.402
- 0.479
- 0.402
- 0.385
- 0.458
- 0.452
- 0.362
- 0.388
- 0.407
- 0.427
- 0.457
- 0.351
- 0.404
- 0.348
- 0.34
- 0.358
- 0.34
- 0.385
- 0.337
- 0.466
- 0.358
- 0.404
- 0.389
- 0.42
- 0.368
- 0.358
- 0.359
- 0.381
- 0.391
- 0.421
- 0.347
- 0.459
- 0.371
- 0.374
- 0.399
- 0.356
- 0.308
- 0.419
- 0.344
- 0.32
- 0.308
- 0.429
- 0.34
- 0.324
- 0.342
- 0.323
- 0.297
- 0.314
- 0.408
- 0.287
- 0.361
- 0.296
- 0.351
- 0.355
- 0.323
- 0.315
- 0.282
- 0.39
- 0.334
- 0.384
- 0.361
- 0.35
- 0.415
- 0.345
- 0.321
- 0.3
- 0.366
- 0.303
- 0.339
- 0.376
- 0.274
- 0.287
- 0.312
- 0.318
- 0.37
- 0.411
- 0.326
- 0.38
- 0.427
- 0.322
- 0.403
- 0.352
- 0.424
- 0.311
unequal: 1
verbose: 1
