avg_train_accuracy: 0.7
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.16345744680851063
- 0.25042553191489364
- 0.3153191489361702
- 0.2743617021276596
- 0.4307446808510638
- 0.4500531914893617
- 0.4486702127659574
- 0.4854255319148936
- 0.46345744680851064
- 0.5222340425531915
- 0.5782446808510638
- 0.5070744680851064
- 0.5101063829787233
- 0.4980851063829787
- 0.5489893617021276
- 0.6002127659574468
- 0.5909574468085106
- 0.5794680851063829
- 0.6339361702127659
- 0.6162234042553192
- 0.6057978723404255
- 0.5151595744680851
- 0.5201595744680851
- 0.5970744680851063
- 0.5656914893617021
- 0.6098404255319149
- 0.5422340425531915
- 0.5752659574468085
- 0.5735106382978723
- 0.6587765957446808
- 0.6077127659574468
- 0.6286702127659575
- 0.5648936170212766
- 0.6085106382978723
- 0.594095744680851
- 0.6511170212765958
- 0.6056382978723405
- 0.6464893617021277
- 0.6395212765957446
- 0.6396808510638298
- 0.6212234042553192
- 0.6278191489361702
- 0.6480851063829787
- 0.6502659574468085
- 0.6057978723404255
- 0.5834574468085106
- 0.5980851063829787
- 0.6538297872340425
- 0.5703191489361702
- 0.6257978723404255
- 0.604468085106383
- 0.6904255319148936
- 0.6265425531914893
- 0.6996808510638298
- 0.6730851063829787
- 0.6526063829787234
- 0.6392021276595745
- 0.6639893617021276
- 0.6517021276595745
- 0.6453723404255319
- 0.666968085106383
- 0.658936170212766
- 0.5581914893617022
- 0.6032446808510639
- 0.6537765957446808
- 0.7071276595744681
- 0.6567021276595745
- 0.7107978723404256
- 0.66
- 0.6225
- 0.6177659574468085
- 0.6873404255319149
- 0.6198404255319149
- 0.6161170212765957
- 0.6114893617021276
- 0.6301063829787235
- 0.6156914893617021
- 0.6927659574468085
- 0.656436170212766
- 0.6837765957446809
- 0.6568617021276596
- 0.670372340425532
- 0.6801595744680851
- 0.685372340425532
- 0.6957978723404256
- 0.6776063829787234
- 0.6648936170212766
- 0.6527659574468085
- 0.6497340425531914
- 0.6720744680851064
- 0.66
- 0.6234042553191489
- 0.6257446808510638
- 0.5855851063829787
- 0.6582978723404256
- 0.6791489361702128
- 0.7041489361702128
- 0.6534574468085106
- 0.6584042553191489
- 0.6921808510638298
test_loss_list:
- 528.234548330307
- 426.427218914032
- 384.43768191337585
- 369.98070001602173
- 280.0434818267822
- 258.782860994339
- 251.44163918495178
- 248.4401571750641
- 241.369610786438
- 219.06806778907776
- 184.27188789844513
- 207.10075426101685
- 210.3617650270462
- 233.19987499713898
- 199.97901105880737
- 183.4133871793747
- 174.99082392454147
- 188.96559298038483
- 177.13107818365097
- 164.3258649110794
- 171.09277296066284
- 203.68986749649048
- 219.18943238258362
- 177.87248021364212
- 181.04001569747925
- 170.53699499368668
- 194.88029193878174
- 172.41273480653763
- 173.1306124329567
- 142.8204185962677
- 154.32622104883194
- 160.67521399259567
- 198.5575020313263
- 173.3208331465721
- 170.37704783678055
- 148.09443420171738
- 169.58168125152588
- 159.87741416692734
- 151.72211337089539
- 150.20695167779922
- 168.14546114206314
- 156.2794480919838
- 154.48160362243652
- 160.9364446401596
- 170.61782091856003
- 171.17247623205185
- 190.9963760972023
- 153.88405054807663
- 178.33236753940582
- 168.84070456027985
- 160.04439240694046
- 138.97539347410202
- 151.67836499214172
- 128.1080597639084
- 129.60388773679733
- 139.71827828884125
- 155.02708035707474
- 142.15808498859406
- 146.1808809041977
- 154.42488700151443
- 129.4339896440506
- 134.77875244617462
- 177.1154447197914
- 155.14507001638412
- 139.32101464271545
- 119.36848396062851
- 141.22668939828873
- 122.94427478313446
- 146.92033326625824
- 176.8372642993927
- 186.51737922430038
- 139.6244882941246
- 152.65518736839294
- 170.66180896759033
- 145.8970844745636
- 156.93649780750275
- 155.73579812049866
- 136.01972490549088
- 139.99833649396896
- 125.95108157396317
- 145.86322951316833
- 142.42593103647232
- 139.03169548511505
- 135.9476916193962
- 123.61175274848938
- 128.68118339776993
- 146.48165845870972
- 151.5768912434578
- 158.30228132009506
- 144.40551006793976
- 149.00967925786972
- 175.03151363134384
- 153.7258853316307
- 169.11316812038422
- 147.24816471338272
- 140.54534494876862
- 127.2491894364357
- 149.84130132198334
- 135.64520835876465
- 138.99586075544357
train_accuracy:
- 0.0
- 0.0
- 0.475
- 0.206
- 0.613
- 0.758
- 0.914
- 0.675
- 0.971
- 0.65
- 0.417
- 0.819
- 0.611
- 0.479
- 0.932
- 0.526
- 0.525
- 0.855
- 0.783
- 0.413
- 0.886
- 0.344
- 0.264
- 0.725
- 0.3
- 0.929
- 0.656
- 0.267
- 0.7
- 0.45
- 0.239
- 0.718
- 0.815
- 0.526
- 0.706
- 0.868
- 0.762
- 0.945
- 0.912
- 0.878
- 0.158
- 0.508
- 0.607
- 0.82
- 0.306
- 0.69
- 0.417
- 0.839
- 0.75
- 0.47
- 0.08
- 0.75
- 0.4
- 0.167
- 0.886
- 0.807
- 0.613
- 0.78
- 0.728
- 0.79
- 0.933
- 0.856
- 0.428
- 0.541
- 0.942
- 0.682
- 0.917
- 0.796
- 0.194
- 0.886
- 0.46
- 0.742
- 0.712
- 0.671
- 0.963
- 0.885
- 0.218
- 0.624
- 0.872
- 0.93
- 0.012
- 0.631
- 0.914
- 0.794
- 0.473
- 0.841
- 0.888
- 0.993
- 0.706
- 0.883
- 0.8
- 0.739
- 0.793
- 0.585
- 0.921
- 0.791
- 0.775
- 0.264
- 0.2
- 0.7
train_loss:
- 1.199
- 0.749
- 0.602
- 0.622
- 0.564
- 0.537
- 0.497
- 0.443
- 0.432
- 0.546
- 0.418
- 0.444
- 0.439
- 0.422
- 0.455
- 0.521
- 0.365
- 0.4
- 0.442
- 0.39
- 0.355
- 0.291
- 0.337
- 0.273
- 0.41
- 0.329
- 0.315
- 0.353
- 0.33
- 0.424
- 0.377
- 0.325
- 0.271
- 0.387
- 0.332
- 0.388
- 0.357
- 0.33
- 0.301
- 0.332
- 0.271
- 0.364
- 0.282
- 0.27
- 0.299
- 0.337
- 0.349
- 0.27
- 0.28
- 0.306
- 0.286
- 0.331
- 0.408
- 0.357
- 0.428
- 0.31
- 0.411
- 0.326
- 0.271
- 0.323
- 0.354
- 0.268
- 0.296
- 0.279
- 0.318
- 0.367
- 0.279
- 0.352
- 0.337
- 0.296
- 0.261
- 0.332
- 0.352
- 0.286
- 0.287
- 0.321
- 0.314
- 0.31
- 0.311
- 0.302
- 0.291
- 0.33
- 0.241
- 0.291
- 0.338
- 0.331
- 0.259
- 0.213
- 0.329
- 0.28
- 0.334
- 0.309
- 0.379
- 0.385
- 0.264
- 0.349
- 0.304
- 0.325
- 0.326
- 0.306
unequal: 1
verbose: 1
