avg_train_accuracy: 0.379
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.14053191489361702
- 0.20186170212765958
- 0.3475
- 0.3593085106382979
- 0.3473936170212766
- 0.38515957446808513
- 0.5168617021276596
- 0.4845212765957447
- 0.44627659574468087
- 0.4233510638297872
- 0.5278191489361702
- 0.4837234042553191
- 0.5695212765957447
- 0.5432446808510638
- 0.5406914893617021
- 0.623563829787234
- 0.6033510638297872
- 0.5134574468085107
- 0.5287234042553192
- 0.581968085106383
- 0.6254787234042554
- 0.5599468085106383
- 0.6160638297872341
- 0.5673936170212766
- 0.5851595744680851
- 0.6192553191489362
- 0.5853723404255319
- 0.6380851063829788
- 0.5918085106382979
- 0.5648404255319149
- 0.5569148936170213
- 0.5837234042553191
- 0.6744148936170212
- 0.6588829787234043
- 0.6202127659574468
- 0.613563829787234
- 0.6390425531914894
- 0.6434042553191489
- 0.6332446808510638
- 0.6594148936170213
- 0.5865425531914894
- 0.6167553191489362
- 0.6355851063829787
- 0.6427659574468085
- 0.6370744680851064
- 0.6496808510638298
- 0.5942021276595745
- 0.6270212765957447
- 0.6196276595744681
- 0.580531914893617
- 0.6504787234042553
- 0.6052127659574468
- 0.6679255319148936
- 0.6542021276595744
- 0.6428723404255319
- 0.6772340425531915
- 0.6773936170212767
- 0.6607446808510639
- 0.6732978723404255
- 0.5906382978723405
- 0.6488829787234043
- 0.6738297872340425
- 0.6225
- 0.6753191489361702
- 0.6630851063829787
- 0.6447872340425532
- 0.6673936170212766
- 0.7062234042553192
- 0.6157978723404255
- 0.6953723404255319
- 0.6777127659574468
- 0.5845744680851064
- 0.671968085106383
- 0.6189893617021277
- 0.671063829787234
- 0.6995212765957447
- 0.7021808510638298
- 0.6527659574468085
- 0.6949468085106383
- 0.6518617021276596
- 0.6747872340425531
- 0.6711170212765958
- 0.6714893617021277
- 0.6445212765957447
- 0.6396276595744681
- 0.6223404255319149
- 0.6927127659574468
- 0.6811170212765958
- 0.6686170212765957
- 0.6572872340425532
- 0.6850531914893617
- 0.6326063829787234
- 0.6377659574468085
- 0.6625
- 0.6735106382978724
- 0.6837765957446809
- 0.6936702127659574
- 0.6620212765957447
- 0.6862234042553191
- 0.6970212765957446
test_loss_list:
- 536.3168532848358
- 453.1691529750824
- 360.8506832122803
- 329.2581213712692
- 320.18398916721344
- 306.66795575618744
- 243.7062178850174
- 255.8345466852188
- 268.1490582227707
- 282.68467950820923
- 220.10451078414917
- 248.9012942314148
- 207.38008832931519
- 201.01084983348846
- 208.9713728427887
- 171.09678715467453
- 174.6751807332039
- 222.69975590705872
- 248.3481023311615
- 200.75651264190674
- 178.99372726678848
- 191.3984249830246
- 162.74450427293777
- 188.13486343622208
- 182.07232481241226
- 168.99004316329956
- 176.75738912820816
- 167.26721853017807
- 175.50539940595627
- 183.5297988653183
- 214.96278977394104
- 187.10168033838272
- 153.08578372001648
- 148.01971703767776
- 172.89401125907898
- 170.78439074754715
- 149.3855458498001
- 158.64326751232147
- 155.30390572547913
- 142.51775753498077
- 169.13598573207855
- 158.64433640241623
- 164.4742906689644
- 151.45329701900482
- 150.12536680698395
- 149.02116066217422
- 171.18763273954391
- 151.77856194972992
- 174.91017597913742
- 187.9127231836319
- 147.81864941120148
- 160.16422069072723
- 154.02106380462646
- 142.37360733747482
- 156.01178294420242
- 138.8153464794159
- 130.01096558570862
- 142.97435706853867
- 135.8540707230568
- 171.85473024845123
- 141.07473081350327
- 145.76120990514755
- 166.04581934213638
- 144.64256542921066
- 133.19398081302643
- 147.0547577738762
- 137.71585458517075
- 122.4453586935997
- 167.5446949005127
- 142.68824076652527
- 140.9550143480301
- 171.3606903553009
- 136.40394431352615
- 160.59032344818115
- 132.25701647996902
- 121.06951302289963
- 127.48363447189331
- 148.12485086917877
- 147.48180639743805
- 157.827638566494
- 131.77219951152802
- 142.644227206707
- 142.5494796037674
- 150.37736254930496
- 149.2964465022087
- 160.65568923950195
- 129.38552302122116
- 137.21145886182785
- 134.97395658493042
- 143.6089921593666
- 132.0589098930359
- 165.92120307683945
- 153.86365658044815
- 157.24199670553207
- 129.71831595897675
- 132.0739944577217
- 138.1173570752144
- 138.00942248106003
- 134.15303200483322
- 146.9555967450142
train_accuracy:
- 0.0
- 0.04
- 0.543
- 0.944
- 0.01
- 0.15
- 0.062
- 0.872
- 0.858
- 0.335
- 0.821
- 0.567
- 0.891
- 0.365
- 0.643
- 0.405
- 0.612
- 0.467
- 0.7
- 0.75
- 0.572
- 1.0
- 0.0
- 0.728
- 0.73
- 0.736
- 0.612
- 0.514
- 0.498
- 0.925
- 0.707
- 0.553
- 0.137
- 0.913
- 0.371
- 0.955
- 0.523
- 0.839
- 0.719
- 0.65
- 0.783
- 0.782
- 0.757
- 0.707
- 0.659
- 0.771
- 0.309
- 0.17
- 0.688
- 0.625
- 0.518
- 0.942
- 0.638
- 0.175
- 0.853
- 0.505
- 0.829
- 0.78
- 0.636
- 0.14
- 0.659
- 0.779
- 0.54
- 0.757
- 0.9
- 0.622
- 0.53
- 0.425
- 0.562
- 0.435
- 0.4
- 0.729
- 0.871
- 0.814
- 0.719
- 0.957
- 0.75
- 0.636
- 0.816
- 0.63
- 0.864
- 0.571
- 0.7
- 0.617
- 0.631
- 0.675
- 0.539
- 0.543
- 1.0
- 0.333
- 0.836
- 0.0
- 0.568
- 0.282
- 0.925
- 0.306
- 0.822
- 0.714
- 0.886
- 0.379
train_loss:
- 1.169
- 0.759
- 0.579
- 0.454
- 0.581
- 0.506
- 0.505
- 0.359
- 0.444
- 0.423
- 0.497
- 0.416
- 0.337
- 0.455
- 0.328
- 0.428
- 0.427
- 0.303
- 0.327
- 0.381
- 0.365
- 0.275
- 0.364
- 0.385
- 0.34
- 0.451
- 0.4
- 0.386
- 0.395
- 0.303
- 0.363
- 0.337
- 0.394
- 0.409
- 0.379
- 0.326
- 0.396
- 0.351
- 0.29
- 0.347
- 0.303
- 0.346
- 0.29
- 0.339
- 0.349
- 0.311
- 0.375
- 0.398
- 0.344
- 0.363
- 0.403
- 0.31
- 0.339
- 0.38
- 0.316
- 0.44
- 0.352
- 0.273
- 0.282
- 0.325
- 0.309
- 0.403
- 0.257
- 0.359
- 0.375
- 0.38
- 0.371
- 0.246
- 0.299
- 0.375
- 0.302
- 0.31
- 0.474
- 0.369
- 0.278
- 0.264
- 0.398
- 0.332
- 0.306
- 0.313
- 0.267
- 0.318
- 0.34
- 0.311
- 0.361
- 0.29
- 0.357
- 0.299
- 0.314
- 0.328
- 0.306
- 0.264
- 0.349
- 0.315
- 0.395
- 0.29
- 0.344
- 0.305
- 0.315
- 0.266
unequal: 1
verbose: 1
