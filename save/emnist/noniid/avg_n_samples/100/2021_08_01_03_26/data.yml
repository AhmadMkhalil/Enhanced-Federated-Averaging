avg_train_accuracy: 0.818
avg_train_loss: 0.003
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.17718085106382978
- 0.2102659574468085
- 0.33117021276595743
- 0.40382978723404256
- 0.4492553191489362
- 0.5017021276595744
- 0.4958510638297872
- 0.49138297872340425
- 0.46180851063829786
- 0.4873936170212766
- 0.5011702127659574
- 0.5311170212765958
- 0.5538829787234043
- 0.543031914893617
- 0.4792021276595745
- 0.5390957446808511
- 0.5480851063829787
- 0.5352659574468085
- 0.5290425531914894
- 0.6161702127659574
- 0.6004787234042553
- 0.6200531914893617
- 0.5975
- 0.6389361702127659
- 0.5676063829787235
- 0.5239893617021276
- 0.5906382978723405
- 0.5824468085106383
- 0.6056382978723405
- 0.6020744680851063
- 0.5836702127659574
- 0.6742021276595744
- 0.5696808510638298
- 0.6148936170212767
- 0.6196808510638298
- 0.5973936170212766
- 0.5891489361702128
- 0.5964893617021276
- 0.6237765957446808
- 0.6081382978723404
- 0.6113297872340425
- 0.612872340425532
- 0.6418085106382979
- 0.6472340425531915
- 0.6448404255319149
- 0.6337234042553191
- 0.6568085106382979
- 0.6770212765957446
- 0.6367553191489361
- 0.6518085106382979
- 0.6859042553191489
- 0.7089893617021277
- 0.6771276595744681
- 0.6782446808510638
- 0.6186702127659575
- 0.6153723404255319
- 0.6471808510638298
- 0.7025
- 0.6211702127659574
- 0.6261702127659574
- 0.6823936170212765
- 0.6527659574468085
- 0.653563829787234
- 0.6096276595744681
- 0.6063829787234043
- 0.5987234042553191
- 0.6584042553191489
- 0.6229787234042553
- 0.6157978723404255
- 0.6553723404255319
- 0.6508510638297872
- 0.6493617021276595
- 0.611968085106383
- 0.5942021276595745
- 0.6625
- 0.6460638297872341
- 0.6362234042553192
- 0.6387765957446808
- 0.6544148936170213
- 0.6845744680851064
- 0.6976063829787233
- 0.6728723404255319
- 0.6622340425531915
- 0.6832978723404255
- 0.665904255319149
- 0.6744148936170212
- 0.6895212765957447
- 0.6496808510638298
- 0.6163297872340425
- 0.6412765957446809
- 0.5976063829787234
- 0.6833510638297873
- 0.6543085106382979
- 0.6678191489361702
- 0.7009042553191489
- 0.6469148936170213
- 0.7168085106382979
- 0.6718085106382978
- 0.6617021276595745
- 0.728563829787234
test_loss_list:
- 529.7434394359589
- 453.4236741065979
- 390.8868341445923
- 323.2408723831177
- 282.8337792158127
- 258.5105813741684
- 252.48389899730682
- 277.4676320552826
- 260.3891410827637
- 243.1681751012802
- 259.673566699028
- 213.34484553337097
- 203.567324757576
- 210.44752144813538
- 240.9186955690384
- 205.41441535949707
- 211.62003540992737
- 204.01409316062927
- 209.60497510433197
- 168.87015932798386
- 170.4257716536522
- 160.23252075910568
- 170.32496654987335
- 160.6452100276947
- 213.27596950531006
- 224.61212646961212
- 201.49663531780243
- 173.61856889724731
- 172.3589386343956
- 180.61361169815063
- 171.06318992376328
- 139.74317795038223
- 194.6654133796692
- 164.6603929400444
- 158.1487459540367
- 163.53454303741455
- 180.21361178159714
- 176.435708463192
- 161.19632411003113
- 155.9255729317665
- 175.44698184728622
- 179.9561863541603
- 166.27131366729736
- 155.47831565141678
- 147.79479730129242
- 167.1800012588501
- 148.56243234872818
- 140.04880279302597
- 148.33428996801376
- 145.85600590705872
- 139.43363851308823
- 128.91407668590546
- 149.2873038649559
- 141.0470616221428
- 153.1445146203041
- 158.69235104322433
- 142.78688871860504
- 128.3072754740715
- 163.33965027332306
- 155.7596389055252
- 136.415589094162
- 148.32002902030945
- 158.62682741880417
- 183.8728643655777
- 180.14097940921783
- 182.16967701911926
- 152.47353208065033
- 161.85989904403687
- 179.53814709186554
- 152.75590592622757
- 142.70926839113235
- 145.32615941762924
- 195.89069837331772
- 157.8345029950142
- 141.73446208238602
- 146.45847219228745
- 143.5766585469246
- 136.06540954113007
- 151.41733115911484
- 135.74961465597153
- 128.60982310771942
- 141.09517139196396
- 152.51579529047012
- 124.06439411640167
- 140.98318785429
- 134.58137154579163
- 135.83023631572723
- 147.74994575977325
- 161.32546544075012
- 153.8528863787651
- 180.48857128620148
- 133.08706724643707
- 143.9578605890274
- 136.20455849170685
- 130.35863208770752
- 142.90814530849457
- 119.08653438091278
- 134.11001259088516
- 132.3890386223793
- 112.40803807973862
train_accuracy:
- 0.383
- 0.25
- 0.0
- 0.272
- 0.575
- 0.388
- 0.15
- 0.912
- 0.494
- 0.45
- 0.446
- 0.479
- 0.489
- 0.521
- 0.56
- 0.9
- 0.55
- 0.173
- 0.321
- 0.35
- 0.582
- 0.967
- 0.641
- 0.833
- 0.478
- 0.146
- 0.612
- 0.858
- 0.608
- 0.378
- 0.87
- 0.825
- 0.4
- 0.286
- 0.955
- 0.447
- 0.562
- 0.711
- 0.932
- 0.5
- 0.0
- 0.77
- 0.3
- 0.583
- 0.544
- 0.532
- 0.844
- 0.442
- 0.668
- 0.893
- 0.471
- 0.604
- 0.791
- 0.3
- 0.481
- 0.947
- 0.733
- 0.783
- 0.664
- 0.504
- 0.461
- 0.65
- 0.694
- 0.03
- 0.667
- 0.878
- 0.808
- 0.61
- 0.622
- 0.914
- 0.28
- 0.709
- 0.017
- 0.706
- 0.891
- 0.371
- 0.725
- 0.892
- 0.917
- 0.865
- 0.575
- 0.665
- 0.529
- 0.522
- 0.027
- 0.46
- 0.676
- 0.55
- 0.6
- 0.775
- 0.571
- 0.519
- 0.855
- 0.686
- 0.819
- 0.711
- 0.533
- 0.011
- 0.415
- 0.818
train_loss:
- 1.191
- 0.848
- 0.606
- 0.642
- 0.538
- 0.461
- 0.477
- 0.395
- 0.491
- 0.443
- 0.445
- 0.543
- 0.479
- 0.497
- 0.413
- 0.399
- 0.381
- 0.446
- 0.388
- 0.397
- 0.469
- 0.341
- 0.338
- 0.391
- 0.347
- 0.371
- 0.393
- 0.466
- 0.435
- 0.386
- 0.393
- 0.367
- 0.335
- 0.346
- 0.435
- 0.392
- 0.325
- 0.385
- 0.397
- 0.336
- 0.304
- 0.383
- 0.381
- 0.331
- 0.333
- 0.415
- 0.357
- 0.284
- 0.406
- 0.338
- 0.37
- 0.319
- 0.291
- 0.359
- 0.383
- 0.4
- 0.285
- 0.353
- 0.349
- 0.362
- 0.301
- 0.3
- 0.35
- 0.253
- 0.37
- 0.34
- 0.371
- 0.31
- 0.355
- 0.406
- 0.358
- 0.366
- 0.368
- 0.415
- 0.292
- 0.332
- 0.323
- 0.323
- 0.319
- 0.293
- 0.311
- 0.357
- 0.354
- 0.321
- 0.297
- 0.315
- 0.285
- 0.322
- 0.325
- 0.31
- 0.304
- 0.323
- 0.356
- 0.307
- 0.33
- 0.382
- 0.34
- 0.298
- 0.363
- 0.294
unequal: 1
verbose: 1
