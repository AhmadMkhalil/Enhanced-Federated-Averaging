avg_train_accuracy: 0.607
avg_train_loss: 0.004
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.17191489361702128
- 0.2098936170212766
- 0.333031914893617
- 0.32138297872340427
- 0.41425531914893615
- 0.41590425531914893
- 0.47952127659574467
- 0.5144148936170213
- 0.454468085106383
- 0.5255851063829787
- 0.5393085106382979
- 0.531063829787234
- 0.5181914893617021
- 0.541063829787234
- 0.5111170212765958
- 0.5681914893617022
- 0.5820212765957447
- 0.6078191489361702
- 0.5894148936170213
- 0.5939893617021277
- 0.5829255319148936
- 0.6253723404255319
- 0.6122340425531915
- 0.5853723404255319
- 0.5419148936170213
- 0.5774468085106383
- 0.6276063829787234
- 0.6657446808510639
- 0.586436170212766
- 0.6604787234042553
- 0.6353723404255319
- 0.6395212765957446
- 0.5773404255319149
- 0.5930851063829787
- 0.6785638297872341
- 0.6584574468085106
- 0.6459574468085106
- 0.6200531914893617
- 0.6691489361702128
- 0.6728191489361702
- 0.6522340425531915
- 0.5879787234042553
- 0.6315957446808511
- 0.6130851063829788
- 0.6170212765957447
- 0.6557978723404255
- 0.6762765957446808
- 0.6298936170212766
- 0.6598404255319149
- 0.6617021276595745
- 0.6342021276595745
- 0.5826595744680851
- 0.6775
- 0.5572872340425532
- 0.646595744680851
- 0.5939893617021277
- 0.6472872340425532
- 0.636436170212766
- 0.6257446808510638
- 0.6247340425531915
- 0.6526063829787234
- 0.6426595744680851
- 0.6311170212765957
- 0.7121276595744681
- 0.6573936170212766
- 0.6906382978723404
- 0.6815425531914894
- 0.6688297872340425
- 0.6547340425531915
- 0.6827127659574468
- 0.6771276595744681
- 0.6822340425531915
- 0.6639893617021276
- 0.6142553191489362
- 0.6282978723404256
- 0.6251595744680851
- 0.6445212765957447
- 0.663936170212766
- 0.6408510638297872
- 0.6723936170212766
- 0.688031914893617
- 0.6423404255319148
- 0.6594148936170213
- 0.6664361702127659
- 0.6529255319148937
- 0.7086702127659574
- 0.6675531914893617
- 0.6815425531914894
- 0.6317553191489361
- 0.6110106382978724
- 0.6717021276595745
- 0.6590425531914894
- 0.6890957446808511
- 0.6628723404255319
- 0.6443617021276595
- 0.6839893617021277
- 0.6773404255319149
- 0.680531914893617
- 0.6821276595744681
- 0.6577659574468085
test_loss_list:
- 532.843291759491
- 463.0012390613556
- 388.64182114601135
- 353.665358543396
- 294.6250013113022
- 295.8517575263977
- 251.77010679244995
- 233.03509509563446
- 247.60052752494812
- 218.68162834644318
- 220.43275487422943
- 220.21679043769836
- 208.1344348192215
- 197.90038347244263
- 210.30049645900726
- 198.45874273777008
- 197.38015234470367
- 173.6717649102211
- 172.83799356222153
- 173.81894332170486
- 171.66145634651184
- 173.058385014534
- 178.97914576530457
- 183.28136837482452
- 198.3133072257042
- 192.84906548261642
- 172.86837148666382
- 157.90447306632996
- 194.43036156892776
- 151.79857325553894
- 165.12549257278442
- 157.02757292985916
- 191.52258878946304
- 176.74082905054092
- 140.8349260687828
- 151.34403836727142
- 163.1194844841957
- 160.98103249073029
- 150.6573405265808
- 166.4439080953598
- 142.82749903202057
- 174.15537458658218
- 155.95967960357666
- 166.81572926044464
- 163.99598866701126
- 143.843312561512
- 131.7681387066841
- 159.04188764095306
- 159.7623645067215
- 146.41928452253342
- 162.9037852883339
- 175.35368037223816
- 133.1783748269081
- 200.5578896999359
- 151.1201946735382
- 165.30959278345108
- 150.72100615501404
- 165.28494197130203
- 166.96390825510025
- 155.91336673498154
- 157.24036473035812
- 164.08728229999542
- 153.8670107126236
- 131.5789892077446
- 166.54101008176804
- 138.30973052978516
- 136.71353322267532
- 134.75770157575607
- 143.8240293264389
- 141.16916596889496
- 132.54823184013367
- 137.58997535705566
- 139.79465556144714
- 183.1245790719986
- 161.60829782485962
- 183.79961383342743
- 142.7001165151596
- 149.38470149040222
- 151.109761595726
- 136.92831671237946
- 127.1117981672287
- 153.98774075508118
- 136.5283555984497
- 145.94142830371857
- 151.308633685112
- 124.76560306549072
- 151.39843219518661
- 136.7920560836792
- 147.46166402101517
- 152.67116904258728
- 147.25449377298355
- 150.8264736533165
- 130.4198402762413
- 142.41613447666168
- 161.77681648731232
- 135.91256260871887
- 137.08984923362732
- 142.4234138727188
- 139.19166886806488
- 141.17172175645828
train_accuracy:
- 0.0
- 0.05
- 0.534
- 0.505
- 0.294
- 0.427
- 0.141
- 0.333
- 0.025
- 0.325
- 0.533
- 0.8
- 0.9
- 0.242
- 0.562
- 0.438
- 0.983
- 0.781
- 0.817
- 0.75
- 0.0
- 0.3
- 0.405
- 0.683
- 0.912
- 0.775
- 0.444
- 0.723
- 0.743
- 0.406
- 0.583
- 0.553
- 0.35
- 0.7
- 0.544
- 0.421
- 0.788
- 0.571
- 0.834
- 0.769
- 0.75
- 0.86
- 0.715
- 0.825
- 0.15
- 0.792
- 0.583
- 0.672
- 0.582
- 0.133
- 0.91
- 0.6
- 0.356
- 0.809
- 0.23
- 0.012
- 0.417
- 0.607
- 0.53
- 0.932
- 0.929
- 0.665
- 0.544
- 0.65
- 0.844
- 0.94
- 0.981
- 0.292
- 0.492
- 0.317
- 0.64
- 0.932
- 0.967
- 0.768
- 0.825
- 0.359
- 0.137
- 0.524
- 0.86
- 0.762
- 0.615
- 0.832
- 0.794
- 0.983
- 0.904
- 0.491
- 0.582
- 0.875
- 0.475
- 0.361
- 1.0
- 0.858
- 0.95
- 0.537
- 0.875
- 0.604
- 0.95
- 0.525
- 0.012
- 0.607
train_loss:
- 1.05
- 0.787
- 0.588
- 0.501
- 0.507
- 0.512
- 0.412
- 0.549
- 0.432
- 0.551
- 0.416
- 0.436
- 0.354
- 0.48
- 0.405
- 0.397
- 0.38
- 0.429
- 0.391
- 0.355
- 0.316
- 0.425
- 0.399
- 0.349
- 0.29
- 0.319
- 0.383
- 0.333
- 0.337
- 0.401
- 0.371
- 0.395
- 0.304
- 0.35
- 0.344
- 0.325
- 0.366
- 0.391
- 0.38
- 0.345
- 0.291
- 0.294
- 0.392
- 0.441
- 0.344
- 0.29
- 0.295
- 0.252
- 0.253
- 0.374
- 0.335
- 0.352
- 0.317
- 0.349
- 0.318
- 0.324
- 0.302
- 0.346
- 0.304
- 0.313
- 0.31
- 0.302
- 0.338
- 0.244
- 0.309
- 0.323
- 0.334
- 0.318
- 0.372
- 0.319
- 0.328
- 0.272
- 0.282
- 0.316
- 0.299
- 0.285
- 0.322
- 0.373
- 0.301
- 0.311
- 0.291
- 0.325
- 0.309
- 0.342
- 0.338
- 0.32
- 0.334
- 0.306
- 0.359
- 0.335
- 0.312
- 0.314
- 0.272
- 0.21
- 0.273
- 0.327
- 0.279
- 0.265
- 0.303
- 0.363
unequal: 1
verbose: 1
