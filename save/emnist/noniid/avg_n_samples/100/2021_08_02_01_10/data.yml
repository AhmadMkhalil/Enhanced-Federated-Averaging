avg_train_accuracy: 0.972
avg_train_loss: 0.002
avg_type: avg_n_samples
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.14138297872340425
- 0.25420212765957445
- 0.301436170212766
- 0.3803191489361702
- 0.411968085106383
- 0.41191489361702127
- 0.3879787234042553
- 0.513563829787234
- 0.4895744680851064
- 0.5027127659574468
- 0.5315425531914894
- 0.5334042553191489
- 0.5536170212765957
- 0.5609042553191489
- 0.5476595744680851
- 0.5542553191489362
- 0.600531914893617
- 0.5755851063829788
- 0.6083510638297872
- 0.5857446808510638
- 0.5961170212765957
- 0.5531914893617021
- 0.5539893617021276
- 0.5961170212765957
- 0.5721808510638298
- 0.6313297872340425
- 0.6282978723404256
- 0.5885638297872341
- 0.6370744680851064
- 0.6270212765957447
- 0.6397340425531914
- 0.6071276595744681
- 0.6328723404255319
- 0.5941489361702128
- 0.5876063829787234
- 0.5836170212765958
- 0.6031382978723404
- 0.6242021276595745
- 0.6846276595744681
- 0.6138297872340426
- 0.6734042553191489
- 0.5884574468085106
- 0.6597872340425532
- 0.660372340425532
- 0.5564893617021277
- 0.6490425531914894
- 0.6440957446808511
- 0.6235106382978723
- 0.673031914893617
- 0.6527127659574468
- 0.6337234042553191
- 0.63
- 0.6580851063829787
- 0.6476063829787234
- 0.6053191489361702
- 0.6346808510638298
- 0.6487765957446808
- 0.7071808510638298
- 0.6878191489361702
- 0.6853191489361702
- 0.6643085106382979
- 0.6151595744680851
- 0.6296808510638298
- 0.658031914893617
- 0.643031914893617
- 0.6273404255319149
- 0.6627127659574468
- 0.6768085106382978
- 0.653563829787234
- 0.6306382978723404
- 0.6628191489361702
- 0.6451063829787234
- 0.6307978723404255
- 0.620531914893617
- 0.6418085106382979
- 0.589095744680851
- 0.6526063829787234
- 0.6605851063829787
- 0.7054255319148937
- 0.7023404255319149
- 0.6202659574468085
- 0.6464893617021277
- 0.6434042553191489
- 0.6877659574468085
- 0.630531914893617
- 0.6513829787234042
- 0.6112234042553192
- 0.6557978723404255
- 0.6595744680851063
- 0.641595744680851
- 0.6288829787234043
- 0.6721276595744681
- 0.6958510638297872
- 0.6583510638297873
- 0.68
- 0.6470212765957447
- 0.6529787234042553
- 0.68
- 0.699627659574468
- 0.6545212765957447
test_loss_list:
- 533.0875627994537
- 430.16865038871765
- 369.97313618659973
- 323.4110418558121
- 299.5345528125763
- 290.50931000709534
- 281.6789172887802
- 231.03637039661407
- 226.62451601028442
- 226.57252526283264
- 215.85212886333466
- 209.1522560119629
- 205.6437896490097
- 194.65031743049622
- 197.66202461719513
- 185.6790144443512
- 179.2711916565895
- 182.9863327741623
- 181.24645394086838
- 174.05374711751938
- 173.3686963915825
- 205.48847889900208
- 189.69322454929352
- 177.07985186576843
- 185.49249470233917
- 163.00847160816193
- 166.788738489151
- 181.30331468582153
- 156.19313567876816
- 151.81758052110672
- 150.45568478107452
- 162.41404217481613
- 162.4259471297264
- 165.87593740224838
- 171.84205424785614
- 175.2344713807106
- 171.43074214458466
- 170.94939148426056
- 139.36015105247498
- 169.139662027359
- 143.09834271669388
- 192.19628483057022
- 150.22325271368027
- 143.05372387170792
- 202.32475048303604
- 148.6404263973236
- 148.0149001479149
- 168.18959671258926
- 136.37780833244324
- 144.69841700792313
- 153.35348522663116
- 154.17931973934174
- 138.49719768762589
- 151.98968374729156
- 183.4393486380577
- 145.79805159568787
- 148.08515411615372
- 122.24160254001617
- 136.3579479455948
- 138.73131185770035
- 148.11165934801102
- 173.45255601406097
- 148.9666953086853
- 136.5397334098816
- 148.74509817361832
- 145.3985180258751
- 144.1895124912262
- 132.60697239637375
- 137.52011859416962
- 149.0536568760872
- 141.92180436849594
- 147.19756251573563
- 145.2348077893257
- 161.34107089042664
- 149.80448424816132
- 188.80347287654877
- 153.5087108016014
- 139.97806596755981
- 132.0649050474167
- 125.8809232711792
- 153.9052975177765
- 136.88383799791336
- 144.86981242895126
- 129.89419907331467
- 165.4965816140175
- 165.60386383533478
- 173.12795233726501
- 140.19221591949463
- 164.31365942955017
- 155.3272374868393
- 152.0422710776329
- 131.62616169452667
- 141.24465650320053
- 158.51740354299545
- 135.95169574022293
- 150.07183372974396
- 144.5583621263504
- 128.85921758413315
- 128.00701522827148
- 137.5812565088272
train_accuracy:
- 0.121
- 0.125
- 0.356
- 0.525
- 0.086
- 0.028
- 0.043
- 0.579
- 0.314
- 0.0
- 0.642
- 0.164
- 0.388
- 0.9
- 0.567
- 0.193
- 0.972
- 0.9
- 0.912
- 0.474
- 0.1
- 0.607
- 0.861
- 0.309
- 0.906
- 0.647
- 0.83
- 0.214
- 0.878
- 0.987
- 0.691
- 0.903
- 0.634
- 0.209
- 0.388
- 0.622
- 0.308
- 0.733
- 0.669
- 0.145
- 0.337
- 0.6
- 0.754
- 0.537
- 0.325
- 0.388
- 0.328
- 0.961
- 0.692
- 0.644
- 0.912
- 0.261
- 0.963
- 0.388
- 0.371
- 0.087
- 0.533
- 0.917
- 0.958
- 0.879
- 0.782
- 0.538
- 0.568
- 0.091
- 0.92
- 0.743
- 0.805
- 0.733
- 0.757
- 0.783
- 0.875
- 0.764
- 0.807
- 0.709
- 0.692
- 0.776
- 0.02
- 0.741
- 0.412
- 0.779
- 0.864
- 0.581
- 0.99
- 0.714
- 0.656
- 0.938
- 0.678
- 0.15
- 0.99
- 0.6
- 0.369
- 0.745
- 0.55
- 0.856
- 0.786
- 0.65
- 0.855
- 0.825
- 0.765
- 0.972
train_loss:
- 1.196
- 0.811
- 0.601
- 0.649
- 0.553
- 0.481
- 0.483
- 0.4
- 0.401
- 0.42
- 0.443
- 0.452
- 0.522
- 0.433
- 0.451
- 0.44
- 0.392
- 0.371
- 0.444
- 0.409
- 0.409
- 0.392
- 0.37
- 0.425
- 0.33
- 0.372
- 0.45
- 0.42
- 0.383
- 0.361
- 0.372
- 0.36
- 0.415
- 0.339
- 0.341
- 0.311
- 0.381
- 0.326
- 0.371
- 0.301
- 0.346
- 0.314
- 0.374
- 0.33
- 0.35
- 0.383
- 0.376
- 0.365
- 0.345
- 0.34
- 0.356
- 0.396
- 0.284
- 0.342
- 0.33
- 0.394
- 0.4
- 0.385
- 0.326
- 0.365
- 0.296
- 0.343
- 0.35
- 0.363
- 0.326
- 0.358
- 0.29
- 0.318
- 0.312
- 0.301
- 0.362
- 0.374
- 0.349
- 0.327
- 0.382
- 0.286
- 0.278
- 0.397
- 0.305
- 0.367
- 0.335
- 0.353
- 0.283
- 0.329
- 0.296
- 0.29
- 0.321
- 0.344
- 0.27
- 0.319
- 0.306
- 0.326
- 0.296
- 0.283
- 0.374
- 0.249
- 0.325
- 0.312
- 0.348
- 0.237
unequal: 1
verbose: 1
