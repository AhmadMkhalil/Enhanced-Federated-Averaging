avg_train_accuracy: 0.885
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.16393617021276596
- 0.25707446808510637
- 0.3446808510638298
- 0.4125
- 0.4718085106382979
- 0.4752659574468085
- 0.40361702127659577
- 0.39845744680851064
- 0.5131914893617021
- 0.545372340425532
- 0.5237765957446808
- 0.500904255319149
- 0.5090425531914894
- 0.5296808510638298
- 0.48851063829787233
- 0.5013297872340425
- 0.5778723404255319
- 0.5717021276595745
- 0.6023936170212766
- 0.4953723404255319
- 0.5559574468085107
- 0.5956382978723405
- 0.5927127659574468
- 0.5571808510638298
- 0.5553191489361702
- 0.6410638297872341
- 0.6054255319148936
- 0.6047872340425532
- 0.6303723404255319
- 0.5528191489361702
- 0.5601063829787234
- 0.596968085106383
- 0.6457978723404255
- 0.6092553191489362
- 0.6154787234042554
- 0.5867553191489362
- 0.5990425531914894
- 0.6077127659574468
- 0.6152127659574468
- 0.6278723404255319
- 0.5952127659574468
- 0.6153723404255319
- 0.6161702127659574
- 0.630531914893617
- 0.6541489361702127
- 0.655904255319149
- 0.655
- 0.6352127659574468
- 0.6023404255319149
- 0.6497872340425532
- 0.6483510638297872
- 0.6517553191489361
- 0.6416489361702128
- 0.6432978723404256
- 0.6448936170212766
- 0.6028191489361702
- 0.614468085106383
- 0.6554787234042553
- 0.6679255319148936
- 0.6192553191489362
- 0.6036702127659574
- 0.6575
- 0.6267553191489361
- 0.5865957446808511
- 0.6218617021276596
- 0.6430851063829788
- 0.6519680851063829
- 0.6155851063829787
- 0.6276063829787234
- 0.628031914893617
- 0.6236170212765958
- 0.5946808510638298
- 0.6166489361702128
- 0.6604787234042553
- 0.6920212765957446
- 0.6227659574468085
- 0.5981914893617021
- 0.6445744680851064
- 0.6525531914893618
- 0.6763297872340426
- 0.6377127659574469
- 0.6551063829787234
- 0.6609574468085107
- 0.6360106382978723
- 0.6709574468085107
- 0.6483510638297872
- 0.6232978723404256
- 0.6353191489361703
- 0.643936170212766
- 0.6223936170212766
- 0.6468085106382979
- 0.6629255319148936
- 0.6575
- 0.6747340425531915
- 0.6201595744680851
- 0.6604787234042553
- 0.6657978723404255
- 0.6782978723404255
- 0.6389893617021276
- 0.6561170212765958
test_loss_list:
- 524.2246127128601
- 424.26088094711304
- 372.91609287261963
- 331.97252905368805
- 279.59127843379974
- 258.55369198322296
- 290.05089592933655
- 299.9114912748337
- 231.65330624580383
- 204.18868935108185
- 203.34475016593933
- 215.53005921840668
- 215.08783495426178
- 203.7709879875183
- 226.0746170282364
- 226.5040134191513
- 181.00798749923706
- 188.10115575790405
- 169.44321018457413
- 207.49510645866394
- 192.85386443138123
- 178.64424574375153
- 180.36335068941116
- 186.33736038208008
- 207.70006382465363
- 149.57421207427979
- 162.16213923692703
- 162.1860350370407
- 154.97514700889587
- 187.88373857736588
- 196.47362679243088
- 173.67509573698044
- 147.89936780929565
- 172.0673057436943
- 174.2064546942711
- 178.50268214941025
- 191.7332962155342
- 190.11925268173218
- 169.55160397291183
- 153.24281352758408
- 176.938190639019
- 187.4389905333519
- 160.66221690177917
- 160.92789644002914
- 157.15312707424164
- 148.03818172216415
- 143.61609148979187
- 153.63123017549515
- 161.66268944740295
- 136.09815871715546
- 143.00949716567993
- 144.88414180278778
- 147.05103534460068
- 143.40900665521622
- 144.7130230665207
- 163.04663079977036
- 156.33370524644852
- 145.90524518489838
- 133.04207915067673
- 151.08856296539307
- 177.66145116090775
- 147.59645318984985
- 152.3966800570488
- 182.3076930642128
- 149.85138642787933
- 149.9060726761818
- 149.24896639585495
- 159.38217616081238
- 161.16860508918762
- 160.13117462396622
- 166.36197078227997
- 166.87037110328674
- 157.5766916871071
- 147.84029322862625
- 133.5298068523407
- 168.1252322793007
- 152.69531404972076
- 142.56603926420212
- 140.23489302396774
- 134.72913867235184
- 144.24542742967606
- 147.42022800445557
- 159.9180063009262
- 155.75999081134796
- 142.30653721094131
- 164.15167820453644
- 167.0081507563591
- 152.11481112241745
- 154.50271481275558
- 182.0386547446251
- 143.9286116361618
- 151.19890940189362
- 132.34735816717148
- 126.41877907514572
- 156.5050002336502
- 134.6365641951561
- 139.15179240703583
- 134.49734926223755
- 149.37687814235687
- 152.07284551858902
train_accuracy:
- 0.079
- 0.0
- 0.413
- 0.481
- 0.369
- 0.312
- 0.523
- 0.332
- 0.933
- 0.369
- 0.444
- 0.0
- 0.98
- 0.505
- 0.908
- 0.684
- 0.558
- 0.242
- 0.621
- 0.1
- 0.382
- 0.728
- 0.97
- 0.39
- 0.388
- 0.831
- 0.914
- 0.531
- 0.731
- 0.0
- 0.559
- 0.544
- 0.455
- 0.4
- 0.794
- 0.44
- 0.87
- 0.931
- 0.873
- 0.631
- 0.787
- 0.204
- 0.725
- 0.562
- 0.518
- 0.585
- 0.97
- 0.623
- 0.782
- 0.647
- 0.833
- 0.75
- 0.685
- 0.367
- 0.225
- 0.481
- 0.923
- 0.719
- 0.91
- 0.561
- 0.0
- 0.95
- 0.713
- 0.675
- 0.58
- 0.653
- 0.75
- 0.427
- 0.359
- 0.9
- 0.242
- 0.307
- 0.218
- 0.488
- 0.84
- 0.23
- 0.49
- 0.769
- 0.917
- 0.242
- 0.55
- 0.9
- 0.885
- 0.269
- 0.975
- 0.4
- 0.89
- 0.742
- 0.65
- 0.723
- 0.794
- 0.838
- 0.793
- 0.721
- 0.862
- 0.904
- 0.667
- 0.819
- 0.823
- 0.885
train_loss:
- 1.236
- 0.813
- 0.633
- 0.542
- 0.491
- 0.632
- 0.436
- 0.453
- 0.541
- 0.458
- 0.476
- 0.422
- 0.421
- 0.392
- 0.435
- 0.491
- 0.468
- 0.402
- 0.403
- 0.374
- 0.364
- 0.341
- 0.403
- 0.387
- 0.333
- 0.404
- 0.323
- 0.364
- 0.454
- 0.353
- 0.308
- 0.358
- 0.371
- 0.368
- 0.377
- 0.349
- 0.348
- 0.396
- 0.299
- 0.343
- 0.34
- 0.347
- 0.352
- 0.335
- 0.282
- 0.405
- 0.362
- 0.347
- 0.307
- 0.269
- 0.344
- 0.303
- 0.351
- 0.316
- 0.373
- 0.3
- 0.336
- 0.364
- 0.319
- 0.323
- 0.278
- 0.35
- 0.345
- 0.342
- 0.261
- 0.281
- 0.288
- 0.349
- 0.235
- 0.337
- 0.314
- 0.299
- 0.274
- 0.29
- 0.306
- 0.282
- 0.289
- 0.353
- 0.383
- 0.303
- 0.306
- 0.35
- 0.332
- 0.373
- 0.266
- 0.351
- 0.286
- 0.267
- 0.307
- 0.273
- 0.318
- 0.362
- 0.375
- 0.364
- 0.263
- 0.382
- 0.285
- 0.341
- 0.323
- 0.302
unequal: 1
verbose: 1
