avg_train_accuracy: 0.35
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.14324468085106384
- 0.24095744680851064
- 0.3144148936170213
- 0.27111702127659576
- 0.4331382978723404
- 0.4734042553191489
- 0.44632978723404254
- 0.4806382978723404
- 0.4668617021276596
- 0.5197340425531914
- 0.4800531914893617
- 0.5538297872340425
- 0.5393617021276595
- 0.5854787234042553
- 0.5890425531914893
- 0.555531914893617
- 0.5815425531914894
- 0.5618617021276596
- 0.5059574468085106
- 0.529468085106383
- 0.5815425531914894
- 0.5957978723404256
- 0.5640957446808511
- 0.6631914893617021
- 0.6140425531914894
- 0.5694148936170212
- 0.5617021276595745
- 0.569468085106383
- 0.6153723404255319
- 0.6447872340425532
- 0.5735106382978723
- 0.6460638297872341
- 0.6367021276595745
- 0.6392553191489362
- 0.6077659574468085
- 0.6368085106382979
- 0.6473404255319148
- 0.6410638297872341
- 0.6038829787234042
- 0.6235106382978723
- 0.6338297872340426
- 0.5973404255319149
- 0.6569680851063829
- 0.6519680851063829
- 0.6487234042553192
- 0.6473936170212766
- 0.6538829787234043
- 0.643563829787234
- 0.636968085106383
- 0.6495744680851064
- 0.6921808510638298
- 0.6794148936170212
- 0.6654255319148936
- 0.6549468085106382
- 0.6618085106382978
- 0.6592021276595744
- 0.6847872340425532
- 0.6640957446808511
- 0.6370212765957447
- 0.6926595744680851
- 0.6809574468085107
- 0.6901063829787234
- 0.5569148936170213
- 0.6292553191489362
- 0.6343085106382979
- 0.6960106382978724
- 0.7218617021276595
- 0.6922872340425532
- 0.6777659574468086
- 0.673936170212766
- 0.6401595744680851
- 0.6767021276595745
- 0.6651595744680852
- 0.617872340425532
- 0.6614361702127659
- 0.6492021276595744
- 0.700531914893617
- 0.6754255319148936
- 0.6576595744680851
- 0.6658510638297872
- 0.6425531914893617
- 0.7151595744680851
- 0.6738829787234043
- 0.6763829787234042
- 0.7005851063829788
- 0.7120744680851064
- 0.6408510638297872
- 0.638031914893617
- 0.5973404255319149
- 0.6504255319148936
- 0.6645212765957447
- 0.6840425531914893
- 0.6785106382978724
- 0.6842553191489362
- 0.6707978723404255
- 0.6685106382978724
- 0.6397340425531914
- 0.6241489361702127
- 0.5989361702127659
- 0.6361702127659574
test_loss_list:
- 535.607180595398
- 440.4825277328491
- 388.0378773212433
- 387.0167155265808
- 302.2760897874832
- 285.9147516489029
- 263.1830756664276
- 250.04533517360687
- 249.92493307590485
- 236.48455023765564
- 238.69143092632294
- 206.82557284832
- 202.9483892917633
- 188.471355676651
- 176.36906653642654
- 212.3512396812439
- 177.09023320674896
- 189.1714784502983
- 211.40198838710785
- 207.9645618200302
- 188.50424993038177
- 181.04446125030518
- 212.82911038398743
- 148.95726430416107
- 168.9393903017044
- 178.8609454035759
- 208.77232122421265
- 180.36334919929504
- 165.10761499404907
- 145.24057441949844
- 179.96783351898193
- 155.71509104967117
- 148.28733229637146
- 156.71294844150543
- 181.47140508890152
- 155.7554391026497
- 144.70584815740585
- 149.384956240654
- 167.51607018709183
- 161.05813938379288
- 155.2576807141304
- 176.8157587647438
- 154.03075736761093
- 153.30676114559174
- 145.54418128728867
- 159.18677270412445
- 159.44865077733994
- 148.38959628343582
- 164.9538032412529
- 143.40201622247696
- 125.6952612400055
- 129.3749150633812
- 141.3648298382759
- 141.71422135829926
- 134.25246387720108
- 146.90802186727524
- 143.65319228172302
- 146.65850937366486
- 153.58226066827774
- 137.36817514896393
- 138.84932363033295
- 134.43148201704025
- 188.6692361831665
- 162.32623809576035
- 162.03077590465546
- 119.8877524137497
- 119.90883207321167
- 130.45548337697983
- 138.1562775373459
- 137.74354195594788
- 162.47324419021606
- 136.16380339860916
- 142.17929297685623
- 177.43953716754913
- 136.17293626070023
- 154.50833266973495
- 130.46050035953522
- 134.43873542547226
- 136.18478089571
- 142.3938210606575
- 144.22452521324158
- 134.9972317814827
- 148.0868023633957
- 161.412339925766
- 129.5258812904358
- 118.9344482421875
- 154.85216110944748
- 155.33274614810944
- 170.33776223659515
- 143.20120561122894
- 133.73759871721268
- 139.62267470359802
- 142.9664168357849
- 128.3483629822731
- 143.993805706501
- 141.497567653656
- 153.86907291412354
- 153.27720522880554
- 169.54265588521957
- 157.70408076047897
train_accuracy:
- 0.189
- 0.115
- 0.056
- 0.728
- 0.0
- 0.0
- 0.565
- 0.289
- 0.95
- 0.397
- 0.58
- 0.279
- 0.783
- 0.726
- 0.373
- 0.744
- 0.911
- 0.269
- 0.685
- 0.781
- 0.69
- 0.471
- 0.914
- 0.679
- 0.389
- 0.51
- 0.35
- 0.609
- 0.743
- 0.987
- 0.496
- 0.735
- 0.408
- 0.489
- 0.773
- 0.321
- 0.6
- 0.33
- 0.612
- 0.4
- 0.841
- 0.612
- 0.6
- 0.93
- 0.55
- 0.506
- 0.12
- 0.457
- 0.679
- 0.365
- 0.85
- 0.75
- 0.67
- 0.05
- 0.697
- 0.665
- 0.704
- 0.9
- 0.9
- 0.847
- 0.01
- 0.818
- 0.523
- 0.4
- 0.918
- 0.9
- 0.85
- 0.911
- 0.607
- 0.825
- 0.322
- 0.617
- 0.222
- 1.0
- 0.218
- 0.75
- 0.844
- 0.796
- 0.85
- 0.354
- 0.987
- 0.538
- 0.587
- 0.07
- 0.65
- 0.878
- 0.65
- 1.0
- 0.508
- 0.883
- 0.403
- 0.987
- 0.744
- 0.447
- 0.933
- 0.691
- 0.737
- 1.0
- 0.032
- 0.35
train_loss:
- 1.195
- 0.928
- 0.566
- 0.595
- 0.549
- 0.471
- 0.457
- 0.424
- 0.46
- 0.421
- 0.446
- 0.444
- 0.476
- 0.405
- 0.405
- 0.383
- 0.317
- 0.379
- 0.454
- 0.406
- 0.357
- 0.361
- 0.444
- 0.407
- 0.375
- 0.367
- 0.372
- 0.39
- 0.421
- 0.348
- 0.387
- 0.359
- 0.35
- 0.42
- 0.317
- 0.334
- 0.413
- 0.342
- 0.337
- 0.414
- 0.382
- 0.407
- 0.363
- 0.292
- 0.369
- 0.378
- 0.334
- 0.437
- 0.249
- 0.38
- 0.314
- 0.401
- 0.403
- 0.292
- 0.411
- 0.356
- 0.354
- 0.254
- 0.296
- 0.343
- 0.314
- 0.405
- 0.342
- 0.399
- 0.375
- 0.376
- 0.307
- 0.337
- 0.368
- 0.317
- 0.322
- 0.293
- 0.348
- 0.328
- 0.431
- 0.314
- 0.352
- 0.312
- 0.427
- 0.278
- 0.311
- 0.36
- 0.277
- 0.226
- 0.286
- 0.342
- 0.341
- 0.282
- 0.329
- 0.26
- 0.3
- 0.315
- 0.279
- 0.388
- 0.297
- 0.361
- 0.312
- 0.363
- 0.351
- 0.342
unequal: 1
verbose: 1
