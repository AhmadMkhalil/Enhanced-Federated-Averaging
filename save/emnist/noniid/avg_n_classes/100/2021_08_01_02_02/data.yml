avg_train_accuracy: 0.825
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.14803191489361703
- 0.21664893617021277
- 0.28191489361702127
- 0.4047872340425532
- 0.4098936170212766
- 0.4170744680851064
- 0.4262234042553191
- 0.4579255319148936
- 0.42595744680851066
- 0.4851063829787234
- 0.48138297872340424
- 0.5103191489361703
- 0.5172872340425532
- 0.5332978723404256
- 0.5731382978723404
- 0.5896808510638298
- 0.5946276595744681
- 0.5614361702127659
- 0.6057446808510638
- 0.6232978723404256
- 0.5921808510638298
- 0.5987234042553191
- 0.6324468085106383
- 0.6302659574468085
- 0.6110638297872341
- 0.6022872340425532
- 0.6117553191489362
- 0.6063297872340425
- 0.6062234042553192
- 0.5913297872340425
- 0.6112765957446809
- 0.6382446808510638
- 0.5991489361702128
- 0.6418617021276596
- 0.6436702127659575
- 0.6342021276595745
- 0.6561702127659574
- 0.6566489361702128
- 0.6508510638297872
- 0.7067021276595745
- 0.6402659574468085
- 0.6007446808510638
- 0.6526595744680851
- 0.5942021276595745
- 0.6914893617021277
- 0.6851063829787234
- 0.6923404255319149
- 0.6552659574468085
- 0.6445212765957447
- 0.6679255319148936
- 0.6126063829787234
- 0.7148404255319148
- 0.7062234042553192
- 0.6348936170212766
- 0.693031914893617
- 0.6154787234042554
- 0.7033510638297872
- 0.6327659574468085
- 0.6431914893617021
- 0.6859574468085107
- 0.643936170212766
- 0.6020212765957447
- 0.655
- 0.6761170212765958
- 0.6570744680851064
- 0.6901063829787234
- 0.6579787234042553
- 0.6424468085106383
- 0.6081914893617021
- 0.6713297872340426
- 0.6457446808510638
- 0.7039893617021277
- 0.7192021276595745
- 0.6878191489361702
- 0.6574468085106383
- 0.6558510638297872
- 0.7060106382978724
- 0.6693617021276596
- 0.6783510638297873
- 0.6770744680851064
- 0.6419680851063829
- 0.6581382978723405
- 0.6769148936170213
- 0.6456382978723404
- 0.6371808510638298
- 0.6921276595744681
- 0.6439893617021276
- 0.6845744680851064
- 0.7009042553191489
- 0.6582446808510638
- 0.6521808510638298
- 0.7086702127659574
- 0.6713297872340426
- 0.6663829787234042
- 0.6748404255319149
- 0.6607978723404255
- 0.646595744680851
- 0.6704255319148936
- 0.6731914893617021
- 0.6076063829787234
test_loss_list:
- 535.0130434036255
- 455.7723171710968
- 412.6951410770416
- 318.48048186302185
- 294.2122474908829
- 282.9170343875885
- 289.9890875816345
- 271.0612816810608
- 284.95731496810913
- 239.69206511974335
- 231.8568422794342
- 221.50446832180023
- 222.76014482975006
- 212.61103308200836
- 192.92603850364685
- 185.4794031381607
- 181.82607448101044
- 203.87174201011658
- 163.24128741025925
- 165.37259882688522
- 169.809106528759
- 187.4438751935959
- 156.3078991174698
- 171.11384463310242
- 178.6438588500023
- 171.662828207016
- 163.80513679981232
- 180.65863966941833
- 174.46133035421371
- 182.82447218894958
- 168.26306247711182
- 159.87125396728516
- 189.9611815214157
- 155.23124581575394
- 168.90521973371506
- 159.76877236366272
- 144.62808579206467
- 143.23776596784592
- 157.33992367982864
- 130.73296004533768
- 143.74638843536377
- 161.36671900749207
- 141.34283059835434
- 173.94803720712662
- 133.91252624988556
- 137.63193958997726
- 129.99320954084396
- 145.24805617332458
- 153.16550141572952
- 153.00195008516312
- 164.54967665672302
- 129.48722559213638
- 121.86002165079117
- 141.79022508859634
- 125.55659317970276
- 146.16681343317032
- 134.6885911822319
- 150.50415444374084
- 151.34710240364075
- 129.1698019504547
- 160.74909788370132
- 169.7041370868683
- 147.31234741210938
- 140.23414552211761
- 146.56735903024673
- 131.85530418157578
- 140.49723541736603
- 160.85634046792984
- 156.79714500904083
- 149.91376447677612
- 147.5454614162445
- 143.0646767616272
- 121.60299164056778
- 142.79334086179733
- 147.71811419725418
- 136.1502450108528
- 123.97750842571259
- 131.62620496749878
- 130.48287320137024
- 134.7073483467102
- 177.71176373958588
- 144.6639199256897
- 131.7869490981102
- 143.86647886037827
- 150.35468596220016
- 136.5314700603485
- 147.13836431503296
- 133.28939229249954
- 120.57030844688416
- 133.74984562397003
- 140.43768352270126
- 124.40555673837662
- 129.59402471780777
- 140.29493135213852
- 133.48009306192398
- 154.78187656402588
- 155.68910431861877
- 145.74452090263367
- 150.32432174682617
- 160.57598847150803
train_accuracy:
- 0.0
- 0.385
- 0.007
- 0.091
- 0.564
- 0.643
- 0.975
- 0.331
- 0.0
- 0.332
- 0.767
- 0.511
- 0.75
- 0.587
- 0.533
- 0.67
- 0.773
- 0.508
- 0.49
- 0.519
- 0.912
- 0.867
- 0.943
- 0.727
- 0.194
- 0.742
- 0.9
- 0.921
- 0.1
- 0.494
- 0.232
- 0.888
- 0.39
- 0.67
- 0.967
- 0.571
- 0.362
- 0.37
- 0.861
- 0.84
- 0.971
- 0.98
- 0.703
- 0.493
- 0.681
- 0.233
- 0.877
- 0.356
- 0.728
- 0.763
- 0.647
- 0.712
- 0.875
- 0.411
- 0.821
- 0.456
- 0.914
- 0.613
- 0.836
- 0.787
- 0.356
- 0.375
- 0.675
- 0.748
- 0.893
- 0.582
- 0.666
- 0.0
- 0.917
- 0.717
- 0.7
- 0.565
- 0.853
- 0.75
- 0.843
- 0.797
- 0.793
- 0.893
- 0.645
- 0.943
- 0.912
- 0.4
- 0.736
- 0.388
- 0.139
- 0.97
- 0.895
- 0.833
- 0.379
- 0.783
- 0.883
- 0.944
- 0.925
- 0.975
- 0.9
- 0.312
- 0.136
- 0.983
- 0.961
- 0.825
train_loss:
- 1.08
- 0.852
- 0.739
- 0.584
- 0.59
- 0.52
- 0.497
- 0.467
- 0.476
- 0.505
- 0.474
- 0.472
- 0.429
- 0.385
- 0.425
- 0.404
- 0.37
- 0.456
- 0.377
- 0.344
- 0.385
- 0.366
- 0.353
- 0.331
- 0.297
- 0.353
- 0.317
- 0.329
- 0.326
- 0.314
- 0.404
- 0.397
- 0.291
- 0.437
- 0.271
- 0.412
- 0.413
- 0.349
- 0.382
- 0.376
- 0.419
- 0.419
- 0.384
- 0.388
- 0.27
- 0.316
- 0.345
- 0.358
- 0.381
- 0.344
- 0.387
- 0.348
- 0.316
- 0.392
- 0.326
- 0.357
- 0.386
- 0.296
- 0.324
- 0.349
- 0.306
- 0.338
- 0.4
- 0.311
- 0.363
- 0.319
- 0.274
- 0.315
- 0.292
- 0.344
- 0.362
- 0.332
- 0.349
- 0.296
- 0.321
- 0.382
- 0.335
- 0.312
- 0.398
- 0.315
- 0.314
- 0.361
- 0.268
- 0.375
- 0.284
- 0.317
- 0.346
- 0.268
- 0.265
- 0.328
- 0.378
- 0.312
- 0.361
- 0.295
- 0.358
- 0.267
- 0.3
- 0.273
- 0.292
- 0.328
unequal: 1
verbose: 1
