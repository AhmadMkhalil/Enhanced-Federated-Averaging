avg_train_accuracy: 0.826
avg_train_loss: 0.004
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1672340425531915
- 0.2881382978723404
- 0.41712765957446807
- 0.37882978723404254
- 0.41627659574468084
- 0.3874468085106383
- 0.533031914893617
- 0.5040425531914894
- 0.539468085106383
- 0.501436170212766
- 0.5481382978723405
- 0.5751595744680851
- 0.5218085106382979
- 0.5063297872340425
- 0.5771276595744681
- 0.5885106382978723
- 0.5944680851063829
- 0.6033510638297872
- 0.5892553191489361
- 0.5601595744680851
- 0.5656914893617021
- 0.6336170212765957
- 0.5504787234042553
- 0.6057978723404255
- 0.5915957446808511
- 0.6003191489361702
- 0.5857446808510638
- 0.5842021276595745
- 0.5962234042553192
- 0.6014893617021276
- 0.6176063829787234
- 0.6546808510638298
- 0.6134574468085107
- 0.6063829787234043
- 0.6145212765957446
- 0.6919148936170213
- 0.6427659574468085
- 0.616968085106383
- 0.6551063829787234
- 0.6572340425531915
- 0.6278191489361702
- 0.5707446808510638
- 0.6590957446808511
- 0.5979787234042553
- 0.6782446808510638
- 0.629095744680851
- 0.6491489361702127
- 0.6111702127659574
- 0.6640957446808511
- 0.5702659574468085
- 0.6643085106382979
- 0.6154787234042554
- 0.6121808510638298
- 0.6068085106382979
- 0.6524468085106383
- 0.6263829787234042
- 0.6779787234042554
- 0.6223404255319149
- 0.6880851063829787
- 0.6362234042553192
- 0.6743085106382979
- 0.6581382978723405
- 0.6631914893617021
- 0.6806382978723404
- 0.649468085106383
- 0.6698404255319149
- 0.6048936170212766
- 0.6907978723404256
- 0.6490957446808511
- 0.6799468085106383
- 0.6559574468085106
- 0.6742021276595744
- 0.6537234042553192
- 0.6589893617021276
- 0.6331382978723404
- 0.6600531914893617
- 0.6767021276595745
- 0.6793085106382979
- 0.6603191489361702
- 0.6267553191489361
- 0.6611170212765958
- 0.6401063829787234
- 0.5860106382978724
- 0.6457978723404255
- 0.6497872340425532
- 0.6476595744680851
- 0.6931914893617022
- 0.6494148936170213
- 0.6411170212765958
- 0.6738829787234043
- 0.698936170212766
- 0.7159042553191489
- 0.6820744680851064
- 0.7086170212765958
- 0.6747340425531915
- 0.656436170212766
- 0.6695212765957447
- 0.6950531914893617
- 0.6703191489361702
- 0.6394148936170213
test_loss_list:
- 524.6961970329285
- 442.1191232204437
- 357.04843616485596
- 350.1294369697571
- 327.7014811038971
- 325.5948089361191
- 228.65341758728027
- 224.24049258232117
- 217.50151944160461
- 232.88143801689148
- 220.0909229516983
- 192.70651257038116
- 203.72219097614288
- 212.80930840969086
- 188.712144613266
- 175.4321408867836
- 175.24914801120758
- 188.79644936323166
- 198.2370331287384
- 204.60650885105133
- 180.74545645713806
- 167.72451609373093
- 221.03671956062317
- 187.18483555316925
- 174.60845351219177
- 168.73178058862686
- 178.63547199964523
- 173.06643569469452
- 168.47969633340836
- 175.08329820632935
- 163.0594293475151
- 155.41608315706253
- 174.0239681005478
- 162.1948006749153
- 177.58875638246536
- 152.6428845524788
- 148.52492195367813
- 161.0025314092636
- 152.91965413093567
- 152.50418746471405
- 159.307366669178
- 203.05062794685364
- 150.8759931921959
- 168.1589447259903
- 140.5081205368042
- 163.42233222723007
- 146.3187374472618
- 160.2536377310753
- 142.8917247056961
- 201.82976770401
- 154.99639111757278
- 169.4643987417221
- 165.6353161931038
- 170.65576803684235
- 156.2213059067726
- 171.97038048505783
- 143.11327004432678
- 161.56095027923584
- 134.2056872844696
- 151.14047825336456
- 133.65975868701935
- 147.01015746593475
- 141.92644399404526
- 133.9148018360138
- 151.67291992902756
- 138.23344826698303
- 181.29612052440643
- 141.25454753637314
- 162.7859010696411
- 126.54380095005035
- 134.46741390228271
- 132.46283221244812
- 146.89996600151062
- 151.62188005447388
- 148.20388013124466
- 159.2552546262741
- 132.16917937994003
- 142.90797573328018
- 154.75761491060257
- 153.75462818145752
- 139.05852782726288
- 160.32044088840485
- 187.83527064323425
- 144.99394154548645
- 144.4445345401764
- 149.33781337738037
- 124.08975780010223
- 153.76131242513657
- 151.3507497906685
- 153.44124233722687
- 129.69615185260773
- 124.22478640079498
- 129.05227875709534
- 119.69119811058044
- 135.4165394306183
- 144.46371990442276
- 134.9741644859314
- 124.96484303474426
- 137.7189096212387
- 151.09846425056458
train_accuracy:
- 0.055
- 0.0
- 0.8
- 0.269
- 0.05
- 0.372
- 0.807
- 0.513
- 0.411
- 0.45
- 0.68
- 0.083
- 0.688
- 0.336
- 0.221
- 0.432
- 0.65
- 0.392
- 0.862
- 0.625
- 0.654
- 0.589
- 0.65
- 0.612
- 0.888
- 0.99
- 0.389
- 0.056
- 0.975
- 0.481
- 0.904
- 0.75
- 0.64
- 0.95
- 0.7
- 0.589
- 0.933
- 0.364
- 0.179
- 0.98
- 0.675
- 0.585
- 0.839
- 0.1
- 0.894
- 0.557
- 0.847
- 0.628
- 0.743
- 0.86
- 0.513
- 0.363
- 0.99
- 0.543
- 0.761
- 0.4
- 0.513
- 0.83
- 0.933
- 0.325
- 0.482
- 0.672
- 0.712
- 0.575
- 0.78
- 0.719
- 0.113
- 0.545
- 0.638
- 0.617
- 0.529
- 0.086
- 0.917
- 0.75
- 0.98
- 0.74
- 0.664
- 0.583
- 0.868
- 0.669
- 0.75
- 0.89
- 0.442
- 0.796
- 0.732
- 0.583
- 0.839
- 0.645
- 0.729
- 0.825
- 0.814
- 0.892
- 0.908
- 0.614
- 0.581
- 0.888
- 0.635
- 0.95
- 0.329
- 0.826
train_loss:
- 1.117
- 0.826
- 0.62
- 0.633
- 0.478
- 0.6
- 0.516
- 0.389
- 0.5
- 0.514
- 0.451
- 0.457
- 0.43
- 0.465
- 0.356
- 0.41
- 0.532
- 0.406
- 0.471
- 0.354
- 0.375
- 0.367
- 0.387
- 0.372
- 0.406
- 0.272
- 0.34
- 0.384
- 0.371
- 0.398
- 0.33
- 0.352
- 0.39
- 0.362
- 0.358
- 0.308
- 0.346
- 0.278
- 0.425
- 0.335
- 0.409
- 0.378
- 0.376
- 0.389
- 0.322
- 0.276
- 0.33
- 0.331
- 0.29
- 0.325
- 0.348
- 0.39
- 0.331
- 0.356
- 0.298
- 0.401
- 0.298
- 0.388
- 0.353
- 0.327
- 0.431
- 0.347
- 0.456
- 0.316
- 0.377
- 0.363
- 0.266
- 0.337
- 0.328
- 0.383
- 0.368
- 0.301
- 0.331
- 0.311
- 0.305
- 0.402
- 0.288
- 0.404
- 0.365
- 0.326
- 0.356
- 0.228
- 0.268
- 0.351
- 0.416
- 0.323
- 0.366
- 0.284
- 0.335
- 0.257
- 0.327
- 0.317
- 0.37
- 0.356
- 0.357
- 0.319
- 0.286
- 0.346
- 0.323
- 0.372
unequal: 1
verbose: 1
