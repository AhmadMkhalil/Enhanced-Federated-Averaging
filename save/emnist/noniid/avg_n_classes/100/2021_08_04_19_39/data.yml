avg_train_accuracy: 0.75
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.11122340425531915
- 0.255
- 0.33904255319148935
- 0.3345744680851064
- 0.4187234042553192
- 0.49882978723404253
- 0.4379787234042553
- 0.42659574468085104
- 0.5231382978723405
- 0.5177659574468085
- 0.5458510638297872
- 0.5516489361702127
- 0.5034042553191489
- 0.5513297872340426
- 0.5436702127659574
- 0.5475531914893617
- 0.5301595744680851
- 0.5630851063829787
- 0.6059042553191489
- 0.5570212765957446
- 0.6063297872340425
- 0.5408510638297872
- 0.6141489361702127
- 0.6171276595744681
- 0.5896808510638298
- 0.5920744680851063
- 0.5675
- 0.5791489361702128
- 0.651595744680851
- 0.6047872340425532
- 0.6138829787234042
- 0.6475
- 0.6096276595744681
- 0.6067553191489362
- 0.5731914893617022
- 0.6537765957446808
- 0.6602659574468085
- 0.6446276595744681
- 0.5784574468085106
- 0.5721276595744681
- 0.685531914893617
- 0.6322340425531915
- 0.6392021276595745
- 0.6153723404255319
- 0.6388829787234043
- 0.6775531914893617
- 0.6631914893617021
- 0.6820212765957446
- 0.6777659574468086
- 0.6340425531914894
- 0.6903723404255319
- 0.645904255319149
- 0.6737765957446809
- 0.6106382978723405
- 0.6417021276595745
- 0.6275
- 0.714627659574468
- 0.6420212765957447
- 0.6626063829787234
- 0.6684574468085106
- 0.6318617021276596
- 0.6588829787234043
- 0.6692553191489362
- 0.6503723404255319
- 0.6534574468085106
- 0.6337234042553191
- 0.6527659574468085
- 0.648031914893617
- 0.6776595744680851
- 0.6531914893617021
- 0.6888829787234042
- 0.6717553191489362
- 0.6399468085106383
- 0.6518085106382979
- 0.6554255319148936
- 0.6622340425531915
- 0.6831382978723404
- 0.6766489361702127
- 0.6939893617021277
- 0.6707446808510639
- 0.6696808510638298
- 0.6520744680851064
- 0.6711702127659575
- 0.678031914893617
- 0.6548404255319149
- 0.6656382978723404
- 0.5970212765957447
- 0.6288829787234043
- 0.6896808510638298
- 0.6557978723404255
- 0.6932978723404255
- 0.6608510638297872
- 0.6648936170212766
- 0.6769148936170213
- 0.6798404255319149
- 0.6617021276595745
- 0.6714893617021277
- 0.7025531914893617
- 0.6302127659574468
- 0.6623936170212766
test_loss_list:
- 532.6787621974945
- 438.58150458335876
- 378.3963987827301
- 365.089102268219
- 301.33309864997864
- 245.09035062789917
- 266.11540853977203
- 265.0158087015152
- 221.7221404314041
- 221.3122591972351
- 204.51140367984772
- 205.43979561328888
- 228.6210731267929
- 190.78202724456787
- 197.8517005443573
- 220.7494673728943
- 201.942054271698
- 194.71274828910828
- 180.44272255897522
- 189.6895216703415
- 171.5965086221695
- 196.80012667179108
- 176.09401512145996
- 169.86366075277328
- 180.60181123018265
- 183.15919148921967
- 188.58907514810562
- 178.71101677417755
- 151.53177672624588
- 166.59675770998
- 164.18695402145386
- 159.48088228702545
- 168.91939508914948
- 178.88068741559982
- 187.83972281217575
- 147.76206851005554
- 153.15382409095764
- 141.72216349840164
- 179.24530351161957
- 178.89831191301346
- 138.54722917079926
- 165.45266091823578
- 159.9604686498642
- 162.79451310634613
- 157.42766308784485
- 138.0334222316742
- 147.770122051239
- 144.77372246980667
- 145.92020744085312
- 148.29581302404404
- 133.4158899784088
- 147.78654062747955
- 138.87450885772705
- 163.97231423854828
- 150.3630421757698
- 166.29011458158493
- 128.73322302103043
- 161.53183525800705
- 147.7124274969101
- 142.16344594955444
- 165.41073989868164
- 144.0167914032936
- 138.36723828315735
- 157.27781736850739
- 144.09320902824402
- 160.0443547964096
- 144.07557725906372
- 151.55421620607376
- 156.03727406263351
- 164.41781508922577
- 129.2092169523239
- 136.32420098781586
- 146.82957482337952
- 140.15623211860657
- 145.75869941711426
- 145.27349889278412
- 140.90682309865952
- 139.08927726745605
- 130.8059754371643
- 141.71468418836594
- 141.53889894485474
- 141.41927284002304
- 150.55174934864044
- 134.82970148324966
- 148.33446890115738
- 140.2297624349594
- 170.34844839572906
- 160.7774401307106
- 133.32898128032684
- 135.99454402923584
- 128.9969466328621
- 145.1116508245468
- 147.8615906238556
- 134.16958737373352
- 132.8015171289444
- 145.22205823659897
- 159.07860922813416
- 132.11215901374817
- 145.66766649484634
- 136.00376373529434
train_accuracy:
- 0.218
- 0.004
- 0.091
- 0.0
- 0.85
- 0.47
- 0.0
- 0.388
- 0.062
- 0.586
- 0.481
- 0.256
- 0.525
- 0.647
- 0.0
- 0.485
- 0.544
- 0.336
- 0.618
- 0.508
- 0.385
- 0.557
- 0.43
- 0.453
- 0.455
- 0.495
- 0.604
- 0.86
- 0.15
- 0.857
- 0.697
- 0.808
- 0.514
- 0.362
- 0.675
- 0.9
- 0.7
- 0.838
- 0.417
- 0.208
- 0.753
- 0.695
- 0.95
- 0.388
- 0.466
- 0.271
- 0.692
- 0.476
- 0.933
- 0.862
- 0.369
- 0.57
- 0.597
- 0.573
- 0.895
- 0.55
- 0.07
- 0.8
- 0.954
- 0.808
- 0.656
- 0.85
- 0.546
- 0.628
- 0.671
- 0.233
- 0.864
- 0.666
- 0.663
- 0.65
- 0.695
- 0.455
- 0.471
- 0.609
- 0.77
- 0.409
- 0.35
- 0.953
- 0.71
- 0.817
- 0.817
- 0.63
- 0.321
- 0.803
- 0.892
- 0.695
- 0.629
- 0.883
- 0.982
- 0.894
- 0.75
- 0.567
- 0.608
- 0.638
- 0.55
- 0.803
- 0.592
- 0.73
- 0.869
- 0.75
train_loss:
- 1.192
- 0.754
- 0.672
- 0.556
- 0.589
- 0.507
- 0.536
- 0.503
- 0.44
- 0.445
- 0.475
- 0.347
- 0.373
- 0.454
- 0.324
- 0.429
- 0.362
- 0.475
- 0.411
- 0.313
- 0.418
- 0.439
- 0.453
- 0.426
- 0.389
- 0.39
- 0.392
- 0.368
- 0.361
- 0.363
- 0.399
- 0.317
- 0.388
- 0.39
- 0.287
- 0.442
- 0.298
- 0.319
- 0.313
- 0.364
- 0.355
- 0.364
- 0.367
- 0.392
- 0.381
- 0.284
- 0.329
- 0.413
- 0.379
- 0.359
- 0.399
- 0.329
- 0.352
- 0.326
- 0.318
- 0.345
- 0.292
- 0.32
- 0.346
- 0.315
- 0.332
- 0.349
- 0.349
- 0.353
- 0.342
- 0.328
- 0.284
- 0.376
- 0.347
- 0.277
- 0.419
- 0.282
- 0.342
- 0.343
- 0.31
- 0.281
- 0.343
- 0.332
- 0.368
- 0.295
- 0.37
- 0.334
- 0.353
- 0.365
- 0.307
- 0.313
- 0.374
- 0.217
- 0.338
- 0.305
- 0.365
- 0.261
- 0.331
- 0.353
- 0.358
- 0.361
- 0.347
- 0.288
- 0.334
- 0.273
unequal: 1
verbose: 1
