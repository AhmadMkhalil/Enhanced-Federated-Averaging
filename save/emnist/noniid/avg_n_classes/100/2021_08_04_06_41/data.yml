avg_train_accuracy: 0.02
avg_train_loss: 0.004
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1947340425531915
- 0.20702127659574468
- 0.2809042553191489
- 0.3949468085106383
- 0.3905851063829787
- 0.41452127659574467
- 0.4276063829787234
- 0.42648936170212765
- 0.4751595744680851
- 0.5089361702127659
- 0.543031914893617
- 0.5403191489361702
- 0.5049468085106383
- 0.5334574468085106
- 0.5708510638297872
- 0.4752127659574468
- 0.5218617021276596
- 0.5982446808510639
- 0.6241489361702127
- 0.5574468085106383
- 0.5820744680851064
- 0.5787234042553191
- 0.6197872340425532
- 0.5828191489361703
- 0.6222872340425532
- 0.6347340425531914
- 0.568031914893617
- 0.5578191489361702
- 0.601063829787234
- 0.5938829787234042
- 0.6346808510638298
- 0.5992553191489361
- 0.6272340425531915
- 0.601968085106383
- 0.6402127659574468
- 0.6521808510638298
- 0.637127659574468
- 0.6334574468085107
- 0.614468085106383
- 0.6725
- 0.6380851063829788
- 0.6716489361702128
- 0.5943617021276596
- 0.6421808510638298
- 0.6168617021276596
- 0.6567021276595745
- 0.6524468085106383
- 0.5723404255319149
- 0.5951595744680851
- 0.6407978723404255
- 0.6384042553191489
- 0.6431914893617021
- 0.6682446808510638
- 0.6634574468085106
- 0.6331382978723404
- 0.6431914893617021
- 0.6675531914893617
- 0.6835638297872341
- 0.6368085106382979
- 0.6867553191489362
- 0.6526595744680851
- 0.6668617021276596
- 0.6729787234042554
- 0.6560106382978723
- 0.663936170212766
- 0.6631914893617021
- 0.6478191489361702
- 0.6678191489361702
- 0.6401595744680851
- 0.7078723404255319
- 0.6961170212765957
- 0.685531914893617
- 0.6738297872340425
- 0.7003191489361702
- 0.7107978723404256
- 0.6857446808510639
- 0.6339361702127659
- 0.695531914893617
- 0.7302127659574468
- 0.7218617021276595
- 0.6706382978723404
- 0.6388297872340426
- 0.5923936170212766
- 0.6813297872340426
- 0.6692553191489362
- 0.6817021276595745
- 0.6821808510638298
- 0.658563829787234
- 0.6515425531914893
- 0.67
- 0.6583510638297873
- 0.658031914893617
- 0.6459574468085106
- 0.6446276595744681
- 0.6886702127659574
- 0.6503191489361703
- 0.6692021276595744
- 0.7093085106382979
- 0.6453191489361703
- 0.6470212765957447
test_loss_list:
- 529.3073375225067
- 455.4591910839081
- 413.2823405265808
- 346.8220217227936
- 325.5177153348923
- 296.43185472488403
- 262.6133282184601
- 269.6425213813782
- 252.8254941701889
- 225.54041063785553
- 207.68869984149933
- 214.091521859169
- 218.2748852968216
- 201.4354646205902
- 196.50675475597382
- 229.80714356899261
- 201.03759944438934
- 173.82603472471237
- 170.8272402882576
- 192.87740290164948
- 189.6319967508316
- 192.6741759777069
- 165.5398052930832
- 176.1978400349617
- 164.68770098686218
- 151.88772529363632
- 188.25749218463898
- 182.7061629295349
- 183.51629656553268
- 166.77300065755844
- 154.2631806731224
- 180.53339439630508
- 169.62866520881653
- 177.85682529211044
- 146.01842546463013
- 143.5177316069603
- 154.78425288200378
- 150.516015291214
- 161.88784664869308
- 141.7081055045128
- 156.24957412481308
- 150.77373957633972
- 155.33280158042908
- 148.7006943821907
- 159.1571456193924
- 153.770378947258
- 144.29549783468246
- 195.17616045475006
- 162.37196069955826
- 161.97745072841644
- 149.29778170585632
- 148.57218444347382
- 154.2499651312828
- 137.9282128214836
- 151.03803277015686
- 144.89320409297943
- 141.57429814338684
- 127.95581644773483
- 149.47655099630356
- 135.92082059383392
- 148.35181081295013
- 138.5832258462906
- 151.8232421875
- 145.8133282661438
- 141.44012135267258
- 144.98604542016983
- 166.79962766170502
- 142.03196436166763
- 157.9568554162979
- 120.20111411809921
- 137.76742619276047
- 126.90647304058075
- 131.06306266784668
- 121.50842010974884
- 126.98321855068207
- 130.73259592056274
- 144.47686797380447
- 124.94730198383331
- 115.25899577140808
- 119.04064548015594
- 140.7145128250122
- 158.60992485284805
- 178.31667643785477
- 147.71931052207947
- 132.29778760671616
- 128.78847312927246
- 142.70286387205124
- 135.95388370752335
- 145.73731923103333
- 131.66172868013382
- 149.00398111343384
- 137.43915289640427
- 134.88717913627625
- 170.52784579992294
- 127.9051942229271
- 133.79599630832672
- 139.47190701961517
- 127.40126824378967
- 141.1601264476776
- 152.2391992211342
train_accuracy:
- 0.07
- 0.135
- 0.25
- 0.356
- 0.23
- 0.75
- 0.088
- 0.01
- 0.562
- 0.394
- 0.822
- 0.867
- 0.775
- 0.694
- 0.711
- 0.304
- 0.197
- 0.883
- 0.1
- 0.594
- 0.1
- 0.356
- 0.65
- 0.879
- 0.438
- 0.529
- 0.084
- 0.931
- 0.37
- 0.679
- 0.819
- 0.775
- 0.729
- 0.815
- 0.528
- 0.844
- 0.2
- 0.561
- 0.854
- 0.584
- 0.635
- 0.732
- 0.564
- 0.628
- 0.731
- 0.01
- 0.825
- 0.895
- 0.918
- 0.844
- 0.257
- 0.606
- 0.912
- 0.45
- 0.818
- 0.119
- 0.587
- 0.8
- 0.936
- 0.208
- 0.883
- 0.93
- 0.95
- 0.35
- 0.843
- 0.883
- 0.481
- 0.365
- 0.571
- 0.8
- 0.775
- 0.65
- 0.509
- 0.5
- 0.692
- 0.578
- 0.7
- 0.909
- 0.712
- 0.675
- 0.867
- 0.731
- 0.425
- 0.27
- 0.839
- 0.754
- 0.867
- 0.029
- 0.803
- 0.693
- 0.931
- 0.538
- 0.433
- 0.417
- 0.554
- 0.679
- 0.569
- 0.654
- 0.836
- 0.02
train_loss:
- 1.083
- 0.735
- 0.582
- 0.635
- 0.562
- 0.643
- 0.525
- 0.466
- 0.483
- 0.445
- 0.489
- 0.428
- 0.524
- 0.376
- 0.385
- 0.413
- 0.395
- 0.387
- 0.399
- 0.387
- 0.411
- 0.348
- 0.43
- 0.46
- 0.4
- 0.419
- 0.4
- 0.374
- 0.364
- 0.481
- 0.294
- 0.395
- 0.373
- 0.387
- 0.389
- 0.391
- 0.283
- 0.353
- 0.378
- 0.336
- 0.299
- 0.389
- 0.37
- 0.336
- 0.41
- 0.29
- 0.315
- 0.319
- 0.41
- 0.325
- 0.362
- 0.305
- 0.361
- 0.34
- 0.358
- 0.353
- 0.375
- 0.23
- 0.41
- 0.378
- 0.34
- 0.311
- 0.319
- 0.371
- 0.37
- 0.326
- 0.256
- 0.285
- 0.262
- 0.364
- 0.317
- 0.364
- 0.294
- 0.361
- 0.294
- 0.334
- 0.267
- 0.412
- 0.385
- 0.316
- 0.324
- 0.342
- 0.342
- 0.352
- 0.359
- 0.28
- 0.285
- 0.314
- 0.36
- 0.376
- 0.35
- 0.278
- 0.352
- 0.267
- 0.316
- 0.303
- 0.359
- 0.393
- 0.338
- 0.351
unequal: 1
verbose: 1
