avg_train_accuracy: 0.9
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.14303191489361702
- 0.25622340425531914
- 0.28893617021276596
- 0.2927659574468085
- 0.43436170212765957
- 0.5626595744680851
- 0.5542021276595744
- 0.4212234042553191
- 0.470531914893617
- 0.48914893617021277
- 0.5157446808510638
- 0.5718085106382979
- 0.6118617021276596
- 0.5356914893617021
- 0.5427659574468086
- 0.5559042553191489
- 0.5525531914893617
- 0.6067021276595744
- 0.555
- 0.620531914893617
- 0.619095744680851
- 0.5875531914893617
- 0.5487234042553192
- 0.6107978723404255
- 0.6232978723404256
- 0.6097340425531915
- 0.6120212765957447
- 0.6212234042553192
- 0.6097872340425532
- 0.6382446808510638
- 0.6138297872340426
- 0.6200531914893617
- 0.6143085106382978
- 0.6439893617021276
- 0.6003723404255319
- 0.6348936170212766
- 0.5969148936170213
- 0.6486170212765957
- 0.6723936170212766
- 0.5513297872340426
- 0.5780851063829787
- 0.6232446808510639
- 0.6127127659574468
- 0.6130851063829788
- 0.5628191489361702
- 0.6210638297872341
- 0.6221808510638298
- 0.6378723404255319
- 0.6329255319148936
- 0.6677127659574468
- 0.690531914893617
- 0.6160106382978724
- 0.6247872340425532
- 0.7004787234042553
- 0.6220212765957447
- 0.71
- 0.5669148936170213
- 0.6454787234042553
- 0.6590425531914894
- 0.6310638297872341
- 0.5801595744680851
- 0.617872340425532
- 0.6468085106382979
- 0.6568085106382979
- 0.6126063829787234
- 0.6675
- 0.7013829787234043
- 0.6657978723404255
- 0.6057978723404255
- 0.6581382978723405
- 0.6421808510638298
- 0.6446276595744681
- 0.6609574468085107
- 0.6643617021276595
- 0.7168617021276595
- 0.7065957446808511
- 0.6087234042553191
- 0.6713297872340426
- 0.6569148936170213
- 0.708404255319149
- 0.6761170212765958
- 0.6962234042553191
- 0.7052659574468085
- 0.6602659574468085
- 0.6078191489361702
- 0.6406382978723404
- 0.6676063829787234
- 0.6607978723404255
- 0.6931382978723404
- 0.6726595744680851
- 0.6670212765957447
- 0.6713297872340426
- 0.6957978723404256
- 0.6886170212765957
- 0.670372340425532
- 0.6793085106382979
- 0.6518617021276596
- 0.6942021276595745
- 0.6727659574468086
- 0.6995744680851064
test_loss_list:
- 526.0802369117737
- 433.59320640563965
- 412.8028373718262
- 389.05095863342285
- 288.1060574054718
- 234.39829337596893
- 222.78915584087372
- 266.09831190109253
- 234.9686940908432
- 227.86235094070435
- 215.89400243759155
- 196.96247947216034
- 178.62744057178497
- 221.09331727027893
- 206.4859629869461
- 194.85325694084167
- 184.31520634889603
- 167.64576303958893
- 186.88170385360718
- 167.13132417201996
- 162.24095451831818
- 179.20529866218567
- 187.13566315174103
- 170.14589196443558
- 158.57594501972198
- 162.70834523439407
- 163.33143919706345
- 159.3726350069046
- 166.18264031410217
- 163.1631833910942
- 163.3547838330269
- 158.26160210371017
- 164.7420618534088
- 158.43571627140045
- 182.2266570329666
- 173.11657464504242
- 186.52421402931213
- 144.9052466750145
- 131.46982526779175
- 185.9047047495842
- 180.6249594092369
- 155.36433899402618
- 160.57021230459213
- 166.61723750829697
- 179.70347326993942
- 154.71924883127213
- 155.84264773130417
- 160.65165239572525
- 152.55923253297806
- 132.4409607052803
- 128.56467747688293
- 155.9874206185341
- 166.20600163936615
- 128.47050535678864
- 173.3558698296547
- 126.79167520999908
- 189.1771735548973
- 144.11087030172348
- 144.53995645046234
- 145.74996662139893
- 183.14761877059937
- 154.59474915266037
- 148.98111128807068
- 144.09946352243423
- 160.93842619657516
- 138.31692641973495
- 116.92395389080048
- 137.92137867212296
- 168.5222247838974
- 139.88395637273788
- 149.20266604423523
- 155.87372076511383
- 138.32072162628174
- 140.35598993301392
- 116.11703419685364
- 119.08502995967865
- 159.2972376346588
- 138.46748811006546
- 132.80914515256882
- 116.41453516483307
- 134.37681448459625
- 130.50306022167206
- 124.38505548238754
- 149.25972819328308
- 164.54510539770126
- 146.77634996175766
- 130.80039697885513
- 140.2012558579445
- 127.68970137834549
- 133.41143095493317
- 142.02310401201248
- 142.86977714300156
- 135.93956154584885
- 123.67978835105896
- 134.32973736524582
- 134.5310744047165
- 134.57408213615417
- 122.60235452651978
- 150.53583949804306
- 116.59625566005707
train_accuracy:
- 0.275
- 0.536
- 0.0
- 0.864
- 0.492
- 0.465
- 0.75
- 0.121
- 0.39
- 0.691
- 0.0
- 0.65
- 0.458
- 0.388
- 0.667
- 0.385
- 0.8
- 0.729
- 0.711
- 0.717
- 0.83
- 0.646
- 0.907
- 0.377
- 0.729
- 0.804
- 0.617
- 0.504
- 0.92
- 0.85
- 0.681
- 0.718
- 0.65
- 0.683
- 0.478
- 0.38
- 0.432
- 0.944
- 0.945
- 0.12
- 0.29
- 0.325
- 0.105
- 0.446
- 0.677
- 0.765
- 0.669
- 0.279
- 0.113
- 0.786
- 0.854
- 0.767
- 0.75
- 0.94
- 0.635
- 0.836
- 0.363
- 0.615
- 0.895
- 0.02
- 0.661
- 0.533
- 0.319
- 0.632
- 0.593
- 0.722
- 0.658
- 0.779
- 0.875
- 0.892
- 0.5
- 0.797
- 0.575
- 0.945
- 0.823
- 0.335
- 0.0
- 0.389
- 0.125
- 0.731
- 0.363
- 0.815
- 0.844
- 0.409
- 0.804
- 0.65
- 0.779
- 0.238
- 0.889
- 0.915
- 0.61
- 0.0
- 0.963
- 0.732
- 0.925
- 0.404
- 0.956
- 0.5
- 0.775
- 0.9
train_loss:
- 1.189
- 0.72
- 0.583
- 0.529
- 0.555
- 0.534
- 0.534
- 0.577
- 0.499
- 0.449
- 0.383
- 0.41
- 0.43
- 0.385
- 0.386
- 0.434
- 0.478
- 0.496
- 0.409
- 0.476
- 0.396
- 0.447
- 0.311
- 0.313
- 0.382
- 0.438
- 0.378
- 0.372
- 0.367
- 0.38
- 0.384
- 0.428
- 0.427
- 0.257
- 0.363
- 0.382
- 0.295
- 0.332
- 0.299
- 0.277
- 0.318
- 0.33
- 0.324
- 0.394
- 0.369
- 0.397
- 0.401
- 0.344
- 0.311
- 0.385
- 0.349
- 0.356
- 0.316
- 0.332
- 0.333
- 0.365
- 0.374
- 0.393
- 0.409
- 0.325
- 0.331
- 0.385
- 0.348
- 0.338
- 0.342
- 0.354
- 0.376
- 0.34
- 0.355
- 0.368
- 0.341
- 0.351
- 0.43
- 0.347
- 0.312
- 0.292
- 0.337
- 0.294
- 0.363
- 0.306
- 0.248
- 0.331
- 0.314
- 0.337
- 0.329
- 0.287
- 0.338
- 0.331
- 0.342
- 0.346
- 0.313
- 0.335
- 0.276
- 0.364
- 0.316
- 0.332
- 0.286
- 0.327
- 0.251
- 0.287
unequal: 1
verbose: 1
