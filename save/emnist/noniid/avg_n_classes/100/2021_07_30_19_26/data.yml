avg_train_accuracy: 0.436
avg_train_loss: 0.004
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.17117021276595745
- 0.20920212765957447
- 0.3121276595744681
- 0.3389893617021277
- 0.47882978723404257
- 0.47404255319148936
- 0.4920744680851064
- 0.5087234042553191
- 0.5075531914893617
- 0.5287765957446808
- 0.5077127659574469
- 0.5726595744680851
- 0.5307446808510639
- 0.5395212765957447
- 0.5323404255319149
- 0.5744148936170212
- 0.5509042553191489
- 0.5629255319148936
- 0.6196276595744681
- 0.6399468085106383
- 0.553936170212766
- 0.6520212765957447
- 0.6350531914893617
- 0.6039893617021277
- 0.6066489361702128
- 0.6456914893617022
- 0.5898936170212766
- 0.5814893617021276
- 0.578404255319149
- 0.5770212765957446
- 0.6101063829787234
- 0.641436170212766
- 0.6094148936170213
- 0.5914893617021276
- 0.6527659574468085
- 0.6286702127659575
- 0.6728723404255319
- 0.6770212765957446
- 0.6704787234042553
- 0.6497340425531914
- 0.6554787234042553
- 0.6326063829787234
- 0.6620212765957447
- 0.5920212765957447
- 0.6464893617021277
- 0.6334042553191489
- 0.648936170212766
- 0.6716489361702128
- 0.6786170212765957
- 0.6527127659574468
- 0.6273936170212766
- 0.626968085106383
- 0.648936170212766
- 0.6318085106382979
- 0.6336702127659575
- 0.6358510638297873
- 0.6210106382978723
- 0.6125
- 0.6581914893617021
- 0.6808510638297872
- 0.6645212765957447
- 0.654468085106383
- 0.6452659574468085
- 0.6498936170212766
- 0.6568617021276596
- 0.67
- 0.6856914893617021
- 0.6279787234042553
- 0.679468085106383
- 0.7134574468085106
- 0.6650531914893617
- 0.658563829787234
- 0.615
- 0.670372340425532
- 0.66
- 0.6681914893617021
- 0.6876595744680851
- 0.6595744680851063
- 0.6963829787234043
- 0.6862765957446808
- 0.6251063829787235
- 0.683936170212766
- 0.694627659574468
- 0.6706382978723404
- 0.6663297872340426
- 0.6772872340425532
- 0.668563829787234
- 0.6583510638297873
- 0.6852127659574468
- 0.643031914893617
- 0.6409574468085106
- 0.6989893617021277
- 0.6976063829787233
- 0.7131914893617022
- 0.6924468085106383
- 0.6734042553191489
- 0.6460638297872341
- 0.7140425531914893
- 0.6763829787234042
- 0.648936170212766
test_loss_list:
- 527.9088110923767
- 469.55477714538574
- 396.53807973861694
- 352.2777168750763
- 280.9197289943695
- 257.35905623435974
- 256.53014945983887
- 248.29979836940765
- 231.8485746383667
- 207.0854378938675
- 218.83234882354736
- 190.7871688604355
- 208.29390001296997
- 201.77586257457733
- 217.41513538360596
- 181.2899348139763
- 194.50827062129974
- 187.80904561281204
- 166.4914190173149
- 159.41703182458878
- 190.2975445985794
- 150.81456404924393
- 151.35521978139877
- 164.28270703554153
- 172.60884183645248
- 146.82058835029602
- 178.77831202745438
- 174.7472870349884
- 186.9629727602005
- 186.23848128318787
- 167.6927137374878
- 161.9550462961197
- 170.114466547966
- 165.70928633213043
- 158.25008940696716
- 159.27282601594925
- 143.19329607486725
- 146.04126280546188
- 140.4859880208969
- 150.13469779491425
- 152.8412190079689
- 152.9346290230751
- 146.039972782135
- 186.21427595615387
- 151.1461346745491
- 154.20083683729172
- 142.1393033862114
- 128.9692457318306
- 138.91258031129837
- 151.1668779850006
- 163.0341792702675
- 162.6789367198944
- 150.15454798936844
- 152.42952978610992
- 141.522869348526
- 147.80573493242264
- 178.62709492444992
- 157.3980346918106
- 147.63311445713043
- 130.13873100280762
- 139.58007234334946
- 149.90657943487167
- 154.25291675329208
- 157.26672261953354
- 142.83244276046753
- 137.80595457553864
- 129.4813945889473
- 156.2437725663185
- 148.0320889353752
- 124.70830148458481
- 139.72391057014465
- 135.58925247192383
- 155.2607477903366
- 130.73090624809265
- 135.6766641139984
- 141.51280039548874
- 130.27078860998154
- 139.88081169128418
- 128.13630878925323
- 135.2214103937149
- 154.9432287812233
- 135.26831048727036
- 129.20297294855118
- 145.99916368722916
- 145.91043943166733
- 129.63480961322784
- 140.2898726463318
- 148.17492127418518
- 132.0269392132759
- 155.51869583129883
- 145.44495928287506
- 124.2919112443924
- 119.65646171569824
- 124.73776072263718
- 127.99560624361038
- 146.35821843147278
- 160.26427006721497
- 122.86925256252289
- 124.87809562683105
- 144.0042600631714
train_accuracy:
- 0.292
- 0.189
- 0.281
- 0.336
- 0.341
- 0.125
- 0.225
- 0.0
- 0.417
- 0.754
- 0.142
- 0.543
- 0.371
- 0.917
- 0.067
- 0.289
- 0.536
- 0.894
- 0.967
- 0.695
- 0.781
- 0.84
- 0.468
- 0.583
- 0.565
- 0.37
- 0.833
- 0.7
- 0.503
- 0.457
- 0.783
- 0.55
- 0.672
- 0.556
- 0.75
- 0.92
- 0.632
- 0.671
- 0.125
- 0.757
- 0.825
- 0.786
- 0.936
- 0.83
- 0.845
- 0.26
- 0.03
- 0.538
- 0.729
- 0.461
- 0.741
- 0.872
- 0.575
- 0.167
- 0.933
- 0.9
- 0.45
- 0.918
- 0.539
- 0.954
- 0.888
- 0.975
- 0.342
- 0.925
- 0.911
- 0.55
- 0.327
- 0.661
- 0.938
- 0.771
- 0.625
- 0.628
- 0.725
- 0.737
- 0.675
- 0.657
- 0.92
- 0.832
- 0.888
- 0.562
- 0.628
- 0.961
- 0.158
- 0.647
- 0.345
- 0.495
- 0.453
- 0.271
- 0.584
- 0.558
- 0.87
- 0.811
- 0.85
- 0.94
- 0.763
- 0.8
- 0.91
- 0.915
- 0.769
- 0.436
train_loss:
- 1.172
- 0.811
- 0.707
- 0.662
- 0.661
- 0.578
- 0.455
- 0.447
- 0.477
- 0.475
- 0.451
- 0.347
- 0.466
- 0.461
- 0.356
- 0.469
- 0.406
- 0.371
- 0.378
- 0.419
- 0.481
- 0.376
- 0.436
- 0.389
- 0.434
- 0.407
- 0.437
- 0.42
- 0.446
- 0.391
- 0.27
- 0.373
- 0.411
- 0.391
- 0.373
- 0.311
- 0.349
- 0.382
- 0.341
- 0.324
- 0.369
- 0.309
- 0.347
- 0.351
- 0.345
- 0.372
- 0.394
- 0.367
- 0.336
- 0.419
- 0.356
- 0.36
- 0.329
- 0.363
- 0.334
- 0.352
- 0.337
- 0.351
- 0.376
- 0.377
- 0.444
- 0.408
- 0.334
- 0.451
- 0.327
- 0.348
- 0.335
- 0.35
- 0.389
- 0.331
- 0.388
- 0.339
- 0.356
- 0.378
- 0.354
- 0.364
- 0.373
- 0.318
- 0.421
- 0.422
- 0.427
- 0.373
- 0.379
- 0.321
- 0.328
- 0.362
- 0.355
- 0.34
- 0.326
- 0.32
- 0.376
- 0.338
- 0.335
- 0.345
- 0.311
- 0.324
- 0.268
- 0.34
- 0.385
- 0.356
unequal: 1
verbose: 1
