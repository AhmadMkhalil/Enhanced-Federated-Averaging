avg_train_accuracy: 0.558
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.17287234042553193
- 0.23542553191489363
- 0.23436170212765958
- 0.3772872340425532
- 0.4375531914893617
- 0.5042021276595745
- 0.4832446808510638
- 0.5196808510638298
- 0.4257446808510638
- 0.4684042553191489
- 0.5423936170212766
- 0.5295212765957447
- 0.5399468085106383
- 0.49127659574468086
- 0.6518085106382979
- 0.6226595744680851
- 0.6361702127659574
- 0.5701063829787234
- 0.6142553191489362
- 0.6260638297872341
- 0.6468085106382979
- 0.6120744680851063
- 0.606436170212766
- 0.5775531914893617
- 0.6270744680851064
- 0.5771276595744681
- 0.6193085106382978
- 0.5650531914893617
- 0.6129787234042553
- 0.6271808510638298
- 0.6382446808510638
- 0.6312765957446809
- 0.6317021276595745
- 0.5549468085106383
- 0.5672872340425532
- 0.6262234042553192
- 0.6238297872340426
- 0.5994680851063829
- 0.6179255319148936
- 0.6370212765957447
- 0.6257446808510638
- 0.6351595744680851
- 0.6468085106382979
- 0.6416489361702128
- 0.6155851063829787
- 0.6481382978723405
- 0.6481382978723405
- 0.6728723404255319
- 0.6647872340425532
- 0.6683510638297873
- 0.6273936170212766
- 0.646595744680851
- 0.6323936170212766
- 0.6213829787234042
- 0.619095744680851
- 0.683404255319149
- 0.6673404255319149
- 0.6713297872340426
- 0.6925
- 0.6639893617021276
- 0.6437234042553192
- 0.6263297872340425
- 0.6027127659574468
- 0.6754787234042553
- 0.6684042553191489
- 0.6333510638297872
- 0.6434042553191489
- 0.6089893617021277
- 0.656063829787234
- 0.6071808510638298
- 0.663936170212766
- 0.7123404255319149
- 0.6796276595744681
- 0.6758510638297872
- 0.6834574468085106
- 0.612659574468085
- 0.6664893617021277
- 0.6863297872340426
- 0.6813297872340426
- 0.6567021276595745
- 0.6512765957446809
- 0.6326063829787234
- 0.6316489361702128
- 0.7079255319148936
- 0.6761702127659575
- 0.695
- 0.6730851063829787
- 0.6551063829787234
- 0.650904255319149
- 0.6531382978723405
- 0.6672340425531915
- 0.6850531914893617
- 0.6504255319148936
- 0.6649468085106383
- 0.6986702127659574
- 0.6283510638297872
- 0.6504787234042553
- 0.6638297872340425
- 0.698936170212766
- 0.6903191489361702
test_loss_list:
- 525.6879940032959
- 431.46956992149353
- 407.61379384994507
- 339.77946650981903
- 287.3899414539337
- 252.575674533844
- 243.98852491378784
- 227.77360343933105
- 263.2648376226425
- 235.27760863304138
- 225.14099383354187
- 209.5599581003189
- 200.19313263893127
- 237.4953659772873
- 161.27781462669373
- 158.80045729875565
- 165.16090267896652
- 192.53969848155975
- 176.07993584871292
- 169.09631669521332
- 159.88107579946518
- 169.03334099054337
- 180.41329061985016
- 195.22197782993317
- 159.4657942056656
- 189.4561843276024
- 157.569337785244
- 191.991897046566
- 178.95461761951447
- 175.89021229743958
- 162.69848400354385
- 147.87174183130264
- 158.8085716366768
- 211.52629232406616
- 186.30024755001068
- 149.6477671265602
- 155.94104492664337
- 164.29233479499817
- 157.63046222925186
- 166.55494332313538
- 165.3949624300003
- 145.9553444981575
- 163.87568944692612
- 159.5143826007843
- 169.4579702615738
- 156.3969914317131
- 143.25639748573303
- 135.9345343708992
- 135.7455893754959
- 136.88558840751648
- 159.06161004304886
- 146.6388368010521
- 148.5750551223755
- 161.3071398139
- 151.01811242103577
- 147.82545059919357
- 142.7373253107071
- 137.21185052394867
- 126.53855663537979
- 150.798969745636
- 145.46299189329147
- 169.0608293414116
- 170.43605291843414
- 138.2894926071167
- 146.86336421966553
- 150.42509984970093
- 154.22389388084412
- 175.58506095409393
- 148.86533999443054
- 155.64114826917648
- 142.66572576761246
- 132.52062278985977
- 132.16442722082138
- 139.40206003189087
- 144.50412476062775
- 173.3897683620453
- 142.16406857967377
- 133.95294630527496
- 144.32264399528503
- 137.1123265028
- 148.03912436962128
- 146.25909996032715
- 156.5012634396553
- 127.06078898906708
- 140.63204163312912
- 141.61741012334824
- 138.47775411605835
- 147.47709798812866
- 152.36701744794846
- 139.77103394269943
- 135.4490241408348
- 131.4002005457878
- 160.88105034828186
- 140.4073222875595
- 129.6292703151703
- 163.3694884777069
- 146.6693235039711
- 142.60926234722137
- 127.29978489875793
- 135.8889491558075
train_accuracy:
- 0.0
- 0.34
- 0.0
- 0.361
- 0.79
- 0.708
- 0.531
- 0.842
- 0.525
- 0.267
- 0.647
- 0.658
- 0.329
- 0.503
- 0.925
- 0.339
- 0.667
- 0.367
- 0.96
- 0.455
- 0.4
- 0.817
- 0.907
- 0.765
- 0.91
- 0.619
- 0.75
- 0.497
- 0.475
- 0.803
- 0.818
- 0.958
- 0.879
- 0.0
- 0.618
- 0.729
- 0.5
- 0.756
- 0.617
- 0.92
- 0.725
- 0.708
- 0.606
- 0.179
- 0.444
- 0.74
- 0.977
- 0.647
- 0.586
- 0.681
- 0.579
- 0.347
- 0.478
- 0.462
- 0.7
- 0.663
- 0.95
- 0.689
- 0.566
- 0.945
- 0.92
- 0.908
- 0.477
- 0.647
- 0.579
- 0.908
- 0.335
- 0.35
- 0.955
- 0.9
- 0.747
- 0.396
- 0.782
- 0.933
- 0.377
- 0.95
- 0.658
- 0.658
- 0.934
- 0.636
- 0.915
- 0.527
- 0.686
- 0.525
- 0.95
- 0.644
- 0.912
- 0.317
- 0.392
- 0.419
- 0.778
- 0.881
- 0.854
- 0.914
- 0.885
- 0.889
- 0.994
- 0.87
- 0.466
- 0.558
train_loss:
- 1.303
- 0.795
- 0.586
- 0.549
- 0.531
- 0.569
- 0.462
- 0.488
- 0.397
- 0.483
- 0.52
- 0.487
- 0.502
- 0.336
- 0.414
- 0.425
- 0.435
- 0.464
- 0.352
- 0.387
- 0.372
- 0.393
- 0.368
- 0.423
- 0.377
- 0.378
- 0.331
- 0.374
- 0.311
- 0.281
- 0.297
- 0.423
- 0.332
- 0.273
- 0.333
- 0.364
- 0.383
- 0.341
- 0.354
- 0.346
- 0.289
- 0.313
- 0.384
- 0.414
- 0.365
- 0.29
- 0.385
- 0.412
- 0.38
- 0.381
- 0.339
- 0.415
- 0.357
- 0.373
- 0.308
- 0.372
- 0.306
- 0.362
- 0.244
- 0.356
- 0.271
- 0.327
- 0.312
- 0.405
- 0.377
- 0.307
- 0.3
- 0.305
- 0.32
- 0.348
- 0.335
- 0.315
- 0.334
- 0.242
- 0.315
- 0.305
- 0.3
- 0.314
- 0.326
- 0.289
- 0.294
- 0.268
- 0.352
- 0.257
- 0.268
- 0.368
- 0.321
- 0.324
- 0.351
- 0.346
- 0.353
- 0.317
- 0.342
- 0.284
- 0.399
- 0.313
- 0.296
- 0.373
- 0.355
- 0.284
unequal: 1
verbose: 1
