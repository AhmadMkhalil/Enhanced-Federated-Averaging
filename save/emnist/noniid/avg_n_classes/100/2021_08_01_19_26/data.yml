avg_train_accuracy: 0.388
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1352659574468085
- 0.2971276595744681
- 0.3098936170212766
- 0.4028191489361702
- 0.3993617021276596
- 0.5340957446808511
- 0.47925531914893615
- 0.49946808510638296
- 0.526436170212766
- 0.5315425531914894
- 0.5215425531914893
- 0.5134042553191489
- 0.5814893617021276
- 0.5368085106382978
- 0.5887234042553191
- 0.5677659574468085
- 0.6580851063829787
- 0.5667021276595745
- 0.5955851063829787
- 0.5386170212765957
- 0.5946276595744681
- 0.6087765957446809
- 0.5845212765957447
- 0.5986702127659574
- 0.6244148936170213
- 0.6488829787234043
- 0.602872340425532
- 0.5949468085106383
- 0.6122340425531915
- 0.5454255319148936
- 0.6214893617021277
- 0.5960106382978724
- 0.5518617021276596
- 0.5728723404255319
- 0.6171276595744681
- 0.6365957446808511
- 0.6025531914893617
- 0.6340957446808511
- 0.6473404255319148
- 0.6139361702127659
- 0.600531914893617
- 0.6723936170212766
- 0.6006382978723405
- 0.683936170212766
- 0.637127659574468
- 0.647127659574468
- 0.6990425531914893
- 0.6093617021276596
- 0.6505319148936171
- 0.6377659574468085
- 0.6482446808510638
- 0.6423936170212766
- 0.6711702127659575
- 0.6616489361702128
- 0.6883510638297873
- 0.6879787234042554
- 0.7176595744680851
- 0.6488297872340425
- 0.6196808510638298
- 0.6569680851063829
- 0.6761170212765958
- 0.6677127659574468
- 0.6465425531914893
- 0.6202127659574468
- 0.6413829787234042
- 0.6768085106382978
- 0.6726063829787234
- 0.679468085106383
- 0.6713297872340426
- 0.6814893617021277
- 0.6605319148936171
- 0.6324468085106383
- 0.6406914893617022
- 0.6167553191489362
- 0.6928191489361702
- 0.651595744680851
- 0.6663829787234042
- 0.7014361702127659
- 0.6163297872340425
- 0.6823404255319149
- 0.636436170212766
- 0.7254787234042553
- 0.6466489361702128
- 0.6528191489361702
- 0.6752127659574468
- 0.6520212765957447
- 0.6407446808510638
- 0.6987234042553192
- 0.6257446808510638
- 0.6538297872340425
- 0.6786170212765957
- 0.6869148936170213
- 0.6806382978723404
- 0.6569680851063829
- 0.7010106382978724
- 0.6677659574468086
- 0.6736702127659574
- 0.6131382978723404
- 0.6481382978723405
- 0.6395212765957446
test_loss_list:
- 527.8477201461792
- 437.98112320899963
- 385.6824390888214
- 335.3684391975403
- 316.8747502565384
- 251.60251820087433
- 253.551007270813
- 246.9460051059723
- 236.24401891231537
- 229.22180604934692
- 229.21885561943054
- 218.67865633964539
- 183.37201380729675
- 214.46868526935577
- 198.17727959156036
- 194.8288290500641
- 159.4008428454399
- 195.08693206310272
- 177.8090934753418
- 209.47854363918304
- 171.7298453450203
- 161.04093408584595
- 179.07087182998657
- 172.26679253578186
- 174.26664662361145
- 155.52255713939667
- 167.6089705824852
- 176.21261513233185
- 160.59077751636505
- 180.8142312169075
- 156.87915474176407
- 175.7492098212242
- 198.59500843286514
- 195.98176491260529
- 165.0600407719612
- 146.72361552715302
- 164.60859936475754
- 157.3504118323326
- 151.23742771148682
- 169.5979019999504
- 177.02898466587067
- 143.97591996192932
- 175.7403153181076
- 138.17957669496536
- 153.84637117385864
- 142.50039142370224
- 134.47664994001389
- 171.14931470155716
- 147.71438366174698
- 149.72533082962036
- 147.93033784627914
- 152.16022258996964
- 144.8041422367096
- 148.1700777411461
- 133.45013135671616
- 133.74621081352234
- 123.8344869017601
- 141.04692977666855
- 161.98494416475296
- 166.42141675949097
- 149.1203486919403
- 143.6660082936287
- 150.72025829553604
- 163.0752671957016
- 153.46188688278198
- 150.40948098897934
- 140.49684756994247
- 132.0302633047104
- 135.25186783075333
- 129.39831000566483
- 148.0895410180092
- 155.6582968235016
- 152.57854843139648
- 169.53149384260178
- 134.49802124500275
- 161.2264176607132
- 139.7636906504631
- 137.87223333120346
- 155.74679952859879
- 139.18106591701508
- 148.4608924984932
- 121.75928872823715
- 154.21263587474823
- 145.55212646722794
- 136.341603577137
- 143.78507953882217
- 151.32792794704437
- 121.5250836610794
- 155.1840808391571
- 142.45663183927536
- 133.86557918787003
- 132.94941902160645
- 139.81294578313828
- 147.2981351017952
- 124.25481587648392
- 145.95348531007767
- 139.65435367822647
- 164.70815819501877
- 144.64419895410538
- 152.26553809642792
train_accuracy:
- 0.233
- 0.371
- 0.812
- 0.396
- 0.687
- 0.867
- 0.318
- 0.682
- 0.661
- 0.18
- 0.904
- 0.64
- 0.43
- 0.059
- 0.44
- 0.9
- 0.286
- 0.813
- 0.916
- 0.775
- 0.786
- 0.667
- 0.85
- 0.912
- 0.85
- 0.832
- 0.877
- 0.438
- 0.6
- 0.615
- 0.85
- 0.85
- 0.803
- 0.688
- 0.2
- 0.626
- 0.967
- 0.775
- 0.521
- 0.656
- 0.439
- 0.685
- 0.746
- 0.697
- 0.938
- 0.98
- 0.804
- 0.767
- 0.525
- 0.781
- 0.958
- 0.509
- 0.843
- 0.825
- 0.75
- 0.283
- 0.978
- 0.81
- 0.9
- 0.1
- 0.567
- 0.279
- 0.464
- 0.474
- 0.686
- 0.391
- 0.62
- 0.81
- 0.921
- 0.81
- 0.675
- 0.89
- 0.255
- 0.36
- 0.423
- 0.917
- 0.321
- 0.75
- 0.487
- 0.9
- 0.55
- 0.874
- 0.52
- 0.655
- 0.142
- 0.05
- 0.539
- 0.667
- 0.683
- 0.592
- 0.85
- 0.981
- 0.929
- 0.604
- 0.83
- 0.891
- 0.833
- 0.279
- 0.406
- 0.388
train_loss:
- 1.158
- 0.832
- 0.676
- 0.572
- 0.478
- 0.471
- 0.497
- 0.464
- 0.411
- 0.363
- 0.446
- 0.514
- 0.417
- 0.473
- 0.371
- 0.385
- 0.463
- 0.418
- 0.405
- 0.347
- 0.343
- 0.368
- 0.549
- 0.341
- 0.34
- 0.306
- 0.32
- 0.365
- 0.338
- 0.389
- 0.391
- 0.386
- 0.461
- 0.351
- 0.393
- 0.422
- 0.325
- 0.327
- 0.362
- 0.387
- 0.375
- 0.34
- 0.332
- 0.358
- 0.319
- 0.357
- 0.411
- 0.312
- 0.365
- 0.401
- 0.3
- 0.449
- 0.355
- 0.343
- 0.293
- 0.383
- 0.345
- 0.332
- 0.3
- 0.282
- 0.337
- 0.401
- 0.385
- 0.338
- 0.317
- 0.274
- 0.268
- 0.293
- 0.341
- 0.373
- 0.387
- 0.323
- 0.28
- 0.345
- 0.392
- 0.254
- 0.257
- 0.331
- 0.285
- 0.339
- 0.315
- 0.374
- 0.298
- 0.305
- 0.413
- 0.303
- 0.351
- 0.365
- 0.343
- 0.372
- 0.332
- 0.28
- 0.322
- 0.349
- 0.347
- 0.233
- 0.305
- 0.292
- 0.348
- 0.35
unequal: 1
verbose: 1
