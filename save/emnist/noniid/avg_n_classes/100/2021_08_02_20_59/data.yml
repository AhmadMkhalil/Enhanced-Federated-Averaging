avg_train_accuracy: 0.921
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.095
- 0.27430851063829786
- 0.3460106382978723
- 0.3175
- 0.495
- 0.4999468085106383
- 0.4850531914893617
- 0.5237765957446808
- 0.550159574468085
- 0.5141489361702127
- 0.5127659574468085
- 0.5464361702127659
- 0.5687765957446809
- 0.5105851063829787
- 0.6221276595744681
- 0.5370212765957447
- 0.5525531914893617
- 0.625904255319149
- 0.5451063829787234
- 0.5709042553191489
- 0.5707446808510638
- 0.5953723404255319
- 0.6363297872340425
- 0.5838829787234042
- 0.6666489361702128
- 0.5560106382978723
- 0.6334042553191489
- 0.568404255319149
- 0.6027127659574468
- 0.5663297872340426
- 0.6396808510638298
- 0.6638829787234043
- 0.6243085106382978
- 0.6114893617021276
- 0.6247872340425532
- 0.6116489361702128
- 0.6184574468085107
- 0.6038297872340426
- 0.5781382978723404
- 0.6705319148936171
- 0.6235106382978723
- 0.64
- 0.6343617021276595
- 0.6195212765957446
- 0.6368617021276596
- 0.6403191489361703
- 0.6420212765957447
- 0.6149468085106383
- 0.6039893617021277
- 0.6262234042553192
- 0.6406914893617022
- 0.5993085106382978
- 0.654468085106383
- 0.6424468085106383
- 0.6514893617021277
- 0.6444148936170213
- 0.6712234042553191
- 0.6588297872340425
- 0.6448404255319149
- 0.5953723404255319
- 0.6821276595744681
- 0.6403191489361703
- 0.6329787234042553
- 0.6525531914893618
- 0.6714361702127659
- 0.6468617021276596
- 0.6467553191489361
- 0.6819148936170213
- 0.6795212765957447
- 0.6401063829787234
- 0.6140425531914894
- 0.685531914893617
- 0.6179787234042553
- 0.6811170212765958
- 0.71
- 0.6254255319148936
- 0.594095744680851
- 0.6905851063829788
- 0.6933510638297873
- 0.7082446808510638
- 0.6914893617021277
- 0.6461702127659574
- 0.676063829787234
- 0.678031914893617
- 0.6373936170212766
- 0.6370744680851064
- 0.6543617021276595
- 0.6624468085106383
- 0.6676595744680851
- 0.6131382978723404
- 0.625531914893617
- 0.693936170212766
- 0.7360106382978724
- 0.7089893617021277
- 0.7177127659574468
- 0.7104255319148937
- 0.7249468085106383
- 0.698031914893617
- 0.7129255319148936
- 0.683031914893617
test_loss_list:
- 533.7847330570221
- 430.06585001945496
- 362.2995800971985
- 362.33922958374023
- 266.5992614030838
- 249.21015048027039
- 249.18924343585968
- 230.29699909687042
- 222.43007040023804
- 231.19832456111908
- 228.62632381916046
- 212.06065905094147
- 189.18480587005615
- 205.8924684524536
- 173.56140679121017
- 196.28756380081177
- 193.74534678459167
- 161.04468148946762
- 189.5933728814125
- 192.34486627578735
- 179.12924164533615
- 167.72162342071533
- 161.06789648532867
- 182.62508088350296
- 155.44858157634735
- 189.02311444282532
- 155.15426605939865
- 186.683917760849
- 165.88235360383987
- 174.8029107451439
- 150.85756486654282
- 145.05196475982666
- 153.16083550453186
- 156.9252033829689
- 169.26295536756516
- 160.60186576843262
- 160.90112406015396
- 168.85299342870712
- 178.53070896863937
- 145.46008223295212
- 156.83406674861908
- 148.04200768470764
- 152.62538415193558
- 158.82840430736542
- 145.94928354024887
- 156.1316700577736
- 148.8814188838005
- 164.25939989089966
- 166.76878690719604
- 171.94804924726486
- 150.24144840240479
- 164.3439700603485
- 142.96278083324432
- 157.40719681978226
- 149.98338174819946
- 141.21279513835907
- 139.93392878770828
- 144.9372701048851
- 141.90489226579666
- 169.8416347503662
- 132.16376298666
- 147.42994791269302
- 162.52703821659088
- 152.2594866156578
- 138.83903747797012
- 146.0637719631195
- 138.05950164794922
- 136.67159062623978
- 133.83782839775085
- 151.8231217265129
- 160.10446685552597
- 140.3426360487938
- 158.8487730026245
- 133.74233973026276
- 126.55277901887894
- 162.7681691646576
- 172.88834458589554
- 140.04794877767563
- 133.02030336856842
- 131.2914931178093
- 132.4691521525383
- 167.43994623422623
- 142.26539319753647
- 136.7762839794159
- 152.21301817893982
- 167.80912119150162
- 156.12896537780762
- 150.50069564580917
- 134.20796871185303
- 159.4974695444107
- 148.3015176653862
- 129.57055801153183
- 112.45380318164825
- 120.49683821201324
- 123.98378682136536
- 122.20965474843979
- 115.08125287294388
- 124.03587180376053
- 121.81567943096161
- 135.13476711511612
train_accuracy:
- 0.108
- 0.117
- 0.657
- 0.082
- 0.16
- 0.85
- 0.012
- 0.517
- 0.417
- 0.439
- 0.615
- 0.307
- 0.02
- 0.347
- 0.497
- 0.881
- 0.747
- 0.822
- 0.0
- 0.758
- 0.223
- 0.325
- 0.828
- 0.76
- 0.734
- 0.85
- 0.821
- 0.758
- 0.363
- 0.95
- 0.644
- 0.542
- 0.594
- 0.921
- 0.597
- 0.547
- 0.706
- 0.797
- 0.8
- 0.842
- 0.918
- 0.75
- 0.947
- 0.289
- 0.875
- 0.895
- 0.743
- 0.605
- 0.53
- 0.311
- 0.025
- 0.0
- 0.474
- 0.8
- 0.671
- 0.8
- 0.957
- 0.25
- 0.467
- 0.614
- 0.421
- 0.712
- 0.5
- 0.0
- 0.542
- 0.715
- 0.721
- 0.0
- 0.544
- 0.806
- 0.674
- 0.775
- 0.746
- 0.815
- 0.0
- 0.835
- 0.564
- 0.921
- 0.659
- 0.761
- 0.5
- 0.842
- 0.631
- 0.028
- 0.756
- 0.819
- 0.856
- 0.564
- 0.03
- 0.397
- 0.403
- 0.913
- 0.897
- 0.831
- 0.929
- 0.721
- 0.95
- 0.822
- 0.843
- 0.921
train_loss:
- 1.205
- 0.92
- 0.691
- 0.579
- 0.605
- 0.541
- 0.421
- 0.566
- 0.491
- 0.452
- 0.601
- 0.389
- 0.393
- 0.504
- 0.47
- 0.396
- 0.411
- 0.444
- 0.368
- 0.471
- 0.416
- 0.406
- 0.406
- 0.359
- 0.437
- 0.398
- 0.38
- 0.366
- 0.433
- 0.366
- 0.385
- 0.364
- 0.383
- 0.44
- 0.402
- 0.432
- 0.373
- 0.421
- 0.326
- 0.335
- 0.346
- 0.339
- 0.376
- 0.379
- 0.397
- 0.381
- 0.378
- 0.285
- 0.36
- 0.355
- 0.386
- 0.309
- 0.405
- 0.382
- 0.34
- 0.35
- 0.274
- 0.258
- 0.389
- 0.378
- 0.265
- 0.373
- 0.348
- 0.362
- 0.392
- 0.424
- 0.377
- 0.304
- 0.331
- 0.375
- 0.296
- 0.337
- 0.319
- 0.302
- 0.356
- 0.365
- 0.332
- 0.334
- 0.32
- 0.312
- 0.314
- 0.321
- 0.325
- 0.304
- 0.318
- 0.322
- 0.373
- 0.314
- 0.365
- 0.388
- 0.353
- 0.302
- 0.335
- 0.355
- 0.374
- 0.285
- 0.342
- 0.285
- 0.327
- 0.341
unequal: 1
verbose: 1
