avg_train_accuracy: 0.904
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.15882978723404256
- 0.20297872340425532
- 0.34047872340425533
- 0.3618085106382979
- 0.4887234042553191
- 0.4622872340425532
- 0.4701595744680851
- 0.49803191489361703
- 0.5409574468085107
- 0.49590425531914895
- 0.5498404255319149
- 0.4838297872340426
- 0.6
- 0.5470212765957447
- 0.5787765957446809
- 0.5986170212765958
- 0.5263829787234042
- 0.5597872340425532
- 0.5942021276595745
- 0.5956382978723405
- 0.5957446808510638
- 0.5743617021276596
- 0.5595744680851064
- 0.550372340425532
- 0.5901063829787234
- 0.5715957446808511
- 0.6427659574468085
- 0.6691489361702128
- 0.6638297872340425
- 0.6470744680851064
- 0.630531914893617
- 0.6443617021276595
- 0.6004787234042553
- 0.6281382978723404
- 0.6257446808510638
- 0.6436702127659575
- 0.6160106382978724
- 0.5911170212765957
- 0.6445212765957447
- 0.6317021276595745
- 0.5642021276595744
- 0.6289361702127659
- 0.6864361702127659
- 0.6379787234042553
- 0.6495212765957447
- 0.6562765957446809
- 0.6650531914893617
- 0.6514893617021277
- 0.6384574468085107
- 0.6715425531914894
- 0.6779255319148936
- 0.6452127659574468
- 0.6602659574468085
- 0.6639893617021276
- 0.6339893617021276
- 0.6408510638297872
- 0.6442553191489362
- 0.6635106382978724
- 0.6519680851063829
- 0.6755851063829788
- 0.671063829787234
- 0.6390957446808511
- 0.6521808510638298
- 0.6811170212765958
- 0.6849468085106383
- 0.6802659574468085
- 0.653936170212766
- 0.6826063829787234
- 0.6409574468085106
- 0.6189361702127659
- 0.6526595744680851
- 0.6575
- 0.6867553191489362
- 0.6742553191489362
- 0.6325531914893617
- 0.6273936170212766
- 0.653563829787234
- 0.6567553191489361
- 0.6856914893617021
- 0.6608510638297872
- 0.6862234042553191
- 0.6725
- 0.675159574468085
- 0.6842553191489362
- 0.6567021276595745
- 0.695
- 0.6823404255319149
- 0.6812765957446808
- 0.7044148936170213
- 0.656595744680851
- 0.596968085106383
- 0.6601063829787234
- 0.6834574468085106
- 0.6440957446808511
- 0.6801595744680851
- 0.6800531914893617
- 0.6748936170212766
- 0.6859042553191489
- 0.6738297872340425
- 0.6906382978723404
test_loss_list:
- 515.9532399177551
- 458.4707751274109
- 377.3317675590515
- 326.92712688446045
- 276.9664132595062
- 259.3521829843521
- 245.3821450471878
- 254.10633993148804
- 226.03979170322418
- 231.3414990901947
- 200.991992354393
- 237.3181266784668
- 179.30842447280884
- 196.43472027778625
- 196.45399737358093
- 181.44769704341888
- 195.13390684127808
- 191.8694885969162
- 168.71217262744904
- 179.83813124895096
- 175.88026344776154
- 190.75675988197327
- 193.0534349679947
- 186.49521273374557
- 176.56544643640518
- 176.44695395231247
- 155.22430783510208
- 148.29542100429535
- 146.70724219083786
- 155.53793263435364
- 159.80780827999115
- 154.4833515882492
- 178.07254511117935
- 161.0148915052414
- 156.39445662498474
- 156.38559514284134
- 174.52625286579132
- 192.4109057188034
- 142.0608388185501
- 154.68721562623978
- 182.64031141996384
- 153.66053861379623
- 141.3787066936493
- 154.56834626197815
- 150.09476310014725
- 143.12079590559006
- 146.44400268793106
- 142.83688628673553
- 152.1882117986679
- 136.05266904830933
- 134.27570098638535
- 154.83978229761124
- 143.47650253772736
- 136.1602110862732
- 149.31110697984695
- 164.2431937456131
- 152.78500002622604
- 137.80393487215042
- 147.16993862390518
- 133.447403550148
- 144.12055444717407
- 147.02784883975983
- 151.9601108431816
- 137.8527666926384
- 131.66987377405167
- 141.36020481586456
- 150.4756218791008
- 140.45561504364014
- 153.26825338602066
- 161.82404386997223
- 142.27494782209396
- 147.2552366256714
- 131.38091814517975
- 140.4530532360077
- 141.73291838169098
- 154.29931658506393
- 138.0286283493042
- 145.90558123588562
- 137.02310061454773
- 146.5657467842102
- 125.2359631061554
- 131.52348560094833
- 131.25504541397095
- 128.96090453863144
- 150.68552196025848
- 142.4742732644081
- 141.77143508195877
- 127.19890892505646
- 120.0169227719307
- 136.04482525587082
- 169.7288401722908
- 147.70858949422836
- 136.3940104842186
- 160.5268566608429
- 134.11670035123825
- 139.0948240160942
- 141.43047273159027
- 133.33290427923203
- 136.76017957925797
- 128.02972894906998
train_accuracy:
- 0.104
- 0.133
- 0.862
- 0.154
- 0.429
- 0.61
- 0.831
- 0.569
- 0.356
- 0.658
- 0.462
- 0.579
- 0.904
- 0.817
- 0.52
- 0.896
- 0.494
- 0.506
- 0.54
- 0.85
- 0.875
- 0.917
- 0.735
- 0.137
- 0.56
- 0.904
- 0.281
- 0.787
- 0.282
- 0.567
- 0.668
- 0.858
- 0.721
- 0.667
- 0.0
- 0.908
- 0.319
- 0.658
- 0.908
- 0.597
- 0.44
- 0.927
- 0.908
- 0.75
- 0.469
- 0.641
- 0.846
- 0.942
- 0.587
- 0.697
- 0.875
- 0.535
- 0.892
- 0.44
- 0.41
- 0.017
- 0.842
- 0.642
- 0.986
- 0.419
- 0.733
- 0.59
- 0.44
- 0.936
- 0.72
- 0.635
- 0.875
- 0.396
- 0.58
- 0.372
- 0.633
- 0.519
- 0.469
- 0.663
- 0.413
- 0.459
- 0.818
- 0.958
- 0.95
- 0.109
- 0.892
- 0.797
- 0.946
- 0.95
- 0.938
- 0.658
- 0.788
- 0.985
- 0.581
- 0.841
- 0.877
- 0.64
- 0.789
- 0.75
- 0.53
- 0.508
- 0.875
- 0.933
- 0.875
- 0.904
train_loss:
- 1.166
- 0.717
- 0.699
- 0.568
- 0.526
- 0.541
- 0.516
- 0.486
- 0.462
- 0.499
- 0.488
- 0.388
- 0.406
- 0.426
- 0.419
- 0.462
- 0.409
- 0.386
- 0.38
- 0.384
- 0.36
- 0.341
- 0.423
- 0.356
- 0.305
- 0.411
- 0.435
- 0.397
- 0.388
- 0.393
- 0.414
- 0.437
- 0.341
- 0.335
- 0.398
- 0.432
- 0.4
- 0.367
- 0.388
- 0.376
- 0.385
- 0.309
- 0.342
- 0.287
- 0.356
- 0.371
- 0.384
- 0.366
- 0.388
- 0.457
- 0.397
- 0.354
- 0.366
- 0.303
- 0.281
- 0.246
- 0.281
- 0.289
- 0.256
- 0.279
- 0.222
- 0.427
- 0.313
- 0.366
- 0.345
- 0.321
- 0.317
- 0.314
- 0.352
- 0.3
- 0.409
- 0.318
- 0.373
- 0.329
- 0.342
- 0.318
- 0.314
- 0.277
- 0.317
- 0.306
- 0.273
- 0.319
- 0.335
- 0.293
- 0.335
- 0.331
- 0.269
- 0.362
- 0.32
- 0.318
- 0.332
- 0.368
- 0.319
- 0.367
- 0.325
- 0.352
- 0.323
- 0.351
- 0.302
- 0.345
unequal: 1
verbose: 1
