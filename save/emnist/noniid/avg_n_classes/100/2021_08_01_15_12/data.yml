avg_train_accuracy: 0.495
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1647340425531915
- 0.2571276595744681
- 0.31643617021276593
- 0.3773936170212766
- 0.404468085106383
- 0.44659574468085106
- 0.5262765957446809
- 0.5079787234042553
- 0.4521276595744681
- 0.5385106382978724
- 0.6233510638297872
- 0.545372340425532
- 0.5528191489361702
- 0.5851595744680851
- 0.576063829787234
- 0.5706382978723404
- 0.6392021276595745
- 0.5729787234042554
- 0.5320744680851064
- 0.583031914893617
- 0.5210106382978723
- 0.6196276595744681
- 0.5512234042553191
- 0.5995744680851064
- 0.6040425531914894
- 0.6351063829787233
- 0.6334042553191489
- 0.6584574468085106
- 0.6286702127659575
- 0.6737765957446809
- 0.637127659574468
- 0.6680851063829787
- 0.6418617021276596
- 0.653031914893617
- 0.6374468085106383
- 0.6506382978723404
- 0.6947872340425532
- 0.6249468085106383
- 0.6110638297872341
- 0.6274468085106383
- 0.6777127659574468
- 0.6566489361702128
- 0.6525531914893618
- 0.6338297872340426
- 0.6861702127659575
- 0.6723404255319149
- 0.6367021276595745
- 0.6285106382978723
- 0.6602659574468085
- 0.6623404255319149
- 0.6410638297872341
- 0.6470212765957447
- 0.6753191489361702
- 0.637127659574468
- 0.645904255319149
- 0.658563829787234
- 0.6265957446808511
- 0.6322340425531915
- 0.6167553191489362
- 0.6139361702127659
- 0.6902659574468085
- 0.6723404255319149
- 0.6510638297872341
- 0.6814361702127659
- 0.6072340425531915
- 0.6136702127659575
- 0.665372340425532
- 0.6997872340425532
- 0.6770212765957446
- 0.6692021276595744
- 0.6738297872340425
- 0.6911170212765958
- 0.6742021276595744
- 0.6470212765957447
- 0.6961702127659575
- 0.6357446808510638
- 0.7098936170212766
- 0.6593085106382979
- 0.6737234042553192
- 0.686968085106383
- 0.6482978723404256
- 0.7048936170212766
- 0.6777127659574468
- 0.6580851063829787
- 0.6801063829787234
- 0.7135638297872341
- 0.6281382978723404
- 0.6696808510638298
- 0.6926595744680851
- 0.6186170212765958
- 0.6339361702127659
- 0.6608510638297872
- 0.6118617021276596
- 0.622872340425532
- 0.6652659574468085
- 0.6648936170212766
- 0.6725531914893617
- 0.7102659574468085
- 0.7005851063829788
- 0.7163297872340425
test_loss_list:
- 523.7393071651459
- 425.8027789592743
- 376.10394048690796
- 331.6055600643158
- 304.37070739269257
- 269.9231572151184
- 234.10547006130219
- 223.61254513263702
- 248.50285840034485
- 204.3741968870163
- 181.2020639181137
- 212.6699539422989
- 199.12772631645203
- 187.9487681388855
- 191.6593292951584
- 188.47358202934265
- 163.04222470521927
- 182.2173238992691
- 207.67424929141998
- 186.5404195189476
- 204.73584234714508
- 174.07019793987274
- 195.5984926223755
- 174.50187093019485
- 172.6330890059471
- 163.2271073460579
- 156.34238517284393
- 144.08467030525208
- 163.64109605550766
- 133.06965094804764
- 146.90058082342148
- 144.09270161390305
- 152.99474322795868
- 146.24186754226685
- 154.72785532474518
- 157.56201243400574
- 135.0062177181244
- 162.8859667778015
- 167.6278167963028
- 165.33987373113632
- 137.07105243206024
- 142.09797364473343
- 151.05696022510529
- 156.23566967248917
- 134.72205066680908
- 136.75225186347961
- 156.29760599136353
- 165.19670176506042
- 136.81662732362747
- 132.85920453071594
- 153.06637108325958
- 148.7155528664589
- 144.24784815311432
- 157.25708889961243
- 148.3772485256195
- 154.2910784482956
- 157.3919004201889
- 155.63796442747116
- 167.11117386817932
- 172.53957957029343
- 140.1662353873253
- 141.43667078018188
- 149.19742143154144
- 143.23729598522186
- 167.4166643023491
- 160.54258197546005
- 150.58405983448029
- 134.10160899162292
- 144.7287763953209
- 144.0558187365532
- 141.50353175401688
- 125.16335541009903
- 139.72977566719055
- 139.06989592313766
- 130.64697670936584
- 140.1372486948967
- 135.37397694587708
- 136.09739005565643
- 139.60046076774597
- 137.73681962490082
- 148.75950169563293
- 127.71823221445084
- 141.08160376548767
- 148.92679756879807
- 143.11380577087402
- 121.54779708385468
- 156.77314889431
- 134.1875085234642
- 135.4927818775177
- 164.99260437488556
- 160.10205894708633
- 149.34666681289673
- 189.22190475463867
- 162.43692445755005
- 134.77072620391846
- 134.92581450939178
- 139.34574335813522
- 118.01723718643188
- 132.09531891345978
- 123.6045047044754
train_accuracy:
- 0.006
- 0.034
- 0.01
- 0.192
- 0.579
- 0.491
- 0.534
- 0.197
- 0.0
- 1.0
- 0.508
- 0.0
- 0.538
- 0.528
- 0.653
- 0.787
- 0.342
- 0.518
- 0.563
- 0.825
- 0.656
- 0.795
- 0.369
- 0.63
- 0.753
- 0.212
- 0.487
- 0.542
- 0.38
- 0.403
- 0.88
- 1.0
- 0.933
- 0.638
- 0.85
- 0.969
- 0.739
- 0.453
- 0.129
- 0.99
- 0.904
- 0.7
- 0.632
- 0.879
- 0.967
- 0.767
- 0.52
- 0.588
- 0.805
- 0.688
- 0.868
- 0.75
- 0.494
- 0.509
- 0.546
- 0.436
- 0.38
- 0.567
- 0.664
- 0.671
- 0.438
- 0.65
- 0.809
- 0.758
- 0.9
- 0.87
- 0.419
- 0.421
- 0.936
- 0.325
- 0.533
- 0.48
- 0.7
- 0.55
- 0.4
- 0.753
- 0.0
- 0.678
- 0.394
- 0.921
- 0.15
- 0.987
- 0.956
- 0.469
- 0.545
- 0.93
- 0.663
- 0.44
- 0.516
- 0.809
- 0.9
- 0.965
- 0.75
- 0.933
- 0.655
- 0.323
- 0.797
- 0.48
- 0.646
- 0.495
train_loss:
- 1.008
- 0.851
- 0.707
- 0.655
- 0.416
- 0.572
- 0.531
- 0.546
- 0.519
- 0.386
- 0.492
- 0.35
- 0.394
- 0.39
- 0.434
- 0.446
- 0.492
- 0.388
- 0.461
- 0.398
- 0.358
- 0.412
- 0.415
- 0.408
- 0.452
- 0.329
- 0.373
- 0.324
- 0.438
- 0.295
- 0.353
- 0.3
- 0.34
- 0.348
- 0.313
- 0.347
- 0.367
- 0.431
- 0.359
- 0.312
- 0.411
- 0.31
- 0.359
- 0.34
- 0.402
- 0.333
- 0.401
- 0.42
- 0.275
- 0.373
- 0.354
- 0.341
- 0.413
- 0.378
- 0.285
- 0.334
- 0.374
- 0.388
- 0.373
- 0.288
- 0.411
- 0.307
- 0.388
- 0.329
- 0.358
- 0.398
- 0.362
- 0.381
- 0.32
- 0.324
- 0.3
- 0.359
- 0.391
- 0.253
- 0.337
- 0.36
- 0.312
- 0.373
- 0.352
- 0.35
- 0.344
- 0.319
- 0.335
- 0.336
- 0.325
- 0.344
- 0.329
- 0.314
- 0.328
- 0.301
- 0.246
- 0.302
- 0.284
- 0.287
- 0.316
- 0.342
- 0.306
- 0.36
- 0.363
- 0.312
unequal: 1
verbose: 1
