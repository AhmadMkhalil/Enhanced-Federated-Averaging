avg_train_accuracy: 0.805
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.13335106382978723
- 0.21106382978723404
- 0.3728191489361702
- 0.3925531914893617
- 0.42638297872340425
- 0.4626595744680851
- 0.4664893617021277
- 0.44292553191489364
- 0.4527127659574468
- 0.4803191489361702
- 0.543563829787234
- 0.5660106382978723
- 0.5488829787234043
- 0.5467553191489362
- 0.5161702127659574
- 0.6154255319148936
- 0.5522340425531915
- 0.5798404255319148
- 0.4775
- 0.5982978723404255
- 0.6379787234042553
- 0.6511702127659574
- 0.6251595744680851
- 0.635
- 0.5765957446808511
- 0.6207446808510638
- 0.5852127659574468
- 0.6338829787234043
- 0.6080319148936171
- 0.6318085106382979
- 0.6159574468085106
- 0.6080319148936171
- 0.5946808510638298
- 0.6519148936170213
- 0.6332978723404256
- 0.5945212765957447
- 0.5732978723404255
- 0.6378191489361702
- 0.6473404255319148
- 0.6171276595744681
- 0.6619148936170213
- 0.6030319148936171
- 0.6661170212765958
- 0.603563829787234
- 0.6352659574468085
- 0.6146808510638297
- 0.6324468085106383
- 0.6127127659574468
- 0.6718617021276596
- 0.6418085106382979
- 0.6180851063829788
- 0.6253191489361702
- 0.5652127659574468
- 0.619095744680851
- 0.6565425531914894
- 0.6368617021276596
- 0.6301595744680851
- 0.6181382978723404
- 0.5806382978723404
- 0.665
- 0.620904255319149
- 0.6682446808510638
- 0.6340425531914894
- 0.6461702127659574
- 0.6307978723404255
- 0.6442021276595745
- 0.6640957446808511
- 0.6505851063829787
- 0.678936170212766
- 0.6468085106382979
- 0.6876595744680851
- 0.6706382978723404
- 0.6630851063829787
- 0.6507446808510639
- 0.6293085106382978
- 0.6490957446808511
- 0.6675
- 0.6525
- 0.6439893617021276
- 0.6645212765957447
- 0.6190425531914894
- 0.6333510638297872
- 0.655904255319149
- 0.6986170212765958
- 0.6689893617021276
- 0.6679255319148936
- 0.6925531914893617
- 0.6698936170212766
- 0.6883510638297873
- 0.6697872340425531
- 0.6343085106382979
- 0.6555851063829787
- 0.6226063829787234
- 0.6495212765957447
- 0.6857446808510639
- 0.6684042553191489
- 0.6463297872340426
- 0.6824468085106383
- 0.6629255319148936
- 0.6401063829787234
test_loss_list:
- 536.272054195404
- 461.9444465637207
- 368.7713530063629
- 319.5056371688843
- 288.38450944423676
- 274.61900806427
- 259.0703879594803
- 254.38060450553894
- 237.10579538345337
- 238.61198139190674
- 204.84640896320343
- 185.3465086221695
- 198.3939082622528
- 200.95434188842773
- 204.3556685447693
- 167.80033159255981
- 209.78207290172577
- 187.85762763023376
- 215.08025455474854
- 171.69848066568375
- 151.2661228775978
- 146.38375651836395
- 158.6634726524353
- 151.6103322505951
- 185.98825418949127
- 165.19288682937622
- 178.63926535844803
- 159.90128713846207
- 174.36688250303268
- 159.6576622724533
- 164.56567466259003
- 165.43098306655884
- 182.8819997906685
- 146.99303191900253
- 169.88525086641312
- 164.0964959859848
- 165.97776412963867
- 161.2965989112854
- 153.69045513868332
- 167.81525206565857
- 143.28228324651718
- 174.19409781694412
- 136.73283225297928
- 166.9864997267723
- 163.29834240674973
- 157.187335729599
- 153.1942046880722
- 154.7884373664856
- 135.35590028762817
- 149.27693563699722
- 165.35994219779968
- 150.93417757749557
- 181.69395691156387
- 153.25320357084274
- 144.8018520474434
- 150.54335618019104
- 151.6423145532608
- 154.89013749361038
- 175.25775128602982
- 138.33568561077118
- 164.9813346862793
- 142.1244722008705
- 145.82836335897446
- 158.42480397224426
- 163.7896467447281
- 140.54714214801788
- 139.52302849292755
- 140.13635557889938
- 131.17246741056442
- 141.29377776384354
- 125.07685428857803
- 149.70110684633255
- 137.15370613336563
- 143.31654489040375
- 152.72372180223465
- 149.93154805898666
- 131.6742240190506
- 144.06399071216583
- 140.80851048231125
- 145.9271029829979
- 155.33247637748718
- 145.542298913002
- 133.173024892807
- 126.0215677022934
- 141.78382056951523
- 131.19388300180435
- 126.8256224989891
- 128.22053891420364
- 127.1764754652977
- 132.70240139961243
- 153.39194113016129
- 146.53891080617905
- 157.7581461071968
- 146.20404809713364
- 127.89010518789291
- 143.79815953969955
- 141.8395466208458
- 125.55011361837387
- 135.13965934515
- 154.62345397472382
train_accuracy:
- 0.0
- 0.045
- 0.331
- 0.617
- 0.441
- 0.511
- 0.717
- 0.219
- 0.95
- 0.225
- 0.678
- 0.775
- 0.6
- 0.872
- 0.581
- 0.815
- 0.344
- 0.784
- 0.533
- 0.622
- 0.856
- 0.787
- 0.792
- 0.85
- 0.11
- 0.921
- 0.87
- 0.677
- 0.74
- 0.67
- 0.944
- 0.628
- 0.083
- 0.837
- 0.338
- 0.389
- 0.938
- 0.759
- 0.2
- 0.692
- 0.485
- 0.894
- 0.75
- 0.45
- 0.836
- 0.175
- 0.342
- 0.565
- 0.796
- 0.587
- 0.95
- 0.7
- 0.553
- 0.419
- 0.683
- 0.743
- 0.069
- 0.556
- 0.867
- 0.905
- 0.481
- 0.814
- 0.75
- 0.675
- 0.491
- 0.0
- 0.038
- 0.9
- 0.565
- 0.817
- 0.639
- 0.453
- 0.881
- 0.475
- 0.137
- 0.858
- 0.987
- 0.438
- 0.419
- 0.845
- 0.469
- 0.987
- 0.536
- 0.669
- 0.883
- 0.927
- 0.983
- 0.933
- 0.286
- 0.236
- 0.85
- 0.506
- 0.612
- 0.592
- 1.0
- 0.45
- 0.883
- 0.759
- 0.867
- 0.805
train_loss:
- 1.133
- 0.788
- 0.767
- 0.569
- 0.56
- 0.527
- 0.495
- 0.464
- 0.569
- 0.487
- 0.532
- 0.342
- 0.304
- 0.438
- 0.458
- 0.405
- 0.378
- 0.402
- 0.442
- 0.371
- 0.376
- 0.374
- 0.419
- 0.365
- 0.306
- 0.412
- 0.425
- 0.438
- 0.367
- 0.433
- 0.284
- 0.402
- 0.372
- 0.393
- 0.41
- 0.382
- 0.32
- 0.332
- 0.303
- 0.371
- 0.406
- 0.368
- 0.39
- 0.385
- 0.392
- 0.351
- 0.39
- 0.382
- 0.328
- 0.405
- 0.36
- 0.413
- 0.316
- 0.409
- 0.292
- 0.334
- 0.369
- 0.395
- 0.365
- 0.415
- 0.339
- 0.347
- 0.386
- 0.386
- 0.289
- 0.354
- 0.309
- 0.358
- 0.405
- 0.263
- 0.303
- 0.313
- 0.466
- 0.344
- 0.388
- 0.291
- 0.362
- 0.336
- 0.37
- 0.432
- 0.386
- 0.222
- 0.387
- 0.393
- 0.357
- 0.305
- 0.308
- 0.295
- 0.344
- 0.291
- 0.32
- 0.295
- 0.392
- 0.278
- 0.339
- 0.34
- 0.377
- 0.395
- 0.328
- 0.319
unequal: 1
verbose: 1
