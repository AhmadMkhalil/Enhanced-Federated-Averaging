avg_train_accuracy: 0.6
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.19138297872340426
- 0.2578723404255319
- 0.3153723404255319
- 0.3803723404255319
- 0.433936170212766
- 0.4574468085106383
- 0.4734042553191489
- 0.48021276595744683
- 0.5475
- 0.5360106382978723
- 0.5823936170212766
- 0.5504255319148936
- 0.5691489361702128
- 0.5346276595744681
- 0.5729787234042554
- 0.5690957446808511
- 0.5927127659574468
- 0.5618617021276596
- 0.5642553191489361
- 0.6073404255319149
- 0.5917021276595744
- 0.6128191489361702
- 0.6261170212765957
- 0.6499468085106384
- 0.6842553191489362
- 0.6472340425531915
- 0.6081914893617021
- 0.6523404255319148
- 0.6122872340425531
- 0.6061170212765957
- 0.61
- 0.6362765957446809
- 0.6167553191489362
- 0.6313829787234042
- 0.6473404255319148
- 0.6678723404255319
- 0.615531914893617
- 0.6611170212765958
- 0.6147872340425532
- 0.6531914893617021
- 0.655904255319149
- 0.6180851063829788
- 0.6304255319148936
- 0.578404255319149
- 0.6212765957446809
- 0.620904255319149
- 0.642127659574468
- 0.6417553191489361
- 0.5722872340425532
- 0.5847340425531915
- 0.6126063829787234
- 0.6201063829787234
- 0.6354255319148936
- 0.6847872340425532
- 0.6664893617021277
- 0.595531914893617
- 0.6401063829787234
- 0.6247872340425532
- 0.6147340425531915
- 0.6879787234042554
- 0.6595744680851063
- 0.6300531914893617
- 0.624468085106383
- 0.6065957446808511
- 0.6352127659574468
- 0.6422872340425532
- 0.6725
- 0.6940957446808511
- 0.7044148936170213
- 0.6412765957446809
- 0.6350531914893617
- 0.666595744680851
- 0.6805851063829788
- 0.6671276595744681
- 0.6877659574468085
- 0.686595744680851
- 0.6829787234042554
- 0.6791489361702128
- 0.6726595744680851
- 0.6405851063829787
- 0.6968617021276595
- 0.668936170212766
- 0.6573936170212766
- 0.6712765957446809
- 0.6265425531914893
- 0.6651063829787234
- 0.6767553191489362
- 0.6161702127659574
- 0.6184042553191489
- 0.6639893617021276
- 0.6965957446808511
- 0.686063829787234
- 0.6775
- 0.685
- 0.6776063829787234
- 0.6778723404255319
- 0.6187234042553191
- 0.6118617021276596
- 0.6975531914893617
- 0.6595744680851063
test_loss_list:
- 528.8130276203156
- 436.3090636730194
- 359.4678225517273
- 341.1906020641327
- 286.69568622112274
- 266.846856713295
- 255.6566789150238
- 241.07147216796875
- 207.59469413757324
- 213.37923741340637
- 194.20183885097504
- 201.94561731815338
- 197.9885811805725
- 211.14167487621307
- 198.8634479045868
- 194.02218902111053
- 186.51785337924957
- 191.2045031785965
- 200.41421043872833
- 172.03026312589645
- 180.50400245189667
- 163.5431963801384
- 159.37951165437698
- 155.56481689214706
- 139.06677985191345
- 163.1277282834053
- 159.6628322005272
- 154.2453088760376
- 157.43507277965546
- 168.5455383658409
- 167.22376930713654
- 149.1966912150383
- 171.50137865543365
- 148.11170357465744
- 140.53668290376663
- 136.5175673365593
- 164.99754059314728
- 149.09687823057175
- 175.4241685271263
- 147.11963421106339
- 154.91679108142853
- 161.68122053146362
- 152.85533344745636
- 172.57007521390915
- 155.4501930475235
- 156.3559911251068
- 149.21326857805252
- 152.2847397327423
- 188.6505749821663
- 183.77815401554108
- 167.9672486782074
- 156.2240168452263
- 157.84263962507248
- 134.8887425661087
- 143.67132115364075
- 182.15296244621277
- 145.0208387374878
- 156.58595550060272
- 161.8661791086197
- 138.8800995349884
- 143.35855948925018
- 157.10570186376572
- 152.60345709323883
- 174.7033435702324
- 145.5022201538086
- 154.09732741117477
- 148.12982243299484
- 131.69267141819
- 123.75924235582352
- 145.64338964223862
- 160.43442845344543
- 137.71111750602722
- 134.2259492278099
- 138.19637763500214
- 129.28337424993515
- 143.40109491348267
- 136.57540547847748
- 137.8952180147171
- 137.2485665678978
- 145.27254033088684
- 128.06956380605698
- 139.73674535751343
- 138.50928354263306
- 150.6084970831871
- 149.83973622322083
- 140.69850158691406
- 140.25234043598175
- 160.38777458667755
- 156.92850077152252
- 139.57303857803345
- 125.73927843570709
- 141.8136420249939
- 146.79066014289856
- 135.41405129432678
- 148.9234917163849
- 129.7413734793663
- 180.57072281837463
- 169.2990592122078
- 138.71369463205338
- 142.95637452602386
train_accuracy:
- 0.664
- 0.33
- 0.132
- 0.61
- 0.99
- 0.379
- 0.795
- 0.725
- 0.825
- 0.517
- 0.857
- 0.775
- 0.56
- 0.14
- 0.625
- 0.386
- 0.633
- 0.596
- 0.814
- 0.45
- 0.65
- 0.98
- 0.864
- 0.2
- 0.735
- 0.688
- 0.407
- 0.641
- 0.555
- 0.385
- 0.814
- 0.875
- 0.731
- 0.686
- 0.779
- 0.611
- 0.537
- 0.718
- 0.579
- 0.619
- 0.615
- 0.15
- 0.361
- 0.487
- 0.65
- 0.98
- 0.731
- 0.488
- 0.94
- 0.338
- 0.475
- 0.879
- 0.788
- 0.867
- 0.607
- 0.84
- 0.462
- 0.167
- 0.535
- 0.525
- 0.664
- 0.845
- 0.577
- 0.47
- 0.481
- 0.838
- 0.903
- 0.715
- 0.275
- 0.875
- 0.933
- 0.944
- 0.987
- 0.475
- 0.806
- 0.787
- 0.767
- 0.331
- 0.862
- 0.943
- 0.725
- 0.906
- 0.567
- 0.776
- 0.59
- 0.88
- 0.97
- 0.792
- 0.619
- 0.577
- 0.738
- 0.838
- 0.929
- 0.807
- 0.393
- 0.507
- 0.645
- 0.67
- 0.962
- 0.6
train_loss:
- 1.056
- 0.868
- 0.712
- 0.586
- 0.49
- 0.549
- 0.479
- 0.577
- 0.414
- 0.401
- 0.492
- 0.459
- 0.463
- 0.413
- 0.465
- 0.454
- 0.495
- 0.385
- 0.405
- 0.368
- 0.421
- 0.445
- 0.386
- 0.353
- 0.427
- 0.367
- 0.283
- 0.36
- 0.347
- 0.395
- 0.433
- 0.406
- 0.372
- 0.376
- 0.346
- 0.399
- 0.389
- 0.356
- 0.31
- 0.416
- 0.326
- 0.365
- 0.34
- 0.383
- 0.358
- 0.332
- 0.318
- 0.399
- 0.278
- 0.35
- 0.289
- 0.31
- 0.317
- 0.384
- 0.306
- 0.259
- 0.432
- 0.351
- 0.36
- 0.314
- 0.422
- 0.376
- 0.313
- 0.298
- 0.343
- 0.373
- 0.345
- 0.303
- 0.337
- 0.347
- 0.355
- 0.285
- 0.287
- 0.33
- 0.334
- 0.351
- 0.35
- 0.322
- 0.337
- 0.358
- 0.33
- 0.292
- 0.268
- 0.302
- 0.332
- 0.328
- 0.277
- 0.273
- 0.323
- 0.355
- 0.418
- 0.359
- 0.351
- 0.307
- 0.33
- 0.304
- 0.342
- 0.276
- 0.329
- 0.3
unequal: 1
verbose: 1
