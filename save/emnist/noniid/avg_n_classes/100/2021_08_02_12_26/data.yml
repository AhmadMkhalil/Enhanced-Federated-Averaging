avg_train_accuracy: 0.99
avg_train_loss: 0.002
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.18388297872340426
- 0.3105851063829787
- 0.33601063829787237
- 0.36446808510638296
- 0.3972872340425532
- 0.4370212765957447
- 0.49031914893617023
- 0.35925531914893616
- 0.4825
- 0.5216489361702128
- 0.4743617021276596
- 0.5172872340425532
- 0.5143085106382979
- 0.5879787234042553
- 0.5689893617021277
- 0.5336170212765957
- 0.5970744680851063
- 0.596436170212766
- 0.5959574468085106
- 0.634468085106383
- 0.5578723404255319
- 0.5971276595744681
- 0.6293085106382978
- 0.5956382978723405
- 0.5910106382978724
- 0.6032978723404255
- 0.6538829787234043
- 0.6025531914893617
- 0.5789893617021277
- 0.641436170212766
- 0.575
- 0.6453723404255319
- 0.6145212765957446
- 0.6525
- 0.6009042553191489
- 0.6365425531914893
- 0.6451063829787234
- 0.6590957446808511
- 0.6643617021276595
- 0.6242021276595745
- 0.6003191489361702
- 0.6111170212765957
- 0.6134574468085107
- 0.6314893617021277
- 0.6736702127659574
- 0.6585106382978724
- 0.663031914893617
- 0.661595744680851
- 0.6449468085106383
- 0.6036702127659574
- 0.6636170212765957
- 0.6057446808510638
- 0.6561170212765958
- 0.686063829787234
- 0.651595744680851
- 0.6232446808510639
- 0.6288829787234043
- 0.6592021276595744
- 0.6288829787234043
- 0.6763297872340426
- 0.6151063829787234
- 0.6725531914893617
- 0.6526595744680851
- 0.6509574468085106
- 0.6426063829787234
- 0.6844148936170212
- 0.6623404255319149
- 0.6581914893617021
- 0.6663829787234042
- 0.6197340425531915
- 0.6789893617021276
- 0.6504255319148936
- 0.675372340425532
- 0.663031914893617
- 0.6656914893617021
- 0.6895744680851064
- 0.6621276595744681
- 0.6634574468085106
- 0.6489893617021276
- 0.675159574468085
- 0.6101063829787234
- 0.671063829787234
- 0.6793085106382979
- 0.6537234042553192
- 0.6488297872340425
- 0.6442021276595745
- 0.6879787234042554
- 0.6828723404255319
- 0.6782978723404255
- 0.6051595744680851
- 0.6624468085106383
- 0.6005851063829787
- 0.6442553191489362
- 0.6595212765957447
- 0.6556914893617021
- 0.6953723404255319
- 0.6906382978723404
- 0.6339893617021276
- 0.6934574468085106
- 0.7007978723404256
test_loss_list:
- 523.1480069160461
- 437.8408296108246
- 391.8232145309448
- 354.9077341556549
- 303.93459832668304
- 284.5703775882721
- 261.74282586574554
- 304.8425279855728
- 238.90815770626068
- 220.58829724788666
- 233.53195357322693
- 221.60772931575775
- 217.00369822978973
- 184.1961784362793
- 210.0169619321823
- 216.15877497196198
- 194.45558506250381
- 175.28291654586792
- 187.28112041950226
- 163.45527237653732
- 192.13887214660645
- 168.73832154273987
- 159.92548620700836
- 172.90434908866882
- 171.3905544281006
- 159.92828899621964
- 148.1601591706276
- 173.45642638206482
- 179.22209286689758
- 152.54769760370255
- 168.97777158021927
- 149.7801360487938
- 158.4717561006546
- 150.56956505775452
- 161.15203911066055
- 159.94788265228271
- 151.51215779781342
- 152.92973697185516
- 141.7775102853775
- 157.74179410934448
- 169.4644819498062
- 165.49447786808014
- 164.03318428993225
- 157.01038789749146
- 135.70203125476837
- 139.1918484568596
- 136.40338093042374
- 138.13539904356003
- 149.3395864367485
- 167.91267836093903
- 137.2898195385933
- 161.4093479514122
- 147.27752143144608
- 131.9014510512352
- 141.08857983350754
- 159.64507389068604
- 154.44590264558792
- 148.13908404111862
- 152.8455798625946
- 145.02799916267395
- 158.96518051624298
- 136.09408020973206
- 164.58126944303513
- 148.6889562010765
- 153.0605068206787
- 136.300222158432
- 149.89743793010712
- 145.36570930480957
- 144.55915051698685
- 159.8971944451332
- 132.73536163568497
- 142.10817229747772
- 137.20678555965424
- 151.94841676950455
- 134.28998285531998
- 135.70291101932526
- 144.66589486598969
- 136.00067949295044
- 146.71157658100128
- 130.7717523574829
- 154.7445930838585
- 137.70501792430878
- 126.94486981630325
- 147.94217145442963
- 149.77000641822815
- 137.89349818229675
- 124.59891849756241
- 138.32335937023163
- 138.1682429909706
- 168.27893924713135
- 146.21497374773026
- 172.67836153507233
- 143.6778848171234
- 146.9515901207924
- 151.3612989783287
- 131.86419397592545
- 135.03956270217896
- 154.73282581567764
- 128.26715302467346
- 130.92647993564606
train_accuracy:
- 0.038
- 0.0
- 0.808
- 0.033
- 0.483
- 0.327
- 0.7
- 0.1
- 0.753
- 0.48
- 0.578
- 0.841
- 0.6
- 0.719
- 0.869
- 0.944
- 0.894
- 0.3
- 0.468
- 0.47
- 0.674
- 0.562
- 0.793
- 0.583
- 0.0
- 0.04
- 0.857
- 0.733
- 0.647
- 0.99
- 0.503
- 0.442
- 0.875
- 0.922
- 0.49
- 0.778
- 0.671
- 0.568
- 0.925
- 0.186
- 0.85
- 0.576
- 0.606
- 0.0
- 0.445
- 0.733
- 0.716
- 0.714
- 0.809
- 0.8
- 0.661
- 0.608
- 0.664
- 0.421
- 0.646
- 0.679
- 0.41
- 0.564
- 0.938
- 0.744
- 0.888
- 0.712
- 0.958
- 0.038
- 0.555
- 0.718
- 0.578
- 0.96
- 0.74
- 0.079
- 0.483
- 0.85
- 0.641
- 0.55
- 0.779
- 0.883
- 0.62
- 0.806
- 0.608
- 0.894
- 0.307
- 0.912
- 0.908
- 0.658
- 0.9
- 0.587
- 0.914
- 0.823
- 0.763
- 0.788
- 0.475
- 0.621
- 0.219
- 0.583
- 0.908
- 0.731
- 0.767
- 0.0
- 0.709
- 0.99
train_loss:
- 1.278
- 0.856
- 0.71
- 0.521
- 0.475
- 0.529
- 0.469
- 0.408
- 0.422
- 0.45
- 0.366
- 0.455
- 0.463
- 0.505
- 0.365
- 0.343
- 0.397
- 0.383
- 0.461
- 0.455
- 0.379
- 0.427
- 0.389
- 0.484
- 0.405
- 0.308
- 0.382
- 0.401
- 0.4
- 0.42
- 0.36
- 0.374
- 0.385
- 0.418
- 0.353
- 0.418
- 0.412
- 0.383
- 0.4
- 0.349
- 0.37
- 0.38
- 0.438
- 0.331
- 0.378
- 0.332
- 0.413
- 0.364
- 0.411
- 0.36
- 0.354
- 0.378
- 0.34
- 0.317
- 0.32
- 0.337
- 0.321
- 0.359
- 0.299
- 0.389
- 0.354
- 0.27
- 0.317
- 0.309
- 0.359
- 0.372
- 0.342
- 0.313
- 0.331
- 0.303
- 0.262
- 0.332
- 0.303
- 0.336
- 0.33
- 0.344
- 0.334
- 0.327
- 0.266
- 0.368
- 0.311
- 0.385
- 0.362
- 0.294
- 0.328
- 0.352
- 0.379
- 0.322
- 0.406
- 0.378
- 0.329
- 0.299
- 0.313
- 0.32
- 0.369
- 0.348
- 0.361
- 0.304
- 0.379
- 0.23
unequal: 1
verbose: 1
