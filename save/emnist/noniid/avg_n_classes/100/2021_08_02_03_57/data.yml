avg_train_accuracy: 0.84
avg_train_loss: 0.002
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1950531914893617
- 0.23824468085106382
- 0.3140425531914894
- 0.38845744680851063
- 0.4271808510638298
- 0.4472872340425532
- 0.4528723404255319
- 0.5395744680851063
- 0.5596276595744681
- 0.5330851063829787
- 0.49042553191489363
- 0.5568085106382978
- 0.5948404255319149
- 0.5387765957446808
- 0.5687765957446809
- 0.630904255319149
- 0.5907978723404256
- 0.6310106382978723
- 0.596436170212766
- 0.6143617021276596
- 0.6222340425531915
- 0.5778191489361703
- 0.6109042553191489
- 0.6023936170212766
- 0.6021276595744681
- 0.5737765957446809
- 0.573031914893617
- 0.5670744680851064
- 0.6420212765957447
- 0.6567553191489361
- 0.6307446808510638
- 0.6935638297872341
- 0.6487234042553192
- 0.6081914893617021
- 0.6340425531914894
- 0.6271808510638298
- 0.6274468085106383
- 0.6638297872340425
- 0.648563829787234
- 0.6579787234042553
- 0.6493617021276595
- 0.6381382978723404
- 0.6029787234042553
- 0.653563829787234
- 0.6588297872340425
- 0.6809574468085107
- 0.6061702127659574
- 0.6324468085106383
- 0.6317553191489361
- 0.6326063829787234
- 0.648936170212766
- 0.6518617021276596
- 0.6245212765957446
- 0.6330851063829788
- 0.6830851063829787
- 0.6498404255319149
- 0.6492021276595744
- 0.6356382978723404
- 0.6340425531914894
- 0.6337765957446808
- 0.6435106382978724
- 0.6621276595744681
- 0.6534574468085106
- 0.6855851063829788
- 0.6804255319148936
- 0.6721808510638297
- 0.6819148936170213
- 0.6465425531914893
- 0.6840425531914893
- 0.6446276595744681
- 0.6881382978723404
- 0.6631382978723405
- 0.7054787234042553
- 0.6837234042553192
- 0.7123936170212766
- 0.6526063829787234
- 0.6338829787234043
- 0.6662234042553191
- 0.6525
- 0.6673404255319149
- 0.7171808510638298
- 0.6825
- 0.706968085106383
- 0.6675
- 0.6967021276595745
- 0.6936702127659574
- 0.7094680851063829
- 0.655904255319149
- 0.6257446808510638
- 0.6296808510638298
- 0.6256382978723404
- 0.6816489361702127
- 0.7007446808510638
- 0.6861702127659575
- 0.7009574468085107
- 0.6302127659574468
- 0.6276595744680851
- 0.6706914893617021
- 0.6623404255319149
- 0.651595744680851
test_loss_list:
- 520.0760049819946
- 436.4469938278198
- 382.80215978622437
- 346.4072433710098
- 302.6211521625519
- 288.1625692844391
- 268.51247465610504
- 237.33369708061218
- 203.97401213645935
- 216.75465369224548
- 233.28081250190735
- 206.31953370571136
- 181.34241300821304
- 205.416686296463
- 201.8598608970642
- 173.33637005090714
- 187.4424295425415
- 166.00914633274078
- 188.73067963123322
- 170.65606784820557
- 170.51524364948273
- 189.5165535211563
- 175.8902969956398
- 165.65685719251633
- 180.57994854450226
- 178.65462040901184
- 180.7613132596016
- 179.30351454019547
- 159.59365785121918
- 164.814350605011
- 153.64057129621506
- 132.73459815979004
- 160.04581648111343
- 175.4581801891327
- 159.91730880737305
- 157.24042975902557
- 155.26485294103622
- 152.24862027168274
- 155.20053082704544
- 153.25258195400238
- 166.71720826625824
- 162.00107270479202
- 177.3803911805153
- 146.29765677452087
- 143.898475587368
- 140.91448497772217
- 181.90139400959015
- 157.3463979959488
- 150.01457178592682
- 148.91372257471085
- 156.37162721157074
- 149.25108301639557
- 169.34577214717865
- 148.47534376382828
- 136.99016571044922
- 144.45137375593185
- 155.86228609085083
- 155.92162162065506
- 158.65060955286026
- 157.25040924549103
- 151.28467452526093
- 152.79422783851624
- 153.81191223859787
- 139.17162930965424
- 142.6016076207161
- 155.7381997704506
- 140.1970466375351
- 155.77114820480347
- 148.06663274765015
- 147.2985242009163
- 145.9990069270134
- 154.00116103887558
- 130.26963138580322
- 140.25839167833328
- 128.2476220726967
- 145.993024289608
- 158.3572200536728
- 141.33092230558395
- 152.9641814827919
- 136.97592478990555
- 126.36036419868469
- 137.6713181734085
- 129.41974651813507
- 149.1537730693817
- 134.56934124231339
- 132.53309494256973
- 123.62596064805984
- 147.26879435777664
- 169.2069507241249
- 149.396519780159
- 158.09419924020767
- 142.17019361257553
- 136.19849264621735
- 138.13962179422379
- 130.79927122592926
- 165.34681010246277
- 155.34876948595047
- 148.06908184289932
- 152.88755702972412
- 152.9047377705574
train_accuracy:
- 0.15
- 0.312
- 0.3
- 0.4
- 0.057
- 0.577
- 0.0
- 0.891
- 0.362
- 0.5
- 0.95
- 0.93
- 0.742
- 0.62
- 0.564
- 0.894
- 0.025
- 0.605
- 0.327
- 0.331
- 0.344
- 0.8
- 0.8
- 0.4
- 0.583
- 0.57
- 0.758
- 0.639
- 0.776
- 0.857
- 0.876
- 0.863
- 0.655
- 0.89
- 0.693
- 0.812
- 0.777
- 0.5
- 0.763
- 0.486
- 0.383
- 0.825
- 0.841
- 0.597
- 0.675
- 0.482
- 0.692
- 0.906
- 0.468
- 0.64
- 0.938
- 0.825
- 0.854
- 0.69
- 0.814
- 0.703
- 0.527
- 0.761
- 0.892
- 0.932
- 0.219
- 0.928
- 0.93
- 0.641
- 0.452
- 0.95
- 0.632
- 0.591
- 0.717
- 0.164
- 0.525
- 0.707
- 0.867
- 0.735
- 0.763
- 0.527
- 0.9
- 0.509
- 0.737
- 0.761
- 0.39
- 0.735
- 0.807
- 0.436
- 0.808
- 0.95
- 0.542
- 0.422
- 0.815
- 0.106
- 0.975
- 0.645
- 0.656
- 0.85
- 0.467
- 0.65
- 0.782
- 0.196
- 0.595
- 0.84
train_loss:
- 1.135
- 0.744
- 0.674
- 0.571
- 0.561
- 0.416
- 0.392
- 0.465
- 0.44
- 0.492
- 0.417
- 0.444
- 0.403
- 0.46
- 0.392
- 0.403
- 0.332
- 0.39
- 0.452
- 0.413
- 0.31
- 0.45
- 0.317
- 0.431
- 0.331
- 0.421
- 0.375
- 0.356
- 0.377
- 0.4
- 0.401
- 0.322
- 0.394
- 0.36
- 0.413
- 0.398
- 0.392
- 0.315
- 0.386
- 0.334
- 0.243
- 0.391
- 0.349
- 0.338
- 0.324
- 0.36
- 0.346
- 0.311
- 0.379
- 0.359
- 0.278
- 0.375
- 0.321
- 0.395
- 0.352
- 0.402
- 0.327
- 0.299
- 0.4
- 0.417
- 0.353
- 0.327
- 0.39
- 0.332
- 0.356
- 0.314
- 0.352
- 0.343
- 0.365
- 0.352
- 0.276
- 0.323
- 0.348
- 0.345
- 0.301
- 0.276
- 0.321
- 0.36
- 0.29
- 0.333
- 0.28
- 0.35
- 0.295
- 0.321
- 0.359
- 0.364
- 0.377
- 0.401
- 0.31
- 0.35
- 0.3
- 0.339
- 0.318
- 0.276
- 0.325
- 0.298
- 0.284
- 0.343
- 0.31
- 0.242
unequal: 1
verbose: 1
