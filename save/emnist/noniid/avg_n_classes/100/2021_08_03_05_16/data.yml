avg_train_accuracy: 0.9
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.11436170212765957
- 0.23340425531914893
- 0.3373404255319149
- 0.37329787234042555
- 0.43148936170212765
- 0.3867553191489362
- 0.5109574468085106
- 0.44920212765957446
- 0.4803723404255319
- 0.468031914893617
- 0.46845744680851065
- 0.5952127659574468
- 0.5673404255319149
- 0.5817553191489362
- 0.5423404255319149
- 0.5464893617021277
- 0.5862234042553192
- 0.5737765957446809
- 0.6108510638297873
- 0.620904255319149
- 0.5686170212765957
- 0.5894680851063829
- 0.6413297872340425
- 0.6185106382978723
- 0.6063297872340425
- 0.5817553191489362
- 0.596436170212766
- 0.6434042553191489
- 0.6201595744680851
- 0.6345744680851064
- 0.6372340425531915
- 0.5912765957446808
- 0.6179787234042553
- 0.6370212765957447
- 0.6748404255319149
- 0.6512765957446809
- 0.6201063829787234
- 0.6805851063829788
- 0.6445744680851064
- 0.6388297872340426
- 0.6277127659574468
- 0.6046808510638297
- 0.6665425531914894
- 0.6956914893617021
- 0.6553191489361702
- 0.6329255319148936
- 0.6245744680851064
- 0.604468085106383
- 0.7088829787234042
- 0.6631914893617021
- 0.6441489361702127
- 0.665372340425532
- 0.6507978723404255
- 0.6187765957446808
- 0.6218085106382979
- 0.6432978723404256
- 0.6619680851063829
- 0.6839893617021277
- 0.6107978723404255
- 0.6398404255319149
- 0.675
- 0.6644148936170213
- 0.6863297872340426
- 0.6598936170212766
- 0.6522340425531915
- 0.62
- 0.6445212765957447
- 0.6364893617021277
- 0.6487234042553192
- 0.6800531914893617
- 0.6115425531914893
- 0.6693617021276596
- 0.6432446808510638
- 0.6252127659574468
- 0.6961702127659575
- 0.6540425531914894
- 0.6302127659574468
- 0.6488829787234043
- 0.6967021276595745
- 0.6580851063829787
- 0.6390425531914894
- 0.6650531914893617
- 0.6868617021276596
- 0.6509574468085106
- 0.6572872340425532
- 0.6745744680851063
- 0.6340957446808511
- 0.6590957446808511
- 0.6625531914893616
- 0.6895744680851064
- 0.7007446808510638
- 0.7321276595744681
- 0.6662234042553191
- 0.6656914893617021
- 0.6693085106382979
- 0.6107446808510638
- 0.6161170212765957
- 0.6712234042553191
- 0.6951595744680851
- 0.6707978723404255
test_loss_list:
- 533.349267244339
- 443.3799705505371
- 366.1240634918213
- 326.6520606279373
- 286.6127576828003
- 306.15948021411896
- 237.2368279695511
- 261.519873380661
- 237.72907030582428
- 233.4453525543213
- 237.90249502658844
- 190.93334484100342
- 205.10680162906647
- 194.9035278558731
- 210.06727075576782
- 206.17746353149414
- 181.5617297887802
- 189.72607326507568
- 171.6341574192047
- 170.57133120298386
- 209.7878314256668
- 177.82532089948654
- 163.97638136148453
- 171.94672960042953
- 181.19644153118134
- 182.3253380060196
- 163.3766360282898
- 160.70852434635162
- 167.12205988168716
- 156.09454250335693
- 151.30837339162827
- 182.55121040344238
- 160.26261949539185
- 158.87972927093506
- 142.25080972909927
- 156.94516199827194
- 156.28417456150055
- 139.97090542316437
- 153.6178936958313
- 162.57539349794388
- 171.74326407909393
- 162.21882379055023
- 153.25904631614685
- 133.44478863477707
- 143.3592791557312
- 161.00077229738235
- 151.23874974250793
- 174.9490893483162
- 132.32631409168243
- 148.43564528226852
- 150.9000083208084
- 147.7255340218544
- 146.38039499521255
- 161.2313820719719
- 165.17914932966232
- 167.08472806215286
- 142.94727635383606
- 137.7906470298767
- 159.8299376964569
- 161.3992133140564
- 134.86398482322693
- 147.15527445077896
- 140.00322037935257
- 141.96380335092545
- 139.64128059148788
- 145.01766282320023
- 148.87048321962357
- 157.78725081682205
- 154.29343855381012
- 143.65783816576004
- 165.57388997077942
- 139.58363872766495
- 140.4820825457573
- 151.05454391241074
- 130.43498468399048
- 146.83824688196182
- 144.448881149292
- 146.80608212947845
- 125.3738249540329
- 141.5356183052063
- 151.57429444789886
- 135.65236777067184
- 139.87047058343887
- 149.66595405340195
- 147.23987710475922
- 139.52446192502975
- 148.3401975631714
- 143.90831291675568
- 136.75545746088028
- 128.78214663267136
- 120.8306245803833
- 122.37118929624557
- 133.91111433506012
- 133.6422694325447
- 135.17774134874344
- 165.02474910020828
- 159.28886115550995
- 139.21491461992264
- 128.11175966262817
- 135.4345538020134
train_accuracy:
- 0.0
- 0.082
- 0.264
- 0.119
- 0.528
- 0.556
- 0.983
- 0.383
- 0.586
- 0.975
- 0.513
- 0.67
- 0.808
- 0.762
- 0.658
- 0.879
- 0.171
- 0.963
- 0.85
- 0.95
- 0.75
- 0.647
- 0.625
- 0.721
- 0.629
- 0.169
- 0.896
- 0.93
- 0.706
- 0.6
- 0.165
- 0.188
- 0.523
- 0.844
- 0.9
- 0.325
- 0.833
- 0.264
- 0.619
- 0.511
- 0.625
- 0.82
- 0.879
- 0.86
- 0.743
- 0.286
- 0.929
- 0.836
- 0.856
- 0.915
- 0.912
- 0.889
- 0.978
- 0.1
- 0.95
- 0.462
- 0.315
- 0.763
- 0.783
- 0.531
- 0.944
- 0.975
- 0.971
- 0.833
- 0.881
- 0.258
- 0.691
- 0.6
- 0.756
- 0.783
- 0.775
- 0.958
- 0.615
- 0.207
- 0.955
- 0.441
- 0.815
- 0.7
- 0.873
- 0.956
- 0.843
- 0.912
- 0.585
- 0.888
- 0.581
- 0.925
- 0.558
- 0.587
- 0.606
- 0.733
- 0.829
- 0.917
- 0.907
- 0.79
- 0.778
- 0.778
- 0.72
- 0.597
- 0.883
- 0.9
train_loss:
- 1.143
- 0.835
- 0.793
- 0.482
- 0.606
- 0.604
- 0.447
- 0.4
- 0.484
- 0.424
- 0.437
- 0.468
- 0.465
- 0.494
- 0.39
- 0.372
- 0.406
- 0.375
- 0.436
- 0.334
- 0.416
- 0.407
- 0.502
- 0.417
- 0.388
- 0.388
- 0.334
- 0.384
- 0.479
- 0.417
- 0.375
- 0.356
- 0.355
- 0.345
- 0.356
- 0.402
- 0.342
- 0.432
- 0.343
- 0.308
- 0.331
- 0.392
- 0.292
- 0.355
- 0.384
- 0.392
- 0.325
- 0.393
- 0.335
- 0.289
- 0.283
- 0.345
- 0.42
- 0.328
- 0.292
- 0.328
- 0.315
- 0.384
- 0.336
- 0.361
- 0.362
- 0.285
- 0.312
- 0.335
- 0.349
- 0.328
- 0.342
- 0.349
- 0.297
- 0.305
- 0.381
- 0.336
- 0.363
- 0.359
- 0.332
- 0.343
- 0.36
- 0.381
- 0.365
- 0.383
- 0.272
- 0.342
- 0.392
- 0.289
- 0.326
- 0.3
- 0.349
- 0.297
- 0.349
- 0.351
- 0.392
- 0.253
- 0.387
- 0.333
- 0.369
- 0.33
- 0.345
- 0.304
- 0.322
- 0.308
unequal: 1
verbose: 1
