avg_train_accuracy: 0.04
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.12824468085106383
- 0.1900531914893617
- 0.2721808510638298
- 0.3536170212765957
- 0.3650531914893617
- 0.45441489361702125
- 0.44845744680851063
- 0.4871276595744681
- 0.5382446808510638
- 0.5336702127659575
- 0.5367553191489361
- 0.5623404255319149
- 0.5128191489361702
- 0.5902127659574468
- 0.5704787234042553
- 0.5654255319148936
- 0.5317021276595745
- 0.614468085106383
- 0.5921276595744681
- 0.6251595744680851
- 0.5784574468085106
- 0.6187234042553191
- 0.5909042553191489
- 0.6579255319148937
- 0.6501595744680851
- 0.5749468085106383
- 0.6046808510638297
- 0.5795744680851064
- 0.6498936170212766
- 0.6268617021276596
- 0.5965425531914894
- 0.608404255319149
- 0.6768085106382978
- 0.6177127659574468
- 0.6732446808510638
- 0.6517021276595745
- 0.6788297872340425
- 0.636436170212766
- 0.6802127659574468
- 0.6651063829787234
- 0.6463297872340426
- 0.6485106382978724
- 0.6464893617021277
- 0.6251595744680851
- 0.6604787234042553
- 0.6704255319148936
- 0.6512765957446809
- 0.7036170212765958
- 0.6549468085106382
- 0.7101595744680851
- 0.7062765957446808
- 0.653563829787234
- 0.6925
- 0.6919148936170213
- 0.6165425531914893
- 0.694627659574468
- 0.6394148936170213
- 0.6168617021276596
- 0.5778723404255319
- 0.6806914893617021
- 0.7051595744680851
- 0.6519148936170213
- 0.6384574468085107
- 0.6538829787234043
- 0.6852127659574468
- 0.6471808510638298
- 0.6307446808510638
- 0.6949468085106383
- 0.6039893617021277
- 0.6757978723404255
- 0.6263829787234042
- 0.7186170212765958
- 0.6620212765957447
- 0.6971808510638298
- 0.6842553191489362
- 0.6432978723404256
- 0.6678191489361702
- 0.6551595744680851
- 0.7034574468085106
- 0.6473404255319148
- 0.649468085106383
- 0.6960106382978724
- 0.6187234042553191
- 0.6423404255319148
- 0.6443085106382979
- 0.5969148936170213
- 0.6847340425531915
- 0.7489893617021277
- 0.7208510638297873
- 0.6674468085106383
- 0.6888297872340425
- 0.6671276595744681
- 0.6505319148936171
- 0.6808510638297872
- 0.7010106382978724
- 0.6558510638297872
- 0.6226063829787234
- 0.6777127659574468
- 0.6872340425531915
- 0.6341489361702127
test_loss_list:
- 540.6666107177734
- 465.01640367507935
- 399.8071143627167
- 357.3925607204437
- 329.35162830352783
- 270.9312741756439
- 278.3246737718582
- 250.0988414287567
- 226.53683638572693
- 216.57357811927795
- 213.25233221054077
- 202.94678938388824
- 212.6011426448822
- 188.93303221464157
- 196.83521437644958
- 182.1974121928215
- 195.84489035606384
- 160.980910718441
- 171.0965782403946
- 157.82574719190598
- 186.7620632648468
- 172.8326386809349
- 171.89237248897552
- 150.40494513511658
- 155.30326968431473
- 181.93883764743805
- 164.9813935160637
- 189.11994123458862
- 155.688625395298
- 160.48442459106445
- 165.86798697710037
- 171.36116099357605
- 148.06688421964645
- 160.89551043510437
- 141.26970911026
- 148.47126811742783
- 140.621020257473
- 155.627800822258
- 139.86582452058792
- 149.18467634916306
- 148.14735114574432
- 146.23402774333954
- 157.0078867673874
- 172.87229281663895
- 148.3634067773819
- 135.8068591952324
- 141.55182337760925
- 128.15162801742554
- 148.75632697343826
- 133.95600068569183
- 127.32136678695679
- 153.5038564801216
- 127.37887579202652
- 123.22119152545929
- 157.6759576201439
- 142.27691441774368
- 157.74204295873642
- 152.07008904218674
- 184.35605031251907
- 131.96821069717407
- 125.17228090763092
- 166.50288343429565
- 163.69844263792038
- 146.6348522901535
- 142.04720121622086
- 150.8775658607483
- 160.86480993032455
- 129.37810844182968
- 190.9853172302246
- 128.63786232471466
- 154.32940012216568
- 123.68031585216522
- 146.15033340454102
- 129.25768852233887
- 137.8347846865654
- 150.41408014297485
- 136.00732773542404
- 142.21388000249863
- 128.25357073545456
- 159.91498988866806
- 152.1289029121399
- 128.46386528015137
- 162.59160429239273
- 162.25323456525803
- 149.60967183113098
- 165.30014795064926
- 126.82391786575317
- 109.88467222452164
- 123.43608158826828
- 140.23473745584488
- 129.0145561695099
- 147.99145758152008
- 154.8846474289894
- 133.54742217063904
- 124.63182854652405
- 136.48532211780548
- 151.59274023771286
- 128.39780181646347
- 132.89897179603577
- 140.1436546444893
train_accuracy:
- 0.0
- 0.0
- 0.0
- 0.0
- 0.678
- 0.707
- 0.66
- 0.491
- 0.279
- 0.575
- 0.427
- 0.817
- 1.0
- 0.83
- 0.99
- 0.62
- 0.381
- 0.715
- 0.379
- 0.056
- 0.77
- 0.533
- 0.472
- 0.554
- 0.483
- 0.782
- 0.508
- 0.82
- 0.84
- 0.732
- 0.861
- 0.778
- 0.025
- 0.761
- 0.874
- 0.933
- 0.674
- 0.602
- 0.463
- 0.925
- 0.747
- 0.632
- 0.893
- 0.931
- 0.671
- 0.817
- 0.93
- 0.932
- 0.8
- 0.818
- 0.805
- 0.638
- 0.674
- 0.688
- 0.987
- 0.663
- 0.546
- 0.604
- 0.603
- 0.843
- 0.661
- 0.606
- 0.46
- 0.571
- 0.438
- 0.55
- 0.243
- 0.633
- 0.525
- 0.86
- 0.4
- 0.495
- 0.6
- 0.642
- 0.427
- 1.0
- 0.586
- 0.827
- 0.933
- 0.914
- 0.494
- 0.85
- 0.672
- 0.679
- 0.533
- 0.5
- 0.607
- 0.775
- 0.839
- 0.6
- 0.754
- 0.668
- 0.6
- 0.98
- 0.838
- 0.821
- 0.675
- 0.76
- 0.93
- 0.04
train_loss:
- 1.053
- 0.786
- 0.729
- 0.545
- 0.462
- 0.514
- 0.505
- 0.553
- 0.471
- 0.476
- 0.451
- 0.418
- 0.455
- 0.376
- 0.414
- 0.314
- 0.478
- 0.413
- 0.439
- 0.382
- 0.347
- 0.385
- 0.384
- 0.386
- 0.401
- 0.463
- 0.417
- 0.364
- 0.333
- 0.41
- 0.336
- 0.337
- 0.256
- 0.368
- 0.378
- 0.336
- 0.362
- 0.406
- 0.285
- 0.38
- 0.425
- 0.336
- 0.385
- 0.352
- 0.38
- 0.393
- 0.3
- 0.344
- 0.345
- 0.354
- 0.322
- 0.37
- 0.364
- 0.255
- 0.308
- 0.339
- 0.296
- 0.309
- 0.316
- 0.363
- 0.367
- 0.331
- 0.327
- 0.367
- 0.329
- 0.371
- 0.326
- 0.349
- 0.314
- 0.353
- 0.27
- 0.361
- 0.397
- 0.326
- 0.351
- 0.232
- 0.328
- 0.354
- 0.366
- 0.322
- 0.291
- 0.26
- 0.244
- 0.253
- 0.388
- 0.309
- 0.312
- 0.339
- 0.292
- 0.363
- 0.346
- 0.335
- 0.313
- 0.337
- 0.379
- 0.284
- 0.323
- 0.223
- 0.38
- 0.288
unequal: 1
verbose: 1
