avg_train_accuracy: 0.317
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1925531914893617
- 0.26388297872340427
- 0.3051063829787234
- 0.3184574468085106
- 0.349468085106383
- 0.3984042553191489
- 0.4068617021276596
- 0.4727659574468085
- 0.48675531914893616
- 0.5109574468085106
- 0.5569148936170213
- 0.5234574468085106
- 0.5627127659574468
- 0.5581382978723404
- 0.528031914893617
- 0.5128723404255319
- 0.5124468085106383
- 0.5666489361702127
- 0.5735638297872341
- 0.6482978723404256
- 0.6228191489361702
- 0.5899468085106383
- 0.6059574468085106
- 0.5868085106382979
- 0.6422340425531915
- 0.6422340425531915
- 0.614095744680851
- 0.6564893617021277
- 0.6001595744680851
- 0.6497872340425532
- 0.6885638297872341
- 0.6593617021276595
- 0.660372340425532
- 0.6369148936170212
- 0.68
- 0.6882978723404255
- 0.604468085106383
- 0.6686702127659574
- 0.6514893617021277
- 0.6399468085106383
- 0.6694148936170212
- 0.6555319148936171
- 0.6505851063829787
- 0.6482978723404256
- 0.6306382978723404
- 0.6498936170212766
- 0.6306382978723404
- 0.6700531914893617
- 0.6445744680851064
- 0.700531914893617
- 0.6618617021276596
- 0.660372340425532
- 0.65
- 0.6385106382978724
- 0.6143617021276596
- 0.6691489361702128
- 0.6282446808510638
- 0.6260106382978723
- 0.6500531914893617
- 0.6678191489361702
- 0.6370212765957447
- 0.6291489361702127
- 0.6602127659574468
- 0.6797340425531915
- 0.6513297872340426
- 0.6564893617021277
- 0.648563829787234
- 0.6246276595744681
- 0.6667553191489362
- 0.6784574468085106
- 0.6393085106382979
- 0.6327127659574469
- 0.6412234042553191
- 0.6687234042553192
- 0.696968085106383
- 0.6492021276595744
- 0.6621276595744681
- 0.7252659574468086
- 0.6990957446808511
- 0.7068085106382979
- 0.6681914893617021
- 0.6886170212765957
- 0.6907978723404256
- 0.6888297872340425
- 0.6556914893617021
- 0.6838297872340425
- 0.6426063829787234
- 0.6912234042553191
- 0.715
- 0.6614893617021277
- 0.6597340425531915
- 0.638031914893617
- 0.6760106382978723
- 0.6918617021276596
- 0.6638297872340425
- 0.6011170212765957
- 0.675531914893617
- 0.6656382978723404
- 0.6851595744680851
- 0.6377659574468085
test_loss_list:
- 516.2183797359467
- 420.4333031177521
- 402.553462266922
- 380.9795858860016
- 338.7918074131012
- 309.04312896728516
- 297.2364559173584
- 255.74169433116913
- 238.22937202453613
- 242.02913463115692
- 199.20248460769653
- 219.64019787311554
- 187.89349126815796
- 187.17594969272614
- 197.67479610443115
- 202.27351474761963
- 228.69199585914612
- 202.41221940517426
- 177.46532928943634
- 155.53916066884995
- 161.92712581157684
- 176.44509506225586
- 178.7208176255226
- 172.0944389104843
- 168.6084207892418
- 153.38681948184967
- 169.3238245844841
- 152.86209517717361
- 178.77113938331604
- 153.05628168582916
- 138.0626105070114
- 144.26890152692795
- 157.78759664297104
- 153.87315952777863
- 137.71883863210678
- 141.76040226221085
- 182.07548755407333
- 140.9337738752365
- 159.0817790031433
- 149.31905490159988
- 158.47105085849762
- 141.67584139108658
- 140.74477577209473
- 168.00785386562347
- 157.2311999797821
- 147.74607837200165
- 158.9726024866104
- 148.56138229370117
- 165.50282949209213
- 135.68848991394043
- 145.41855973005295
- 148.75659829378128
- 139.7319695353508
- 159.30885803699493
- 166.53146624565125
- 140.4049642086029
- 150.85277831554413
- 156.58361279964447
- 149.07914745807648
- 145.95557284355164
- 159.6992735862732
- 163.3143379688263
- 153.67397558689117
- 130.3436443209648
- 153.1893054842949
- 135.4619272351265
- 149.5873031616211
- 146.9855102300644
- 149.45878154039383
- 139.02204883098602
- 166.7854090332985
- 156.10777842998505
- 168.71158879995346
- 144.32941037416458
- 138.7962549328804
- 144.61092329025269
- 144.13940608501434
- 122.19599628448486
- 126.34464120864868
- 127.21011650562286
- 140.78989565372467
- 133.68052679300308
- 128.37803882360458
- 135.89504981040955
- 144.9422134757042
- 137.5004504919052
- 148.2291303873062
- 127.46679645776749
- 126.27831864356995
- 134.661627471447
- 158.97016137838364
- 162.29140347242355
- 135.80825179815292
- 141.17330062389374
- 142.4992641210556
- 175.54660338163376
- 134.40004789829254
- 148.93127644062042
- 128.54728829860687
- 152.9543406367302
train_accuracy:
- 0.066
- 0.1
- 0.319
- 0.154
- 0.96
- 0.83
- 0.023
- 0.303
- 0.705
- 0.289
- 0.0
- 0.716
- 0.464
- 0.706
- 0.508
- 0.575
- 0.711
- 0.661
- 0.85
- 0.571
- 0.2
- 0.377
- 0.911
- 0.717
- 0.975
- 0.862
- 0.495
- 0.561
- 0.0
- 0.912
- 0.354
- 0.892
- 0.775
- 0.753
- 0.844
- 0.892
- 0.912
- 0.763
- 0.728
- 0.694
- 0.983
- 0.859
- 0.95
- 0.581
- 0.888
- 0.95
- 0.785
- 0.789
- 0.0
- 0.88
- 0.75
- 0.388
- 0.706
- 0.05
- 0.427
- 0.837
- 0.732
- 0.638
- 0.444
- 0.675
- 0.619
- 0.642
- 0.6
- 0.25
- 0.807
- 0.611
- 0.944
- 0.6
- 0.883
- 0.875
- 0.256
- 0.82
- 0.575
- 0.891
- 0.424
- 0.37
- 0.85
- 0.876
- 0.72
- 0.425
- 0.963
- 0.671
- 0.825
- 0.867
- 0.75
- 0.925
- 0.963
- 0.85
- 0.803
- 0.471
- 0.861
- 0.527
- 0.608
- 0.483
- 0.634
- 0.818
- 0.643
- 0.589
- 0.843
- 0.317
train_loss:
- 1.252
- 0.786
- 0.622
- 0.571
- 0.46
- 0.473
- 0.591
- 0.601
- 0.496
- 0.492
- 0.447
- 0.403
- 0.492
- 0.477
- 0.411
- 0.367
- 0.394
- 0.456
- 0.353
- 0.448
- 0.432
- 0.333
- 0.394
- 0.449
- 0.474
- 0.366
- 0.367
- 0.38
- 0.347
- 0.44
- 0.383
- 0.336
- 0.359
- 0.325
- 0.405
- 0.36
- 0.297
- 0.361
- 0.321
- 0.3
- 0.345
- 0.359
- 0.333
- 0.266
- 0.307
- 0.439
- 0.404
- 0.388
- 0.326
- 0.328
- 0.342
- 0.34
- 0.339
- 0.304
- 0.3
- 0.393
- 0.332
- 0.378
- 0.405
- 0.296
- 0.291
- 0.364
- 0.341
- 0.291
- 0.333
- 0.333
- 0.315
- 0.337
- 0.376
- 0.349
- 0.315
- 0.364
- 0.296
- 0.307
- 0.288
- 0.322
- 0.31
- 0.383
- 0.346
- 0.308
- 0.351
- 0.359
- 0.323
- 0.325
- 0.379
- 0.358
- 0.335
- 0.323
- 0.359
- 0.419
- 0.313
- 0.319
- 0.346
- 0.314
- 0.32
- 0.297
- 0.368
- 0.296
- 0.344
- 0.29
unequal: 1
verbose: 1
