avg_train_accuracy: 0.745
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1499468085106383
- 0.18941489361702127
- 0.34117021276595744
- 0.39904255319148935
- 0.4225
- 0.41212765957446806
- 0.4368085106382979
- 0.5052659574468085
- 0.4837234042553191
- 0.5163297872340425
- 0.5004787234042554
- 0.5656382978723404
- 0.4786170212765957
- 0.5812765957446808
- 0.5932446808510639
- 0.6094148936170213
- 0.541595744680851
- 0.526436170212766
- 0.5538829787234043
- 0.6387234042553191
- 0.5620744680851064
- 0.6024468085106383
- 0.5850531914893617
- 0.6412234042553191
- 0.6448936170212766
- 0.6278191489361702
- 0.6076595744680852
- 0.6473404255319148
- 0.5943617021276596
- 0.5898404255319148
- 0.6121276595744681
- 0.6727127659574468
- 0.5875531914893617
- 0.5968085106382979
- 0.5987234042553191
- 0.6538297872340425
- 0.6574468085106383
- 0.6098936170212766
- 0.6378191489361702
- 0.6519148936170213
- 0.6317021276595745
- 0.6082978723404255
- 0.6363829787234042
- 0.6334042553191489
- 0.653031914893617
- 0.6404787234042553
- 0.6278191489361702
- 0.632127659574468
- 0.6264893617021277
- 0.5829255319148936
- 0.6541489361702127
- 0.7177127659574468
- 0.6655851063829787
- 0.6855851063829788
- 0.6897340425531915
- 0.6601063829787234
- 0.6241489361702127
- 0.6423936170212766
- 0.6487234042553192
- 0.6241489361702127
- 0.6263297872340425
- 0.7118085106382979
- 0.6418085106382979
- 0.6689893617021276
- 0.6921276595744681
- 0.6445212765957447
- 0.66
- 0.6554787234042553
- 0.6783510638297873
- 0.6456914893617022
- 0.6551595744680851
- 0.6215957446808511
- 0.6443617021276595
- 0.650904255319149
- 0.6742553191489362
- 0.6140425531914894
- 0.6685106382978724
- 0.6566489361702128
- 0.633031914893617
- 0.624468085106383
- 0.6901063829787234
- 0.6887234042553192
- 0.6756914893617021
- 0.6467553191489361
- 0.6031914893617021
- 0.6295744680851064
- 0.6284574468085107
- 0.6468085106382979
- 0.6299468085106383
- 0.6418617021276596
- 0.6826595744680851
- 0.6182978723404255
- 0.6806382978723404
- 0.7054255319148937
- 0.6770212765957446
- 0.6935638297872341
- 0.6964361702127659
- 0.6736702127659574
- 0.6556382978723404
- 0.7115957446808511
test_loss_list:
- 522.297413110733
- 441.17430114746094
- 355.9809980392456
- 320.3700910806656
- 284.0519597530365
- 280.6009864807129
- 273.4098154306412
- 235.86133229732513
- 258.00702595710754
- 225.33180975914001
- 218.09826576709747
- 196.9407433271408
- 243.0762790441513
- 181.1881502866745
- 193.10739278793335
- 179.7515407204628
- 213.2375580072403
- 208.78833734989166
- 199.00336694717407
- 159.61570745706558
- 187.48547261953354
- 165.5539150238037
- 205.38565266132355
- 151.9321254491806
- 150.72803449630737
- 149.7779496908188
- 177.22589260339737
- 154.35793298482895
- 173.3261221051216
- 174.19382095336914
- 168.43179780244827
- 136.15151035785675
- 193.47444820404053
- 174.31882190704346
- 169.24526625871658
- 152.41544383764267
- 152.80016630887985
- 182.06119883060455
- 162.98783260583878
- 146.49478662014008
- 161.64331531524658
- 169.2539283633232
- 158.5340400338173
- 164.20337408781052
- 140.06168538331985
- 149.3234402537346
- 152.77977031469345
- 147.67062896490097
- 152.37538504600525
- 176.70502752065659
- 150.8832187652588
- 118.94323146343231
- 142.04490727186203
- 139.29194539785385
- 132.49053412675858
- 139.2825877070427
- 155.46771848201752
- 151.2931963801384
- 158.1064065694809
- 148.10904276371002
- 153.178839802742
- 120.2941101193428
- 147.8067861199379
- 136.43920189142227
- 134.87008929252625
- 161.70599949359894
- 159.70728826522827
- 141.51784372329712
- 137.45453083515167
- 152.49096810817719
- 131.42123353481293
- 145.63646310567856
- 154.87289416790009
- 152.19673418998718
- 129.5596165060997
- 152.52305567264557
- 142.29471397399902
- 152.11753886938095
- 154.94398993253708
- 156.22073644399643
- 137.03597593307495
- 136.08709979057312
- 129.6688802242279
- 149.9480238556862
- 167.66210055351257
- 150.65914857387543
- 156.50073808431625
- 140.2710508108139
- 143.98092871904373
- 160.34819662570953
- 131.45988768339157
- 146.83482497930527
- 123.70608824491501
- 124.76338922977448
- 136.0330646634102
- 128.7777053117752
- 128.09623485803604
- 127.86203992366791
- 140.3702756166458
- 118.91488415002823
train_accuracy:
- 0.506
- 0.003
- 0.033
- 0.35
- 0.261
- 0.5
- 0.607
- 0.482
- 0.431
- 0.89
- 0.944
- 0.623
- 0.0
- 0.66
- 0.0
- 0.464
- 0.775
- 0.94
- 0.95
- 0.629
- 0.854
- 0.317
- 0.107
- 0.962
- 0.683
- 0.96
- 0.389
- 0.85
- 0.676
- 0.822
- 0.617
- 0.712
- 0.9
- 0.908
- 0.314
- 0.688
- 0.775
- 0.911
- 0.483
- 0.82
- 0.391
- 0.444
- 0.85
- 0.575
- 0.431
- 0.775
- 0.771
- 0.262
- 0.631
- 0.794
- 0.9
- 0.754
- 0.892
- 0.567
- 0.212
- 0.944
- 0.841
- 0.947
- 0.692
- 0.4
- 0.789
- 0.538
- 0.525
- 0.883
- 0.866
- 0.179
- 0.66
- 0.0
- 0.538
- 0.578
- 0.958
- 0.433
- 0.93
- 0.682
- 0.654
- 0.612
- 0.8
- 0.587
- 0.694
- 0.503
- 0.9
- 0.862
- 0.66
- 0.576
- 0.539
- 0.017
- 0.679
- 0.922
- 0.371
- 0.825
- 0.605
- 0.944
- 0.618
- 0.942
- 0.619
- 0.725
- 0.892
- 0.914
- 0.321
- 0.745
train_loss:
- 1.225
- 0.886
- 0.613
- 0.573
- 0.53
- 0.558
- 0.527
- 0.474
- 0.402
- 0.471
- 0.48
- 0.395
- 0.428
- 0.408
- 0.45
- 0.41
- 0.416
- 0.431
- 0.41
- 0.402
- 0.452
- 0.335
- 0.403
- 0.427
- 0.303
- 0.351
- 0.441
- 0.36
- 0.355
- 0.316
- 0.43
- 0.373
- 0.389
- 0.261
- 0.347
- 0.383
- 0.31
- 0.404
- 0.38
- 0.376
- 0.395
- 0.348
- 0.354
- 0.318
- 0.301
- 0.338
- 0.325
- 0.351
- 0.427
- 0.334
- 0.304
- 0.378
- 0.36
- 0.35
- 0.396
- 0.416
- 0.344
- 0.395
- 0.293
- 0.349
- 0.334
- 0.369
- 0.413
- 0.306
- 0.312
- 0.339
- 0.346
- 0.307
- 0.325
- 0.352
- 0.384
- 0.309
- 0.338
- 0.389
- 0.37
- 0.332
- 0.399
- 0.369
- 0.344
- 0.34
- 0.345
- 0.348
- 0.288
- 0.31
- 0.345
- 0.313
- 0.325
- 0.308
- 0.334
- 0.333
- 0.35
- 0.325
- 0.37
- 0.324
- 0.354
- 0.324
- 0.34
- 0.4
- 0.304
- 0.348
unequal: 1
verbose: 1
