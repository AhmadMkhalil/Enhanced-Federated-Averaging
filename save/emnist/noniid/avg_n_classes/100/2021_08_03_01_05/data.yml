avg_train_accuracy: 0.518
avg_train_loss: 0.004
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1955851063829787
- 0.2473936170212766
- 0.3730851063829787
- 0.3927127659574468
- 0.3197340425531915
- 0.4168085106382979
- 0.47936170212765955
- 0.47893617021276597
- 0.4664893617021277
- 0.5273404255319148
- 0.5253723404255319
- 0.5223936170212766
- 0.5177127659574469
- 0.5873404255319149
- 0.5652127659574468
- 0.5717553191489362
- 0.5214893617021277
- 0.5491489361702128
- 0.6030319148936171
- 0.6779255319148936
- 0.6086702127659575
- 0.5733510638297873
- 0.5843617021276596
- 0.5944680851063829
- 0.6085106382978723
- 0.5768617021276595
- 0.6230851063829788
- 0.6131914893617021
- 0.6131382978723404
- 0.701063829787234
- 0.6171276595744681
- 0.6512234042553191
- 0.6579255319148937
- 0.6453191489361703
- 0.6230851063829788
- 0.5803723404255319
- 0.6275
- 0.6738297872340425
- 0.6266489361702128
- 0.6477659574468085
- 0.5858510638297872
- 0.6248404255319149
- 0.636968085106383
- 0.6414893617021277
- 0.6086702127659575
- 0.6452127659574468
- 0.6322872340425532
- 0.6557446808510639
- 0.6489893617021276
- 0.6037234042553191
- 0.6295212765957446
- 0.6495744680851064
- 0.6082446808510639
- 0.6009574468085106
- 0.6861702127659575
- 0.6616489361702128
- 0.6463297872340426
- 0.6320744680851064
- 0.6388297872340426
- 0.6935106382978723
- 0.6621808510638297
- 0.6038829787234042
- 0.6280851063829788
- 0.6526595744680851
- 0.6244148936170213
- 0.6236702127659575
- 0.6595212765957447
- 0.6331914893617021
- 0.7030851063829787
- 0.6804255319148936
- 0.6384042553191489
- 0.6560106382978723
- 0.6629787234042553
- 0.645
- 0.6456914893617022
- 0.6565425531914894
- 0.6747872340425531
- 0.6664893617021277
- 0.6747340425531915
- 0.71
- 0.696063829787234
- 0.6857978723404256
- 0.6581382978723405
- 0.721968085106383
- 0.6873936170212765
- 0.6511170212765958
- 0.6493617021276595
- 0.6732978723404255
- 0.6360106382978723
- 0.7107446808510638
- 0.6847340425531915
- 0.6848404255319149
- 0.6777127659574468
- 0.7116489361702127
- 0.6710106382978723
- 0.7026063829787234
- 0.6605319148936171
- 0.640531914893617
- 0.6463829787234042
- 0.6475531914893617
test_loss_list:
- 523.8477504253387
- 451.1289768218994
- 365.9857108592987
- 310.90747928619385
- 332.90095841884613
- 308.8774482011795
- 266.29088270664215
- 248.31015121936798
- 239.38690567016602
- 217.2015837430954
- 220.21484649181366
- 222.86953210830688
- 223.45744955539703
- 174.14766818284988
- 199.3153121471405
- 197.3821405172348
- 212.86038720607758
- 216.45761692523956
- 176.17179989814758
- 154.96523970365524
- 166.49672305583954
- 185.7217736840248
- 182.23572719097137
- 176.4769955277443
- 179.87173998355865
- 186.53189903497696
- 164.22152853012085
- 170.38858437538147
- 165.81490349769592
- 138.030690908432
- 181.36664348840714
- 156.10351276397705
- 147.6678368449211
- 154.81497311592102
- 154.4140378832817
- 177.26765590906143
- 160.138642847538
- 136.14813792705536
- 156.38523238897324
- 153.63791418075562
- 202.1916697025299
- 162.57400059700012
- 155.35826754570007
- 142.61731284856796
- 163.8663399219513
- 158.82169657945633
- 148.55804634094238
- 153.59875535964966
- 161.8664107322693
- 158.58581274747849
- 146.8329443335533
- 151.1112676858902
- 167.4769143462181
- 173.01096433401108
- 141.002896130085
- 142.0207446217537
- 146.52715903520584
- 158.9751946926117
- 147.63088673353195
- 126.53717344999313
- 139.37227112054825
- 166.91352772712708
- 153.10623770952225
- 138.02683180570602
- 159.48356926441193
- 159.53730684518814
- 153.313814163208
- 156.74615490436554
- 124.91946578025818
- 126.17561113834381
- 144.35352659225464
- 139.89241194725037
- 144.41818195581436
- 151.25629502534866
- 144.51318448781967
- 141.50231510400772
- 133.40279150009155
- 140.01945978403091
- 137.51572161912918
- 118.16225808858871
- 130.75727099180222
- 138.7930679321289
- 145.33069455623627
- 118.76404148340225
- 133.41426968574524
- 158.90391570329666
- 139.79000574350357
- 138.54370027780533
- 160.382697224617
- 122.46250933408737
- 127.03268146514893
- 127.6005928516388
- 142.6162742972374
- 129.03384047746658
- 137.21535062789917
- 127.77215653657913
- 141.81974416971207
- 165.65358304977417
- 152.21061789989471
- 150.5477418899536
train_accuracy:
- 0.162
- 0.556
- 0.757
- 0.443
- 0.142
- 0.733
- 0.569
- 0.7
- 0.178
- 0.634
- 0.775
- 0.633
- 0.214
- 0.75
- 0.325
- 0.0
- 0.769
- 0.546
- 0.525
- 0.35
- 0.0
- 0.668
- 0.0
- 0.547
- 0.683
- 0.911
- 0.821
- 0.513
- 0.631
- 0.658
- 0.566
- 0.844
- 0.61
- 0.2
- 0.691
- 0.88
- 0.337
- 0.921
- 0.532
- 0.375
- 0.709
- 0.738
- 0.887
- 0.56
- 0.846
- 0.758
- 0.75
- 0.855
- 0.297
- 0.915
- 0.933
- 0.646
- 0.575
- 0.84
- 0.523
- 0.925
- 0.554
- 0.459
- 0.413
- 0.963
- 0.761
- 0.523
- 0.833
- 0.306
- 0.317
- 0.483
- 0.653
- 0.55
- 0.583
- 0.895
- 0.754
- 0.509
- 0.881
- 0.987
- 0.343
- 0.625
- 0.728
- 0.575
- 0.773
- 0.347
- 0.875
- 0.963
- 0.544
- 0.875
- 0.188
- 0.411
- 0.673
- 0.805
- 0.192
- 0.734
- 0.73
- 0.784
- 0.95
- 0.9
- 0.46
- 0.908
- 0.646
- 0.133
- 0.829
- 0.518
train_loss:
- 1.153
- 0.801
- 0.72
- 0.599
- 0.438
- 0.542
- 0.474
- 0.497
- 0.501
- 0.485
- 0.5
- 0.411
- 0.422
- 0.38
- 0.441
- 0.475
- 0.45
- 0.401
- 0.424
- 0.424
- 0.402
- 0.361
- 0.397
- 0.391
- 0.41
- 0.363
- 0.424
- 0.366
- 0.329
- 0.383
- 0.383
- 0.366
- 0.308
- 0.371
- 0.339
- 0.381
- 0.299
- 0.401
- 0.353
- 0.378
- 0.379
- 0.36
- 0.35
- 0.336
- 0.325
- 0.405
- 0.381
- 0.336
- 0.334
- 0.375
- 0.336
- 0.354
- 0.363
- 0.331
- 0.372
- 0.324
- 0.346
- 0.331
- 0.353
- 0.309
- 0.355
- 0.377
- 0.38
- 0.372
- 0.33
- 0.324
- 0.422
- 0.34
- 0.381
- 0.338
- 0.353
- 0.337
- 0.281
- 0.286
- 0.309
- 0.292
- 0.337
- 0.39
- 0.323
- 0.365
- 0.328
- 0.348
- 0.335
- 0.379
- 0.322
- 0.336
- 0.305
- 0.338
- 0.295
- 0.34
- 0.353
- 0.361
- 0.332
- 0.304
- 0.319
- 0.326
- 0.297
- 0.29
- 0.366
- 0.358
unequal: 1
verbose: 1
