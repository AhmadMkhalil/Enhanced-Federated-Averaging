avg_train_accuracy: 0.538
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 100
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.16351063829787235
- 0.2712765957446808
- 0.3751063829787234
- 0.41547872340425535
- 0.41808510638297874
- 0.44835106382978723
- 0.4373936170212766
- 0.4825531914893617
- 0.49877659574468086
- 0.556595744680851
- 0.5010638297872341
- 0.5302127659574468
- 0.5721276595744681
- 0.5560106382978723
- 0.5535638297872341
- 0.5732446808510638
- 0.5879787234042553
- 0.6025
- 0.5961170212765957
- 0.5388829787234043
- 0.5954255319148937
- 0.6101063829787234
- 0.5366489361702128
- 0.6095744680851064
- 0.6082978723404255
- 0.6626595744680851
- 0.5593085106382979
- 0.5972872340425532
- 0.5943617021276596
- 0.636968085106383
- 0.6016489361702128
- 0.5927659574468085
- 0.6340425531914894
- 0.6251595744680851
- 0.6572872340425532
- 0.6144148936170213
- 0.5781914893617022
- 0.6312765957446809
- 0.6477659574468085
- 0.6793085106382979
- 0.6301063829787235
- 0.6731914893617021
- 0.6646276595744681
- 0.6441489361702127
- 0.6431382978723404
- 0.6645744680851063
- 0.6260106382978723
- 0.6156382978723405
- 0.6003191489361702
- 0.637127659574468
- 0.6526595744680851
- 0.6745744680851063
- 0.613031914893617
- 0.6357978723404255
- 0.6707446808510639
- 0.64
- 0.691595744680851
- 0.6817553191489362
- 0.6605319148936171
- 0.6957978723404256
- 0.6981382978723404
- 0.663031914893617
- 0.6346808510638298
- 0.6141489361702127
- 0.6186170212765958
- 0.6636702127659575
- 0.6486702127659575
- 0.6402127659574468
- 0.6711702127659575
- 0.6310638297872341
- 0.6245744680851064
- 0.6690957446808511
- 0.653031914893617
- 0.6794148936170212
- 0.6503191489361703
- 0.6802659574468085
- 0.6404255319148936
- 0.635904255319149
- 0.6535106382978724
- 0.6279255319148936
- 0.6625
- 0.6604787234042553
- 0.6332978723404256
- 0.6204787234042554
- 0.6191489361702127
- 0.6657446808510639
- 0.6461702127659574
- 0.6811170212765958
- 0.6509574468085106
- 0.6473936170212766
- 0.6486702127659575
- 0.6688829787234043
- 0.6596808510638298
- 0.6436170212765957
- 0.6648936170212766
- 0.6727127659574468
- 0.660904255319149
- 0.6519680851063829
- 0.6442021276595745
- 0.6137234042553191
test_loss_list:
- 514.7620332241058
- 412.61424803733826
- 349.43963527679443
- 324.41131150722504
- 305.75114035606384
- 274.60090577602386
- 269.6309697628021
- 248.19293355941772
- 245.9442925453186
- 222.281853556633
- 231.22590816020966
- 227.66153609752655
- 187.56509393453598
- 197.45299768447876
- 196.50056660175323
- 180.7024946808815
- 181.7673982977867
- 180.09112298488617
- 185.06722164154053
- 217.89645397663116
- 175.5291387438774
- 187.64160984754562
- 219.33761703968048
- 173.32016438245773
- 168.31172066926956
- 150.678893327713
- 207.95023548603058
- 188.14582043886185
- 170.40905272960663
- 160.2124783396721
- 172.62130796909332
- 182.8929619193077
- 165.40375369787216
- 169.42484086751938
- 150.81763088703156
- 168.3959328532219
- 191.7476722598076
- 168.25951570272446
- 154.45448070764542
- 139.8863132596016
- 173.08980530500412
- 145.61271613836288
- 152.51774615049362
- 157.5522044301033
- 149.58944940567017
- 156.74314880371094
- 164.38794714212418
- 181.46351021528244
- 168.76108556985855
- 159.69739520549774
- 150.86877489089966
- 144.23463761806488
- 170.85892981290817
- 161.41439390182495
- 138.27726274728775
- 152.93951106071472
- 133.36814332008362
- 146.07551229000092
- 152.84078305959702
- 136.921704351902
- 135.75780749320984
- 158.85712933540344
- 163.60943347215652
- 166.9489312171936
- 154.47615313529968
- 140.79503351449966
- 149.44418054819107
- 146.62221390008926
- 136.19946670532227
- 162.17241048812866
- 159.19172656536102
- 135.02795732021332
- 150.28848803043365
- 151.05233597755432
- 145.20892053842545
- 132.04935103654861
- 147.9636579155922
- 157.1367525458336
- 139.39475893974304
- 150.9961103796959
- 143.3168214559555
- 151.38171714544296
- 166.35192596912384
- 176.97174936532974
- 177.00477027893066
- 139.9135269522667
- 155.82013922929764
- 141.56235128641129
- 150.18542683124542
- 156.0913691520691
- 153.32744014263153
- 148.07507342100143
- 145.5937084555626
- 143.0881700515747
- 141.68590146303177
- 137.19376522302628
- 133.43946659564972
- 147.70916032791138
- 147.25784051418304
- 154.3207002878189
train_accuracy:
- 0.217
- 0.394
- 0.403
- 0.825
- 0.058
- 0.667
- 0.742
- 0.375
- 0.46
- 0.715
- 0.025
- 0.193
- 0.509
- 0.63
- 0.668
- 0.453
- 0.15
- 0.262
- 0.678
- 0.543
- 0.52
- 0.68
- 0.411
- 0.625
- 0.76
- 0.825
- 0.69
- 0.347
- 0.731
- 0.457
- 0.675
- 0.277
- 0.659
- 0.825
- 0.763
- 0.65
- 0.432
- 0.158
- 0.721
- 0.953
- 0.95
- 0.294
- 0.903
- 0.533
- 0.838
- 0.345
- 0.77
- 0.59
- 0.213
- 0.781
- 0.364
- 0.927
- 0.562
- 0.663
- 0.85
- 0.812
- 0.9
- 0.938
- 0.856
- 0.172
- 0.46
- 0.545
- 0.438
- 0.821
- 0.815
- 0.639
- 0.688
- 0.381
- 0.506
- 0.75
- 0.445
- 0.744
- 0.34
- 0.85
- 0.975
- 0.695
- 0.85
- 0.742
- 0.88
- 0.866
- 0.625
- 0.975
- 0.771
- 0.744
- 0.817
- 0.232
- 0.844
- 0.835
- 0.736
- 0.842
- 0.908
- 0.963
- 0.894
- 0.86
- 0.669
- 0.427
- 0.45
- 0.225
- 0.095
- 0.538
train_loss:
- 1.208
- 0.875
- 0.578
- 0.538
- 0.561
- 0.518
- 0.548
- 0.45
- 0.426
- 0.403
- 0.394
- 0.33
- 0.457
- 0.407
- 0.359
- 0.375
- 0.41
- 0.394
- 0.369
- 0.438
- 0.356
- 0.385
- 0.344
- 0.39
- 0.377
- 0.36
- 0.414
- 0.348
- 0.46
- 0.427
- 0.422
- 0.333
- 0.396
- 0.43
- 0.343
- 0.326
- 0.388
- 0.368
- 0.366
- 0.354
- 0.265
- 0.285
- 0.389
- 0.37
- 0.297
- 0.41
- 0.289
- 0.331
- 0.367
- 0.282
- 0.34
- 0.308
- 0.393
- 0.323
- 0.33
- 0.374
- 0.323
- 0.397
- 0.351
- 0.309
- 0.366
- 0.327
- 0.37
- 0.363
- 0.395
- 0.282
- 0.302
- 0.327
- 0.324
- 0.325
- 0.309
- 0.292
- 0.324
- 0.345
- 0.317
- 0.32
- 0.312
- 0.324
- 0.323
- 0.349
- 0.366
- 0.322
- 0.327
- 0.279
- 0.265
- 0.311
- 0.333
- 0.248
- 0.253
- 0.357
- 0.289
- 0.265
- 0.333
- 0.349
- 0.259
- 0.311
- 0.353
- 0.344
- 0.289
- 0.337
unequal: 1
verbose: 1
