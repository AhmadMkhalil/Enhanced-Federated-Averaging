avg_train_accuracy: 0.779
avg_train_loss: 0.009
avg_type: avg
dataset: emnist-balanced
epochs: 50
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 10
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 47
number_of_classes_of_half_of_user: 2
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.1452659574468085
- 0.5497340425531915
- 0.5642553191489361
- 0.5642553191489361
- 0.5642553191489361
- 0.6061702127659574
- 0.6597340425531915
- 0.6597340425531915
- 0.6878723404255319
- 0.7004255319148937
- 0.7004255319148937
- 0.7004255319148937
- 0.7135638297872341
- 0.7323936170212766
- 0.7323936170212766
- 0.7323936170212766
- 0.7323936170212766
- 0.7323936170212766
- 0.7323936170212766
- 0.7323936170212766
- 0.7323936170212766
- 0.734095744680851
- 0.7507446808510638
- 0.7507446808510638
- 0.7507446808510638
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7710106382978723
- 0.7819680851063829
- 0.7819680851063829
- 0.7819680851063829
- 0.7819680851063829
- 0.7819680851063829
- 0.7819680851063829
- 0.7819680851063829
- 0.7819680851063829
- 0.7819680851063829
- 0.7819680851063829
test_loss_list:
- 487.4751238822937
- 251.2812123298645
- 215.0867636203766
- 215.0867636203766
- 215.0867636203766
- 183.64605724811554
- 157.2868856191635
- 157.2868856191635
- 146.78961193561554
- 138.16102761030197
- 138.16102761030197
- 138.16102761030197
- 128.51302981376648
- 120.59127539396286
- 120.59127539396286
- 120.59127539396286
- 120.59127539396286
- 120.59127539396286
- 120.59127539396286
- 120.59127539396286
- 120.59127539396286
- 115.62913101911545
- 110.79638576507568
- 110.79638576507568
- 110.79638576507568
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 104.42827516794205
- 98.06701719760895
- 98.06701719760895
- 98.06701719760895
- 98.06701719760895
- 98.06701719760895
- 98.06701719760895
- 98.06701719760895
- 98.06701719760895
- 98.06701719760895
- 98.06701719760895
train_accuracy:
- 0.183
- 0.604
- 0.954
- 0.633
- 0.287
- 0.993
- 0.683
- 0.942
- 0.95
- 0.683
- 0.667
- 0.646
- 0.982
- 0.721
- 0.688
- 0.692
- 0.696
- 0.975
- 1.0
- 0.904
- 0.796
- 0.767
- 0.729
- 0.996
- 0.962
- 0.812
- 0.287
- 0.946
- 0.792
- 0.625
- 0.258
- 0.596
- 1.0
- 0.783
- 0.75
- 0.762
- 0.75
- 0.7
- 0.792
- 0.979
- 0.812
- 0.971
- 0.986
- 0.987
- 0.733
- 0.696
- 0.986
- 0.767
- 0.779
- 0.779
train_loss:
- 1.747
- 2.481
- 1.435
- 0.902
- 0.492
- 0.828
- 1.219
- 0.444
- 1.17
- 1.087
- 0.74
- 0.713
- 1.05
- 1.326
- 0.68
- 0.684
- 0.957
- 0.367
- 0.393
- 0.693
- 0.947
- 0.941
- 0.914
- 0.083
- 0.651
- 1.182
- 0.675
- 0.383
- 0.894
- 0.337
- 0.673
- 0.313
- 0.08
- 0.642
- 0.881
- 0.611
- 0.597
- 0.341
- 0.596
- 0.886
- 1.075
- 0.839
- 0.584
- 0.342
- 0.563
- 0.321
- 0.61
- 0.619
- 0.61
- 0.857
unequal: 0
verbose: 1
